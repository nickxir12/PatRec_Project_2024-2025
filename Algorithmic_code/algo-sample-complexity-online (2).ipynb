{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"authorship_tag":"ABX9TyMSJa3XxLl7rvVV5eF0x78J","gpuType":"T4","provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"064bec9d457349bb9fa6a559339e9263":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9bf43e30b5a499a8de4604dc3f10ee2","IPY_MODEL_17166645b0b14019a9f5fe3d70b6caa5","IPY_MODEL_e451f4ad11bc44f8ab881eed658a8ee3"],"layout":"IPY_MODEL_ceb7ec9cff0049c098ac4b5fc5b6bd3c"}},"07c3736f38804f78bea83bb64c695cac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ff27ff9889744e6af586debee44669c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"17166645b0b14019a9f5fe3d70b6caa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1776d91874314cd0ab3df42e1b77bd5d","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_915f46f7e6694bf0b14a91ced2256596","value":100}},"1776d91874314cd0ab3df42e1b77bd5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c00be62ed5f48868daac8e39944ba63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_740bfc3868e947e68c87f28690b96945","placeholder":"​","style":"IPY_MODEL_73b20e21e4394b6c9326c75594d6f062","value":"Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"}},"30e1fcaaa18c40fcbb2182e33df28ea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c0b6a5234ea48e3bf8fa9fb04e7442d","placeholder":"​","style":"IPY_MODEL_ccfc9ef4719e4aef9379f11160003ce1","value":" 100/100 [04:53&lt;00:00,  3.31s/it]"}},"328beb9c29ed4dc094b6e476734a725f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36d9e94b822a4a5bb069cd474608bdb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37733b27dcd64ffaafd07959bc847f26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a3e587d4127443eb3bbae84621d4d06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eb47dc8d09747718044ca73cfadc307":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_936aa9172b2d4e0aa2f0cb84c0dabfad","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07c3736f38804f78bea83bb64c695cac","value":100}},"5c0b6a5234ea48e3bf8fa9fb04e7442d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b20e21e4394b6c9326c75594d6f062":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"740bfc3868e947e68c87f28690b96945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"915f46f7e6694bf0b14a91ced2256596":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"936aa9172b2d4e0aa2f0cb84c0dabfad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9bf43e30b5a499a8de4604dc3f10ee2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a3e587d4127443eb3bbae84621d4d06","placeholder":"​","style":"IPY_MODEL_37733b27dcd64ffaafd07959bc847f26","value":"Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"}},"ccfc9ef4719e4aef9379f11160003ce1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceb7ec9cff0049c098ac4b5fc5b6bd3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e451f4ad11bc44f8ab881eed658a8ee3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_328beb9c29ed4dc094b6e476734a725f","placeholder":"​","style":"IPY_MODEL_36d9e94b822a4a5bb069cd474608bdb6","value":" 100/100 [04:51&lt;00:00,  3.11s/it]"}},"fe873b542e0243ebbf591f5bc21ccd2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c00be62ed5f48868daac8e39944ba63","IPY_MODEL_4eb47dc8d09747718044ca73cfadc307","IPY_MODEL_30e1fcaaa18c40fcbb2182e33df28ea1"],"layout":"IPY_MODEL_0ff27ff9889744e6af586debee44669c"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Idea 4: Samples fitering - online\n\n## Περιγραφή του αλγορίθμου\n- Γίνεται εκπαίδευση με grokfast - EMA. Όταν **appl_sampl_filter** is False έχω μόνο αυτό, ενώ για True εφαρμόζω επιπλέον και την ιδέα 4 για πιο έξυπνη επιλογή δειγμάτων.\n- Ο Dataloader έχει έναν custom sampler (WeightedRandomSampler) ο οποίος κάθε φορά διαλέγει ένα δείγμα με βάση κάποιο βάρος/πιθανότητα.\n- Στην αρχή τα βάρη είναι όλα ίδια (ομοιόμορφη κατανομή) οπότε ο Dataloader λειτουργεί όπως συνήθως διαλέγοντας τυχαία ένα sample.\n- Σε κάθε επανάληψη φτιάχνεται ένα ranking των δειγμάτων (με βάση του πόσο high frequency περιέχει το καθένα) το οποίο χρησιμοποιείται για να αποφασιστεί τι βάρος/πιθανότητα θα δοθεί σε κάθε δείγμα να επιλεγεί για εκπαίδευση. Το διάνυσμα βαρών/πιθανοτήτων ανανεώνεται κάθε **sampling_distr_upd_freq** επαναλήψεις.\n- Στην κατασκευή του διανύσματος βαρών από την συνολική πιθανότητα 1 δίνουμε στα **top_k** δείγματα συνολικά **top_k_sampling_prob** (και στα υπόλοιπα length(dataset) - **top_k** δείγματα δίνουμε συνολικά το υπόλοιπο 1 - **top_k_sampling_prob**).\n- Με **high_freq_better** is True ακολουθούμε την αρχική μας υπόθεση ότι τα δείγματα με high frequency είναι αυτά που θα πρέπει να ταΐσουμε το δίκτυο περισσότερο για να μάθει γρηγορότερα, για False γίνεται το αντίθετο.\n\n## Οδηγίες χρήσης για τρέξιμο\nΠήγαινε στον τίτλο **Execute training (by running main funciton)**. Πήγαινε στο parser.parse_args και όρισε τις τιμές που θες να δοκιμάσεις για grid search. Οι υπερπαράμετροι που σχετίζονται με την ιδέα 4 online είναι:\n\n- **top_k**\n- **top_k_sampling_prob**\n- **high_freq_better**\n- **sampling_distr_upd_freq**: Μάλλον είναι οκ στο 1 γιατί ακόμα και έτσι η εκπαίδευση δεν είναι αργή οπότε δεν έχω λόγο να το αυξήσω.\n\nΑν κάποιος θέλει να τρέξει κάποιες τιμές για το grid search, έχω βάλει στον φάκελο και ένα αρχείο για να σημειώνουμε τις τιμές των υπερπαραμέτρων που δοκίμασε ο καθένας για να μην τρέχουμε όλοι τα ίδια. Βάλτε GPU P100 (νομίζω είναι ελαφρώς καλύτερη), εμένα για τα 100.000 βήματα που έχω βάλει να είναι το default ένα τρέξιμο που κάνω μόνο με grokfast (δηλαδή **appl_sampl_filter** is False) παίρνει περίπου **7 λεπτά** οπότε καλά είμαστε από χρόνο.\n\n","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Maybe this is needed if you want to import private datasets\n# kagglehub.login()\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:06.278449Z","iopub.execute_input":"2025-02-12T04:01:06.278807Z","iopub.status.idle":"2025-02-12T04:01:08.523935Z","shell.execute_reply.started":"2025-02-12T04:01:06.278781Z","shell.execute_reply":"2025-02-12T04:01:08.523274Z"},"executionInfo":{"elapsed":24569,"status":"ok","timestamp":1737367419894,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"f4s6HWPGPSBJ","outputId":"be8dc80d-eeb5-492c-f36e-15ba319dec89","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\n# hojjatk_mnist_dataset_path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n\n# The dataset was uploaded from me but I made it public so you too can probably load it with this line\n# _ = kagglehub.dataset_download(\"konstantinosbarkas/mnist-dataset-processed-from-local\")\n\n# print(\"Data source import complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:08.524866Z","iopub.execute_input":"2025-02-12T04:01:08.525205Z","iopub.status.idle":"2025-02-12T04:01:08.528182Z","shell.execute_reply.started":"2025-02-12T04:01:08.525173Z","shell.execute_reply":"2025-02-12T04:01:08.527513Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import sys, os\n# Install the Grokfast library\n!wget https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\n\nsys.path.append(\"/kaggle/working\")\nos.makedirs('/kaggle/working/results/algo_online', exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:08.529900Z","iopub.execute_input":"2025-02-12T04:01:08.530089Z","iopub.status.idle":"2025-02-12T04:01:08.870740Z","shell.execute_reply.started":"2025-02-12T04:01:08.530072Z","shell.execute_reply":"2025-02-12T04:01:08.869974Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--2025-02-12 04:01:08--  https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1703 (1.7K) [text/plain]\nSaving to: ‘grokfast.py’\n\ngrokfast.py         100%[===================>]   1.66K  --.-KB/s    in 0s      \n\n2025-02-12 04:01:08 (30.9 MB/s) - ‘grokfast.py’ saved [1703/1703]\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# import argparse\n# import gzip\nimport math\nimport random\n# import struct\nimport time\nfrom argparse import ArgumentParser\n# from collections import Counter, defaultdict, deque\nimport itertools\nfrom itertools import islice\n# from pathlib import Path\n# from typing import Dict, List, Literal, Optional\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom functorch import grad, vmap\nfrom sklearn.model_selection import train_test_split\nfrom torch.autograd import grad\n\n# from torch.nn.utils.stateless import functional_call, # This is deprecated, use the next one instead\nfrom torch.func import functional_call\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler, Dataset\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom itertools import islice\nimport torch.nn.functional as F\n\nfrom grokfast import gradfilter_ema,gradfilter_ma\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:08.872073Z","iopub.execute_input":"2025-02-12T04:01:08.872341Z","iopub.status.idle":"2025-02-12T04:01:17.915953Z","shell.execute_reply.started":"2025-02-12T04:01:08.872318Z","shell.execute_reply":"2025-02-12T04:01:17.915276Z"},"executionInfo":{"elapsed":36273,"status":"ok","timestamp":1737367466050,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"QLUi9XZMRpId","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Enables detailed CUDA error messages\n\nresults_dir = \"/kaggle/working/results/algo_online\"\nos.makedirs(results_dir, exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:17.916715Z","iopub.execute_input":"2025-02-12T04:01:17.917195Z","iopub.status.idle":"2025-02-12T04:01:17.921167Z","shell.execute_reply.started":"2025-02-12T04:01:17.917163Z","shell.execute_reply":"2025-02-12T04:01:17.920340Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:17.922041Z","iopub.execute_input":"2025-02-12T04:01:17.922276Z","iopub.status.idle":"2025-02-12T04:01:18.032567Z","shell.execute_reply.started":"2025-02-12T04:01:17.922247Z","shell.execute_reply":"2025-02-12T04:01:18.031662Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"optimizer_dict = {\"AdamW\": torch.optim.AdamW, \"Adam\": torch.optim.Adam, \"SGD\": torch.optim.SGD}\n\nactivation_dict = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"Sigmoid\": nn.Sigmoid, \"GELU\": nn.GELU}\n\nloss_function_dict = {\"MSE\": nn.MSELoss, \"CrossEntropy\": nn.CrossEntropyLoss}\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.033631Z","iopub.execute_input":"2025-02-12T04:01:18.033860Z","iopub.status.idle":"2025-02-12T04:01:18.052769Z","shell.execute_reply.started":"2025-02-12T04:01:18.033841Z","shell.execute_reply":"2025-02-12T04:01:18.052110Z"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1737367466051,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"8hhZpMIuSC4q","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\"Causal transformer block\n    \"\"\"\n\n    def __init__(self, dim, num_heads):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(dim)\n        self.ln_2 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim),\n        )\n\n    def forward(self, x):\n        attn_mask = torch.full(\n            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n        )\n        attn_mask = torch.triu(attn_mask, diagonal=1)\n        attn_mask[torch.isnan(attn_mask)] = 0.0 # fixes all 'nan' on 'mps' device\n\n        x = self.ln_1(x)\n        a, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n        x = x + a\n        m = self.mlp(self.ln_2(x))\n        x = x + m\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.055091Z","iopub.execute_input":"2025-02-12T04:01:18.055316Z","iopub.status.idle":"2025-02-12T04:01:18.074950Z","shell.execute_reply.started":"2025-02-12T04:01:18.055298Z","shell.execute_reply":"2025-02-12T04:01:18.074367Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \"\"\"Causal Transformer decoder\n    \"\"\"\n\n    def __init__(self, dim=128, num_layers=2, num_heads=4, num_tokens=97, seq_len=5):\n        super().__init__()\n        self.token_embeddings = nn.Embedding(num_tokens, dim)\n        self.position_embeddings = nn.Embedding(seq_len, dim)\n        self.layers = nn.ModuleList()\n        for _ in range(num_layers):\n            self.layers.append(Block(dim, num_heads))\n\n        self.ln_f = nn.LayerNorm(dim)\n        self.head = nn.Linear(dim, num_tokens, bias=False)\n\n    def forward(self, x):\n        # Ensure input tensor x contains valid token IDs\n        x = torch.clamp(x, 0, self.token_embeddings.num_embeddings - 1)\n\n        h = self.token_embeddings(x)\n        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n        positions = torch.clamp(positions, 0, self.position_embeddings.num_embeddings - 1)\n\n        h = h + self.position_embeddings(positions).expand_as(h)\n        for layer in self.layers:\n            h = layer(h)\n\n        h = self.ln_f(h)\n        logits = self.head(h)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.075972Z","iopub.execute_input":"2025-02-12T04:01:18.076225Z","iopub.status.idle":"2025-02-12T04:01:18.095534Z","shell.execute_reply.started":"2025-02-12T04:01:18.076206Z","shell.execute_reply":"2025-02-12T04:01:18.094960Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def multiplication_mod_p_data(p, eq_token, op_token):\n    \"\"\"x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n    \"\"\"\n    x = torch.arange(p)\n    y = torch.arange(1, p)\n    x, y = torch.cartesian_prod(x, y).T\n\n    eq = torch.ones_like(x) * eq_token\n    op = torch.ones_like(x) * op_token\n    result = x * y % p\n\n    return torch.stack([x, op, y, eq, result])\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.096290Z","iopub.execute_input":"2025-02-12T04:01:18.096481Z","iopub.status.idle":"2025-02-12T04:01:18.122800Z","shell.execute_reply.started":"2025-02-12T04:01:18.096456Z","shell.execute_reply":"2025-02-12T04:01:18.122023Z"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1737367466051,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"RIifrNowR89e","trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# def cycle(iterable):\n#     while True:\n#         for x in iterable:\n#             yield x\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.123703Z","iopub.execute_input":"2025-02-12T04:01:18.123999Z","iopub.status.idle":"2025-02-12T04:01:18.141827Z","shell.execute_reply.started":"2025-02-12T04:01:18.123957Z","shell.execute_reply":"2025-02-12T04:01:18.141268Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # Not used anymore\n# def custom_collate_fn_2(batch):\n#     \"\"\"Custom collate function to handle extra fields in the dataset.\"\"\"\n#     images, labels, _, _ = zip(*batch)  # Ignore the indices and extra_fields for loss computation\n#     images = torch.stack(images)  # Stack images into a single tensor\n#     labels = torch.tensor(labels)  # Convert labels to a tensor\n#     return images, labels\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.142680Z","iopub.execute_input":"2025-02-12T04:01:18.142958Z","iopub.status.idle":"2025-02-12T04:01:18.160704Z","shell.execute_reply.started":"2025-02-12T04:01:18.142930Z","shell.execute_reply":"2025-02-12T04:01:18.159872Z"},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1737367551771,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"zlciE-2nKkLg","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    samples, labels, indices, extra_fields = zip(*batch)\n    samples = torch.stack(samples)  # Stack samples into a single tensor\n    labels = torch.tensor(labels)  # Convert labels to a tensor\n\n    return samples, labels, indices, extra_fields\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.161544Z","iopub.execute_input":"2025-02-12T04:01:18.161852Z","iopub.status.idle":"2025-02-12T04:01:18.179367Z","shell.execute_reply.started":"2025-02-12T04:01:18.161821Z","shell.execute_reply":"2025-02-12T04:01:18.178640Z"},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1737367559166,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"zif6Q-IEjFJ7","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Needed for per sample gradient computations\ndef select_random_subset(tensor, percentage, seed=42):\n    \"\"\"\n    Flatten the parameter dimensions for each batch sample, select a percentage of elements,\n    and return a tensor with shape [batch_size, selected_elements].\n\n    Args:\n        tensor (torch.Tensor): The gradient tensor of shape [batch_size, *parameter_dims].\n        percentage (float): The percentage of elements to select.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        torch.Tensor: A tensor of shape [batch_size, selected_elements].\n    \"\"\"\n    batch_size, *param_dims = tensor.shape  # Extract batch and parameter dimensions\n    total_params = torch.prod(torch.tensor(param_dims))  # Total parameters per sample\n    subset_size = int(total_params * percentage)  # 20% of parameters\n\n    # Set seed for reproducibility\n    random.seed(seed)\n    indices = random.sample(range(total_params), subset_size)  # Random indices for selection\n\n    # Flatten parameter dimensions and select elements for each batch\n    flat_tensor = tensor.view(batch_size, -1)  # Flatten parameter dimensions for each sample\n    selected_subset = flat_tensor[:, indices]  # Select the same random indices across the batch\n\n    return selected_subset\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.180017Z","iopub.execute_input":"2025-02-12T04:01:18.180191Z","iopub.status.idle":"2025-02-12T04:01:18.205536Z","shell.execute_reply.started":"2025-02-12T04:01:18.180176Z","shell.execute_reply":"2025-02-12T04:01:18.204778Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Needed for online sample filtering\ndef rank_to_sampling_weights(my_dataset, top_k, top_k_sampling_prob, high_freq_better):\n    \"\"\"\n    Rank samples by variance_metric and assign sampling weights.\n\n    Parameters:\n    - my_dataset: MyMNIST object.\n    - top_k: Fraction of top samples to assign higher sampling probability.\n    - top_k_sampling_prob: Probability assigned to the top_k fraction of samples.\n\n    Returns:\n    - new_weights: List of sampling weights for each sample.\n    \"\"\"\n    # Calculate the number of top_k samples\n    num_samples = len(my_dataset)\n    top_k_count = int(top_k * num_samples)\n\n    # Sort indices by variance_metric in descending order\n    sorted_indices = sorted(\n        range(num_samples),\n        key=lambda idx: my_dataset.dataset.extra_fields[idx][\"variance_metric\"],\n        reverse=high_freq_better,\n    )\n\n    # Initialize new_weights with zeros\n    new_weights = [0.0] * num_samples\n\n    # Assign weights to the top_k samples\n    for idx in sorted_indices[:top_k_count]:\n        new_weights[idx] = top_k_sampling_prob / top_k_count\n\n    # Assign weights to the rest of the samples\n    for idx in sorted_indices[top_k_count:]:\n        new_weights[idx] = (1 - top_k_sampling_prob) / (num_samples - top_k_count)\n\n    return new_weights\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.206435Z","iopub.execute_input":"2025-02-12T04:01:18.206731Z","iopub.status.idle":"2025-02-12T04:01:18.224235Z","shell.execute_reply.started":"2025-02-12T04:01:18.206704Z","shell.execute_reply":"2025-02-12T04:01:18.223470Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class AlgoDataset(torch.utils.data.Dataset):\n    def __init__(self, data, targets):\n        \"\"\"\n        Custom dataset to store dataset + extra fields for each sample.\n        Insert data as a tensor of shape [num_samples, num_features] and targets as a tensor of shape [num_samples].\n        \"\"\"\n\n        self.data = data\n        self.targets = targets\n\n        # Initialize extra fields\n        self.extra_fields = [\n            {\n                \"ema_grad\": 0.0,\n                \"num_updates\": 0,\n                \"variance_metric\": 0.0,\n            }\n            for _ in range(len(self.data))\n        ]\n\n    def __getitem__(self, index):\n        \"\"\"Returns a single data sample with extra fields.\"\"\"\n\n        sample, target = self.data[index], self.targets[index]\n\n        extra_field = self.extra_fields[index]\n        return sample, target, int(index), extra_field\n\n    def __len__(self):\n        return len(self.data)\n\n    def update_fields(self, indices, grad_stats, ema_alpha=0.9):\n        \"\"\"\n        Update the extra fields for specified dataset indices.\n        \"\"\"\n\n        for idx, grad in zip(indices, grad_stats):\n            # Update EMA\n            sample_field = self.extra_fields[idx]\n\n            current_ema = sample_field[\"ema_grad\"]\n            updated_ema = ema_alpha * current_ema + (1 - ema_alpha) * grad\n            sample_field[\"ema_grad\"] = updated_ema\n\n            deviation = abs(grad - current_ema)\n\n            num_updates = sample_field[\"num_updates\"] + 1  # Increment the update count\n\n            current_avg_deviation = sample_field[\"variance_metric\"] ** 0.5\n            new_avg_deviation = ((current_avg_deviation * (num_updates - 1)) + deviation) / num_updates\n\n            sample_field[\"num_updates\"] = num_updates\n\n            # Variance estimate (for higher sensitivity to fast changes)\n            sample_field[\"variance_metric\"] = new_avg_deviation**2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:01:18.225108Z","iopub.execute_input":"2025-02-12T04:01:18.225365Z","iopub.status.idle":"2025-02-12T04:01:18.250923Z","shell.execute_reply.started":"2025-02-12T04:01:18.225346Z","shell.execute_reply":"2025-02-12T04:01:18.250185Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## def main","metadata":{}},{"cell_type":"code","source":"def main(args):\n    torch.manual_seed(args.seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    eq_token = args.p\n    op_token = args.p + 1\n\n    # Create model\n    model = Decoder(\n        dim=128, num_layers=2, num_heads=4, num_tokens=args.p + 2, seq_len=5\n    ).to(device)\n    print(model)\n    nparams = sum([p.numel() for p in model.parameters() if p.requires_grad])\n    print(f'Total number of parameters: {nparams}')\n\n    # Create dataset\n    data = multiplication_mod_p_data(args.p, eq_token, op_token)\n    # data = data.T\n\n    # Split the dataset into training and validation sets\n    train_idx, valid_idx = torch.randperm(data.shape[1]).split(data.shape[1] // 2)\n    train_data, valid_data = data[:, train_idx], data[:, valid_idx]\n\n    # Create initial weights for uniform sampling\n    weights = [1.0] * len(train_data)\n    sampler = WeightedRandomSampler(weights, len(weights))\n\n    # DataLoader\n    train_dataset = AlgoDataset(train_data[:4].T, train_data[4])\n    valid_dataset = AlgoDataset(valid_data[:4].T, valid_data[4])\n\n    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=sampler, collate_fn=custom_collate_fn)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, collate_fn=custom_collate_fn)\n\n    # data_iter = cycle(train_loader)\n\n    # Optimizer\n    optimizer = getattr(torch.optim, args.optimizer)(\n        model.parameters(),\n        lr=args.lr,\n        weight_decay=args.weight_decay,\n        betas=(args.beta1, args.beta2),\n    )\n\n    # Scheduler\n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optimizer, lambda update: 1 if update > 10 else update / 10\n    )\n\n    steps_per_epoch = math.ceil(data.shape[1] / args.batch_size)\n\n    # ----------------------------------------------------------------------------------\n    # # Needed for per sample gradient computations\n    # if args.appl_sampl_filter:\n    #     # Define a function for forward + loss computation\n    #     def compute_loss_vmap(params, buffers, model, x, y):\n    #         logits = functional_call(model, {**params, **buffers}, x.unsqueeze(0))\n    #         loss = loss_fn(logits, y.unsqueeze(0))  # Single output\n    #         return loss.mean()\n\n    #     # Prepare model parameters and buffers\n    #     params_and_buffers = {**dict(model.named_parameters()), **dict(model.named_buffers())}\n\n    #     params = {k: v for k, v in params_and_buffers.items() if v.requires_grad}\n    #     buffers = {k: v for k, v in params_and_buffers.items() if not v.requires_grad}\n\n    #     # Create the gradient function\n    #     gradient_fn = grad(compute_loss_vmap)\n    # ----------------------------------------------------------------------------------\n\n    # Start Training below\n    its, train_acc, val_acc, train_loss, val_loss = [], [], [], [], []\n    grads = None\n    i = 0\n\n    # For logging network weights.\n    net_its, nets = [], []\n\n    optim_steps = int(args.budget) // steps_per_epoch\n    log_tqdm_freq = math.ceil(optim_steps / 150)\n\n    with tqdm(total=(optim_steps), dynamic_ncols=True) as pbar:\n\n        stable_steps = 0\n        stable_threshold = 1000\n        reached_early_stop = False\n        steps_to_reach_val_acc = None\n\n        for e in range(optim_steps):\n            if reached_early_stop: break\n\n            for dataloader, is_train in [(train_dataloader, True), (valid_dataloader, False)]:\n\n                # Update sampling distribution periodically\n                if args.appl_sampl_filter and i > args.start and i % args.sampling_distr_upd_freq == 0:\n                    new_weights = rank_to_sampling_weights(train_dataset, args.top_k, args.top_k_sampling_prob, args.high_freq_better)\n                    new_sampler = WeightedRandomSampler(new_weights, num_samples=len(train_data), replacement=True)\n                    train_dataloader = DataLoader(\n                        train_data,\n                        batch_size=args.batch_size,\n                        sampler=new_sampler,\n                        collate_fn=custom_collate_fn\n                    )\n\n                model.train(is_train)\n                total_loss = 0\n                total_acc = 0\n\n                for samples, targets, indices, _, in dataloader:\n\n                    samples, targets = samples.to(device), targets.to(device)\n\n                    with torch.set_grad_enabled(is_train):\n                        logits = model(samples.T)\n                        # calculate loss only on the answer part of the equation (last element\n                        loss = F.cross_entropy(logits[-1], targets)\n                        total_loss += loss.item() * samples.shape[0]\n\n                        if is_train:\n                            optimizer.zero_grad()\n                            loss.backward()\n\n\n                        # if args.appl_sampl_filter:  # Unnecessary if we are not applying sample filtering\n                        #     # -----------------------------------------------------------------\n                        #     #   Gradient Stats: Capture grads for each sample\n                        #     # -----------------------------------------------------------------\n                        #     # Identify the last two Linear layers dynamically\n                        #     # batch_gradients = []\n\n                        #     with torch.no_grad():\n                        #         per_sample_grads = vmap(gradient_fn, in_dims=(None, None, None, 0, 0))(\n                        #             params, buffers, mlp, x, labels\n                        #         )  # Use torch.vmap\n\n\n                        #         # Extract gradients for the target layers\n                        #         last_layer_grad = per_sample_grads[\"3.weight\"]  # Adjust key as needed\n                        #         second_last_layer_grad = per_sample_grads[\"5.weight\"]\n\n                        #         # Select a subset of gradients\n                        #         percentage_s_l = 0.2\n                        #         percentage_l = 1\n                        #         selected_last = select_random_subset(last_layer_grad, percentage_l, seed=42)\n                        #         selected_second_last = select_random_subset(second_last_layer_grad, percentage_s_l, seed=42)\n\n                        #         # Compute the average and detach\n                        #         selected_last_avg = selected_last.mean(dim=-1).detach().cpu()\n                        #         selected_second_last_avg = selected_second_last.mean(dim=-1).detach().cpu()\n                        #         total_avg = (selected_last_avg + selected_second_last_avg) / 2\n\n                        #     train_dataset.dataset.update_fields(indices, total_avg, args.ema_alpha_sampl_rank)\n\n                        # # -----------------------------------------------------------------\n                        # # -----------------------------------------------------------------\n\n\n                        #######\n                        # Grokfast\n\n                        trigger = i < 500 if args.two_stage else False\n\n                        if args.filter == \"none\":\n                            pass\n                        elif args.filter == \"ma\":\n                            grads = gradfilter_ma(model, grads=grads, window_size=args.window_size, lamb=args.lamb, trigger=trigger)\n                        elif args.filter == \"ema\":\n                            grads = gradfilter_ema(model, grads=grads, alpha=args.alpha, lamb=args.lamb)\n                        else:\n                            raise ValueError(f\"Invalid gradient filter type `{args.filter}`\")\n\n                        #######\n\n                        optimizer.step()\n                        scheduler.step()\n                        i += 1\n\n                        acc = (logits[-1].argmax(-1) == samples.T).float().mean()\n                        total_acc += acc.item() * samples.shape[0]\n\n                    if is_train:\n                        train_acc.append(total_acc / len(train_dataset))\n                        train_loss.append(total_loss / len(train_dataset))\n                        its.append(i)\n                    else:\n                        val_acc.append(total_acc / len(valid_dataset))\n                        val_loss.append(total_loss / len(valid_dataset))\n\n                    do_log_tqdm = (i < 150 and i % 10 == 0) or i % log_tqdm_freq == 0\n                    if do_log_tqdm and (not is_train):\n                        pbar.set_description(\n                        \"Loss: {0:1.1e}|{1:1.1e}. Acc: {2:2.1f}%|{3:2.1f}%\".format(\n                            train_loss[-1],\n                            val_loss[-1],\n                            train_acc[-1] * 100,\n                            val_acc[-1] * 100,))\n\n\n                    # Early Stopping Logic\n                    val_acc_last = val_acc[-1] if len(val_acc) > 0 else 0\n                    if val_acc_last >= 0.92 and steps_to_reach_val_acc is None:\n                        steps_to_reach_val_acc = i\n\n                    # Check for stable performance\n                    if val_acc_last > 0.9:\n                        stable_steps += 1\n                    else:\n                        stable_steps = 0  # Reset counter if accuracy drops below 0.85\n\n                    if stable_steps >= stable_threshold and val_acc_last >= 0.9 and steps_to_reach_val_acc is not None:\n                        reached_early_stop = True\n                        print(f\"Validation accuracy of 0.92 reached and remained > 0.9 for {stable_threshold} steps at step {i}\")\n\n            pbar.update(1)\n\n    # Save results\n    specific_result_dir = f\"algo_{args.label}.pt\"\n    results_filename = os.path.join(results_dir, specific_result_dir)\n    torch.save(\n        {\n            \"its\": its,\n            \"train_acc\": train_acc,\n            \"train_loss\": train_loss,\n            \"val_acc\": val_acc,\n            \"val_loss\": val_loss,\n            \"steps_to_reach\": steps_to_reach_val_acc,\n            \"model_state_dict\": model.state_dict(),\n        },\n        results_filename,\n    )\n    print(f\"Saving to {results_filename}\")\n    print(f\"\\nTraining complete!\")\n    print(f\"Steps to reach 0.92 validation accuracy: {steps_to_reach_val_acc}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.251746Z","iopub.execute_input":"2025-02-12T04:01:18.252019Z","iopub.status.idle":"2025-02-12T04:01:18.272146Z","shell.execute_reply.started":"2025-02-12T04:01:18.251983Z","shell.execute_reply":"2025-02-12T04:01:18.271590Z"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1737370352760,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"LWPwaBWpSF52","trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Remove the extra arguments passed by the Jupyter Notebook kernel\nsys.argv = [\"\"]\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.272812Z","iopub.execute_input":"2025-02-12T04:01:18.272999Z","iopub.status.idle":"2025-02-12T04:01:18.294306Z","shell.execute_reply.started":"2025-02-12T04:01:18.272976Z","shell.execute_reply":"2025-02-12T04:01:18.293668Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_results(label, results_dir=\"/kaggle/working/results/algo_online\"):\n    \"\"\"\n    Loads model results, extracts accuracy/loss data, and generates plots.\n\n    Args:\n        label (str): Label identifier for the results file.\n        results_dir (str): Directory where results are stored.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Define file paths\n    filename = f\"algo_{label}.pt\"  # Adjusted filename format\n    results_filename = os.path.join(results_dir, filename)\n\n    filename_plot_acc = f\"algo_{label}_acc.png\"\n    results_filename_plot_acc = os.path.join(results_dir, filename_plot_acc)\n\n    filename_plot_loss = f\"algo_{label}_loss.png\"\n    results_filename_plot_loss = os.path.join(results_dir, filename_plot_loss)\n\n    try:\n        # Load results\n        results = torch.load(results_filename)  # Removed invalid weights_only=True\n\n        # Extract data\n        its = results[\"its\"]  # Optimization steps\n        train_acc = results[\"train_acc\"]  # Training accuracy\n        val_acc = results[\"val_acc\"]  # Validation accuracy\n        train_loss = results[\"train_loss\"]  # Training loss\n        val_loss = results[\"val_loss\"]  # Validation loss\n        steps_to_reach = results.get(\"steps_to_reach\", None)  # Handle missing key\n\n        if steps_to_reach:\n            print(f\"Steps needed to reach 0.9 validation accuracy: {steps_to_reach}\")\n\n        # Plot Accuracy\n        plt.figure()\n        plt.plot(its, train_acc, label=\"Train Accuracy\", color=\"blue\")\n        plt.plot(its, val_acc, label=\"Validation Accuracy\", color=\"red\")\n\n        # Find and annotate the maximum validation accuracy\n        max_val_acc = max(val_acc)\n        max_val_idx = val_acc.index(max_val_acc)\n        plt.annotate(f\"Max Val Acc: {max_val_acc:.4f}\",\n                     (its[max_val_idx], max_val_acc),\n                     textcoords=\"offset points\",\n                     xytext=(0, 10),\n                     ha='center',\n                     fontsize=10,\n                     color='red')\n\n        plt.legend()\n        plt.title(f\"Accuracy - {label}\")\n        plt.xlabel(\"Optimization Steps\")\n        plt.ylabel(\"Accuracy\")\n        plt.xscale(\"log\")\n        plt.grid()\n        if steps_to_reach:\n            plt.figtext(0.5, -0.1, f\"Steps to reach val=0.9 = {steps_to_reach}\",\n                        ha=\"center\", fontsize=10, style=\"italic\")\n\n        plt.savefig(results_filename_plot_acc, dpi=150)\n        plt.show()\n        plt.close()\n\n        print(f\"Plots saved successfully at {results_filename_plot_acc}\")\n\n    except FileNotFoundError:\n        print(f\"Error: Results file {results_filename} not found.\")\n    except Exception as e:\n        print(f\"Error while processing {label}: {e}\")\n\ndef plot_all_experiments_together(labels, results_dir=\"/kaggle/working/results/algo_online\", show_only_val=False):\n    \"\"\"\n    Plots train and validation accuracy for multiple experiments in a single graph.\n    Allows showing only validation accuracy if `show_only_val=True`.\n\n    Args:\n        labels (list of str): List of labels corresponding to result files.\n        results_dir (str): Directory where results are stored.\n        show_only_val (bool): If True, only plots validation accuracy.\n\n    Returns:\n        None\n    \"\"\"\n    plt.figure(figsize=(10, 6))  # Set figure size\n\n    # Generate distinct colors for each experiment\n    base_colors = plt.cm.viridis(np.linspace(0, 1, len(labels)))\n\n    for i, label in enumerate(labels):\n        results_filename = os.path.join(results_dir, f\"algo_{label}.pt\")\n\n        try:\n            # Load results\n            results = torch.load(results_filename)\n            its = results[\"its\"]\n            train_acc = results[\"train_acc\"]\n            val_acc = results[\"val_acc\"]\n            steps_to_reach = results.get(\"steps_to_reach\", None)  # Handle missing key\n\n            # Assign colors for train and validation curves\n            val_color = base_colors[i]  # Primary color for validation\n            train_color = tuple(c * 0.7 for c in base_colors[i])  # Slightly darker shade for train\n\n            # Plot validation accuracy (always shown)\n            plt.plot(its, val_acc, label=f\"Validation ({label})\", color=val_color, linestyle=\"solid\")\n\n            if steps_to_reach:\n                plt.figtext(0.5, -0.1, f\"Steps to reach val=0.9 = {steps_to_reach}\",\n                            ha=\"center\", fontsize=10, style=\"italic\")\n\n            # Plot train accuracy if `show_only_val` is False\n            if not show_only_val:\n                plt.plot(its, train_acc, label=f\"Train ({label})\", color=train_color, linestyle=\"dashed\")\n\n        except FileNotFoundError:\n            print(f\"Warning: Results file {results_filename} not found.\")\n        except Exception as e:\n            print(f\"Error while processing {label}: {e}\")\n\n    plt.legend()\n    plt.title(\"Train & Validation Accuracy for Multiple Experiments\" if not show_only_val else \"Validation Accuracy for Multiple Experiments\")\n    plt.xlabel(\"Optimization Steps\")\n    plt.ylabel(\"Accuracy\")\n    plt.xscale(\"log\")\n    plt.grid()\n\n    # Save and show the plot\n    filename = \"combined_train_val_plot.png\" if not show_only_val else \"combined_val_plot.png\"\n    plot_path = os.path.join(results_dir, filename)\n    plt.savefig(plot_path, dpi=150)\n    plt.show()\n\n    print(f\"Combined plot saved at {plot_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.295146Z","iopub.execute_input":"2025-02-12T04:01:18.295418Z","iopub.status.idle":"2025-02-12T04:01:18.311804Z","shell.execute_reply.started":"2025-02-12T04:01:18.295389Z","shell.execute_reply":"2025-02-12T04:01:18.311227Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Execute training (by running main function)","metadata":{}},{"cell_type":"markdown","source":"From now on i just train networks with different configurations every timeand then I print their results after.","metadata":{}},{"cell_type":"markdown","source":"### Simple training:\n\n    * no grokfast applied\n    * no filtering\n    * wd = 0\n\n\n\n            ","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Same as used in paper of Grokfast\n    parser = ArgumentParser(description=\"Train a model on Algorithmic Dataset without custom sampling\")\n\n    parser.add_argument(\"--label\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--p\", type=int, default=97)\n    parser.add_argument(\"--budget\", type=int, default=3e5)\n    parser.add_argument(\"--batch_size\", type=int, default=512)\n    parser.add_argument(\"--optimizer\", type=str, default=\"Adam\")\n    parser.add_argument(\"--beta1\", type=float, default=0.9)\n    parser.add_argument(\"--beta2\", type=float, default=0.98)\n    parser.add_argument(\"--weight_decay\", type=float, default=0)\n    parser.add_argument(\"--lr\", type=float, default=1e-3)\n\n\n    # Grokfast\n    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"none\")\n    parser.add_argument(\"--alpha\", type=float, default=0.99)\n    parser.add_argument(\"--window_size\", type=int, default=100)\n    parser.add_argument(\"--lamb\", type=float, default=5.0)\n\n    # Ablation studies\n    parser.add_argument(\"--two_stage\", action='store_true')\n    parser.add_argument(\"--save_weights\", action='store_true')\n\n    # Samples ranking\n    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n\n    # Boolean arguements need this due to bad behavior of parser.parse_args\n    def boolean_string(s):\n        if s not in {\"False\", \"True\"}:\n            raise ValueError(\"Not a valid boolean string\")\n        return s == \"True\"\n\n    # These are the hyperparameters related to our online sampling filtering algorithm\n    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n    parser.add_argument(\"--start\", type=int, default=1000) # When to start the sample filtering\n    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.7)  # Probability of selecting a sample from the top-k\n    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n\n\n    # -----------------------------------------------------------------\n    # Try different hyperparameter values for your grid search here\n    # -----------------------------------------------------------------\n    args = parser.parse_args(\n        [\n            \"--appl_sampl_filter\", \"False\",\n            \"--sampling_distr_upd_freq\", \"1\",\n            \"--top_k\", \"0.1\",\n            \"--top_k_sampling_prob\", \"0.7\",\n            \"--high_freq_better\", \"True\",\n            \"--filter\", \"none\",\n        ]\n    )\n    # -----------------------------------------------------------------\n    # -----------------------------------------------------------------\n\n    # Create arg.label for the filename of the saved results\n    if not args.appl_sampl_filter:\n        args.label = f\"filter{args.filter}_sampling_{args.appl_sampl_filter}\"\n    else:\n        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n\n    # Training with time recording\n\n    # Start the timer\n    start_time = time.time()\n\n    # Call your training function\n    main(args)\n\n    # End the timer\n    end_time = time.time()\n\n    # Calculate elapsed time\n    elapsed_time = end_time - start_time\n\n    # Convert to minutes and seconds (optional)\n    minutes, seconds = divmod(elapsed_time, 60)\n\n    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n    print(f\"label:{args.label}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:01:18.312691Z","iopub.execute_input":"2025-02-12T04:01:18.312921Z","iopub.status.idle":"2025-02-12T04:09:12.096726Z","shell.execute_reply.started":"2025-02-12T04:01:18.312901Z","shell.execute_reply":"2025-02-12T04:09:12.095408Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Decoder(\n  (token_embeddings): Embedding(99, 128)\n  (position_embeddings): Embedding(5, 128)\n  (layers): ModuleList(\n    (0-1): 2 x Block(\n      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n    )\n  )\n  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=128, out_features=99, bias=False)\n)\nTotal number of parameters: 422784\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15789 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9b50b3e7a2408798c9130ebed7d1ec"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-85f9e1aae70e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Call your training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# End the timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-a90a675fecd8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                         \u001b[0;31m# calculate loss only on the answer part of the equation (last element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-8cc41acea24a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-bf49ee86af6b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             )\n\u001b[1;32m   1367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1369\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6276\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6278\u001b[0;31m         attn_output = scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m   6279\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6280\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"# results_filename = \"/kaggle/working/results/algo_online/algo_filternone_sampling_False.pt\" # Not needed because analyze_results has it inside\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T04:00:26.739603Z","iopub.status.idle":"2025-02-12T04:00:26.739876Z","shell.execute_reply":"2025-02-12T04:00:26.739772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# args.label = \"filternone_sampling_False\" # Not needed because we already have args.label from above\nanalyze_results(args.label)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:49:27.716816Z","iopub.execute_input":"2025-02-12T03:49:27.717078Z","iopub.status.idle":"2025-02-12T03:49:27.966292Z","shell.execute_reply.started":"2025-02-12T03:49:27.717047Z","shell.execute_reply":"2025-02-12T03:49:27.965477Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-199-916eb565f695>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  results = torch.load(results_filename)  # Removed invalid weights_only=True\n","output_type":"stream"},{"name":"stdout","text":"Error while processing filternone_sampling_False: x and y must have same first dimension, but have shapes (15789,) and (157890,)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":202},{"cell_type":"markdown","source":"### Only Grokfast\n    * grokfast applied\n    * no filtering\n    * wd = 0","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Same as used in paper of Grokfast\n    parser = ArgumentParser(description=\"Train a model on MNIST without custom sampling\")\n\n    parser.add_argument(\"--label\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--p\", type=int, default=97)\n    parser.add_argument(\"--budget\", type=int, default=3e5)\n    parser.add_argument(\"--batch_size\", type=int, default=512)\n    parser.add_argument(\"--optimizer\", type=str, default=\"Adam\")\n    parser.add_argument(\"--beta1\", type=float, default=0.9)\n    parser.add_argument(\"--beta2\", type=float, default=0.98)\n    parser.add_argument(\"--weight_decay\", type=float, default=0)\n    parser.add_argument(\"--lr\", type=float, default=1e-3)\n\n    # Grokfast\n    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"ema\")\n    parser.add_argument(\"--alpha\", type=float, default=0.8)\n    parser.add_argument(\"--lamb\", type=float, default=0.1)\n\n    # Samples ranking\n    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n\n    # Boolean arguements need this due to bad behavior of parser.parse_args\n    def boolean_string(s):\n        if s not in {\"False\", \"True\"}:\n            raise ValueError(\"Not a valid boolean string\")\n        return s == \"True\"\n\n    # These are the hyperparameters related to our online sampling filtering algorithm\n    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n    parser.add_argument(\"--start\", type=int, default=1000) # When to start the sample filtering\n    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.7)  # Probability of selecting a sample from the top-k\n    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n\n    # -----------------------------------------------------------------\n    # Try different hyperparameter values for your grid search here\n    # -----------------------------------------------------------------\n    args = parser.parse_args(\n        [\n            \"--appl_sampl_filter\", \"False\",\n            \"--sampling_distr_upd_freq\", \"1\",\n            \"--top_k\", \"0.1\",\n            \"--top_k_sampling_prob\", \"0.7\",\n            \"--high_freq_better\", \"True\",\n        ]\n    )\n    # -----------------------------------------------------------------\n    # -----------------------------------------------------------------\n\n    # Create arg.label for the filename of the saved results\n    if not args.appl_sampl_filter:\n        args.label = f\"filter{args.filter}_sampling_{args.appl_sampl_filter}\"\n    else:\n        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n\n    # Training with time recording\n\n    # Start the timer\n    start_time = time.time()\n\n    # Call your training function\n    main(args)\n\n    # End the timer\n    end_time = time.time()\n\n    # Calculate elapsed time\n    elapsed_time = end_time - start_time\n\n    # Convert to minutes and seconds (optional)\n    minutes, seconds = divmod(elapsed_time, 60)\n\n    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n    print(f\"label:{args.label}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-02-11T14:20:21.967367Z","iopub.status.idle":"2025-02-11T14:20:21.967709Z","shell.execute_reply":"2025-02-11T14:20:21.967544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# args.label=\"filterema_sampling_False\" # Not needed because we already have args.label from above\nanalyze_results(args.label)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Grid Search","metadata":{}},{"cell_type":"code","source":"# Define possible values for each parameter\nparam_grid = {\n    \"top_k\": [0.2],  # Convert to float\n    \"top_k_sampling_prob\": [0.7],  # Convert to float\n    \"sampling_distr_upd_freq\": [10],  # Convert to int\n    \"high_freq_better\": [True]  # Boolean parameter\n}\n\n# Generate all combinations of parameters\nparam_combinations = list(itertools.product(*param_grid.values()))\n\n# Run main in a loop for each combination\nfor param_values in param_combinations:\n    # Extract parameter values\n    top_k = param_values[0]\n    top_k_sampling_prob = param_values[1]\n    sampling_distr_upd_freq = param_values[2]\n    high_freq_better = param_values[3]  # Boolean value\n\n    # Ensure boolean values are correctly formatted as strings for argparse\n    high_freq_better_str = \"True\" if high_freq_better else \"False\"\n\n    # Create args dynamically\n    args_list = [\n        \"--appl_sampl_filter\" , \"True\",\n        \"--top_k\", str(top_k),\n        \"--top_k_sampling_prob\", str(top_k_sampling_prob),\n        \"--sampling_distr_upd_freq\", str(sampling_distr_upd_freq),\n        \"--high_freq_better\", high_freq_better_str,\n        \"--label\", f\"{top_k}_{top_k_sampling_prob}_{sampling_distr_upd_freq}_{high_freq_better}\"\n    ]\n\n    # Debug print statement\n    print(f\"\\nRunning with parameters: {args_list}\")\n\n    # Parse the arguments dynamically\n    args = parser.parse_args(args_list)\n\n    # Call main() with the updated args\n    main(args)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#/kaggle/working/results/mnist_online/mnist_high_freq_True_top_k_0.2_top_k_prob_0.9_upd_freq_1.pt\n# args.label=\"high_freq_True_top_k_0.2_top_k_prob_0.9_upd_freq_1\" # Not needed because we already have args.label from above\nanalyze_results(args.label)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Grid search for high_freq_better=False\nno sure if the code is ok","metadata":{}},{"cell_type":"code","source":"args_list = [\n    \"--appl_sampl_filter\" , \"False\",\n]\n\n# Parse the arguments dynamically\nargs = parser.parse_args(args_list)\n\n# Call main() with the updated args\nmain(args)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-11T09:43:00.339715Z","iopub.status.idle":"2025-02-11T09:43:00.340111Z","shell.execute_reply":"2025-02-11T09:43:00.339922Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"analyze_results(args.label)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}