{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":892850,"sourceType":"datasetVersion","datasetId":476917},{"sourceId":10611406,"sourceType":"datasetVersion","datasetId":6569340}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"authorship_tag":"ABX9TyMSJa3XxLl7rvVV5eF0x78J","gpuType":"T4","provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"064bec9d457349bb9fa6a559339e9263":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9bf43e30b5a499a8de4604dc3f10ee2","IPY_MODEL_17166645b0b14019a9f5fe3d70b6caa5","IPY_MODEL_e451f4ad11bc44f8ab881eed658a8ee3"],"layout":"IPY_MODEL_ceb7ec9cff0049c098ac4b5fc5b6bd3c"}},"07c3736f38804f78bea83bb64c695cac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ff27ff9889744e6af586debee44669c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"17166645b0b14019a9f5fe3d70b6caa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1776d91874314cd0ab3df42e1b77bd5d","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_915f46f7e6694bf0b14a91ced2256596","value":100}},"1776d91874314cd0ab3df42e1b77bd5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c00be62ed5f48868daac8e39944ba63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_740bfc3868e947e68c87f28690b96945","placeholder":"​","style":"IPY_MODEL_73b20e21e4394b6c9326c75594d6f062","value":"Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"}},"30e1fcaaa18c40fcbb2182e33df28ea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c0b6a5234ea48e3bf8fa9fb04e7442d","placeholder":"​","style":"IPY_MODEL_ccfc9ef4719e4aef9379f11160003ce1","value":" 100/100 [04:53&lt;00:00,  3.31s/it]"}},"328beb9c29ed4dc094b6e476734a725f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36d9e94b822a4a5bb069cd474608bdb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37733b27dcd64ffaafd07959bc847f26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a3e587d4127443eb3bbae84621d4d06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eb47dc8d09747718044ca73cfadc307":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_936aa9172b2d4e0aa2f0cb84c0dabfad","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07c3736f38804f78bea83bb64c695cac","value":100}},"5c0b6a5234ea48e3bf8fa9fb04e7442d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b20e21e4394b6c9326c75594d6f062":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"740bfc3868e947e68c87f28690b96945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"915f46f7e6694bf0b14a91ced2256596":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"936aa9172b2d4e0aa2f0cb84c0dabfad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9bf43e30b5a499a8de4604dc3f10ee2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a3e587d4127443eb3bbae84621d4d06","placeholder":"​","style":"IPY_MODEL_37733b27dcd64ffaafd07959bc847f26","value":"Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"}},"ccfc9ef4719e4aef9379f11160003ce1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceb7ec9cff0049c098ac4b5fc5b6bd3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e451f4ad11bc44f8ab881eed658a8ee3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_328beb9c29ed4dc094b6e476734a725f","placeholder":"​","style":"IPY_MODEL_36d9e94b822a4a5bb069cd474608bdb6","value":" 100/100 [04:51&lt;00:00,  3.11s/it]"}},"fe873b542e0243ebbf591f5bc21ccd2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c00be62ed5f48868daac8e39944ba63","IPY_MODEL_4eb47dc8d09747718044ca73cfadc307","IPY_MODEL_30e1fcaaa18c40fcbb2182e33df28ea1"],"layout":"IPY_MODEL_0ff27ff9889744e6af586debee44669c"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Idea 4: Samples fitering - online\n\n## Περιγραφή του αλγορίθμου\n- Γίνεται εκπαίδευση με grokfast - EMA. Όταν **appl_sampl_filter** is False έχω μόνο αυτό, ενώ για True εφαρμόζω επιπλέον και την ιδέα 4 για πιο έξυπνη επιλογή δειγμάτων.\n- Ο Dataloader έχει έναν custom sampler (WeightedRandomSampler) ο οποίος κάθε φορά διαλέγει ένα δείγμα με βάση κάποιο βάρος/πιθανότητα.\n- Στην αρχή τα βάρη είναι όλα ίδια (ομοιόμορφη κατανομή) οπότε ο Dataloader λειτουργεί όπως συνήθως διαλέγοντας τυχαία ένα sample.\n- Σε κάθε επανάληψη φτιάχνεται ένα ranking των δειγμάτων (με βάση του πόσο high frequency περιέχει το καθένα) το οποίο χρησιμοποιείται για να αποφασιστεί τι βάρος/πιθανότητα θα δοθεί σε κάθε δείγμα να επιλεγεί για εκπαίδευση. Το διάνυσμα βαρών/πιθανοτήτων ανανεώνεται κάθε **sampling_distr_upd_freq** επαναλήψεις.\n- Στην κατασκευή του διανύσματος βαρών από την συνολική πιθανότητα 1 δίνουμε στα **top_k** δείγματα συνολικά **top_k_sampling_prob** (και στα υπόλοιπα length(dataset) - **top_k** δείγματα δίνουμε συνολικά το υπόλοιπο 1 - **top_k_sampling_prob**).\n- Με **high_freq_better** is True ακολουθούμε την αρχική μας υπόθεση ότι τα δείγματα με high frequency είναι αυτά που θα πρέπει να ταΐσουμε το δίκτυο περισσότερο για να μάθει γρηγορότερα, για False γίνεται το αντίθετο.\n\n## Οδηγίες χρήσης για τρέξιμο\nΠήγαινε στον τίτλο **Execute training (by running main funciton)**. Πήγαινε στο parser.parse_args και όρισε τις τιμές που θες να δοκιμάσεις για grid search. Οι υπερπαράμετροι που σχετίζονται με την ιδέα 4 online είναι:\n\n- **top_k**\n- **top_k_sampling_prob**\n- **high_freq_better**\n- **sampling_distr_upd_freq**: Μάλλον είναι οκ στο 1 γιατί ακόμα και έτσι η εκπαίδευση δεν είναι αργή οπότε δεν έχω λόγο να το αυξήσω.\n\nΑν κάποιος θέλει να τρέξει κάποιες τιμές για το grid search, έχω βάλει στον φάκελο και ένα αρχείο για να σημειώνουμε τις τιμές των υπερπαραμέτρων που δοκίμασε ο καθένας για να μην τρέχουμε όλοι τα ίδια. Βάλτε GPU P100 (νομίζω είναι ελαφρώς καλύτερη), εμένα για τα 100.000 βήματα που έχω βάλει να είναι το default ένα τρέξιμο που κάνω μόνο με grokfast (δηλαδή **appl_sampl_filter** is False) παίρνει περίπου **7 λεπτά** οπότε καλά είμαστε από χρόνο.\n\n","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Maybe this is needed if you want to import private datasets \n# kagglehub.login()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24569,"status":"ok","timestamp":1737367419894,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"f4s6HWPGPSBJ","outputId":"be8dc80d-eeb5-492c-f36e-15ba319dec89","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:38.410779Z","iopub.execute_input":"2025-02-11T14:18:38.411082Z","iopub.status.idle":"2025-02-11T14:18:40.120926Z","shell.execute_reply.started":"2025-02-11T14:18:38.411056Z","shell.execute_reply":"2025-02-11T14:18:40.120065Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\n# hojjatk_mnist_dataset_path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n\n# The dataset was uploaded from me but I made it public so you too can probably load it with this line\n# _ = kagglehub.dataset_download(\"konstantinosbarkas/mnist-dataset-processed-from-local\")\n\n# print(\"Data source import complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:40.122059Z","iopub.execute_input":"2025-02-11T14:18:40.122537Z","iopub.status.idle":"2025-02-11T14:18:40.126294Z","shell.execute_reply.started":"2025-02-11T14:18:40.122506Z","shell.execute_reply":"2025-02-11T14:18:40.125352Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import sys, os\n# Install the Grokfast library\n!wget https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\n\nsys.path.append(\"/kaggle/working\")\nos.makedirs('/kaggle/working/results/algo_online', exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:40.127846Z","iopub.execute_input":"2025-02-11T14:18:40.128143Z","iopub.status.idle":"2025-02-11T14:18:40.500361Z","shell.execute_reply.started":"2025-02-11T14:18:40.128116Z","shell.execute_reply":"2025-02-11T14:18:40.499528Z"}},"outputs":[{"name":"stdout","text":"--2025-02-11 14:18:40--  https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1703 (1.7K) [text/plain]\nSaving to: ‘grokfast.py’\n\ngrokfast.py         100%[===================>]   1.66K  --.-KB/s    in 0s      \n\n2025-02-11 14:18:40 (26.3 MB/s) - ‘grokfast.py’ saved [1703/1703]\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# import argparse\n# import gzip\nimport math\nimport random\n# import struct\nimport time\nfrom argparse import ArgumentParser\n# from collections import Counter, defaultdict, deque\nfrom itertools import islice\n# from pathlib import Path\n# from typing import Dict, List, Literal, Optional\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom functorch import grad, vmap\nfrom sklearn.model_selection import train_test_split\nfrom torch.autograd import grad\n\n# from torch.nn.utils.stateless import functional_call, # This is deprecated, use the next one instead\nfrom torch.func import functional_call\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler, Dataset\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom itertools import islice\nimport torch.nn.functional as F\n\nfrom grokfast import gradfilter_ema,gradfilter_ma\n","metadata":{"executionInfo":{"elapsed":36273,"status":"ok","timestamp":1737367466050,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"QLUi9XZMRpId","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:40.502071Z","iopub.execute_input":"2025-02-11T14:18:40.502465Z","iopub.status.idle":"2025-02-11T14:18:43.614210Z","shell.execute_reply.started":"2025-02-11T14:18:40.502426Z","shell.execute_reply":"2025-02-11T14:18:43.613564Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Enables detailed CUDA error messages\n\nresults_dir = \"/kaggle/working/results/algo_online\"\nos.makedirs(results_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.615040Z","iopub.execute_input":"2025-02-11T14:18:43.615566Z","iopub.status.idle":"2025-02-11T14:18:43.620106Z","shell.execute_reply.started":"2025-02-11T14:18:43.615532Z","shell.execute_reply":"2025-02-11T14:18:43.619205Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.621201Z","iopub.execute_input":"2025-02-11T14:18:43.622028Z","iopub.status.idle":"2025-02-11T14:18:43.687188Z","shell.execute_reply.started":"2025-02-11T14:18:43.622003Z","shell.execute_reply":"2025-02-11T14:18:43.686316Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"optimizer_dict = {\"AdamW\": torch.optim.AdamW, \"Adam\": torch.optim.Adam, \"SGD\": torch.optim.SGD}\n\nactivation_dict = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"Sigmoid\": nn.Sigmoid, \"GELU\": nn.GELU}\n\nloss_function_dict = {\"MSE\": nn.MSELoss, \"CrossEntropy\": nn.CrossEntropyLoss}\n","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1737367466051,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"8hhZpMIuSC4q","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.688131Z","iopub.execute_input":"2025-02-11T14:18:43.688474Z","iopub.status.idle":"2025-02-11T14:18:43.702301Z","shell.execute_reply.started":"2025-02-11T14:18:43.688440Z","shell.execute_reply":"2025-02-11T14:18:43.701457Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\"Causal transformer block\n    \"\"\"\n\n    def __init__(self, dim, num_heads):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(dim)\n        self.ln_2 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, num_heads)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim),\n        )\n\n    def forward(self, x):\n        attn_mask = torch.full(\n            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n        )\n        attn_mask = torch.triu(attn_mask, diagonal=1)\n        attn_mask[torch.isnan(attn_mask)] = 0.0 # fixes all 'nan' on 'mps' device\n\n        x = self.ln_1(x)\n        a, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n        x = x + a\n        m = self.mlp(self.ln_2(x))\n        x = x + m\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.703085Z","iopub.execute_input":"2025-02-11T14:18:43.703337Z","iopub.status.idle":"2025-02-11T14:18:43.717775Z","shell.execute_reply.started":"2025-02-11T14:18:43.703306Z","shell.execute_reply":"2025-02-11T14:18:43.716862Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \"\"\"Causal Transformer decoder\n    \"\"\"\n\n    def __init__(self, dim=128, num_layers=2, num_heads=4, num_tokens=97, seq_len=5):\n        super().__init__()\n        self.token_embeddings = nn.Embedding(num_tokens, dim)\n        self.position_embeddings = nn.Embedding(seq_len, dim)\n        self.layers = nn.ModuleList()\n        for _ in range(num_layers):\n            self.layers.append(Block(dim, num_heads))\n\n        self.ln_f = nn.LayerNorm(dim)\n        self.head = nn.Linear(dim, num_tokens, bias=False)\n\n    def forward(self, x):\n        # Ensure input tensor x contains valid token IDs\n        x = torch.clamp(x, 0, self.token_embeddings.num_embeddings - 1)\n\n        h = self.token_embeddings(x)\n        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n        positions = torch.clamp(positions, 0, self.position_embeddings.num_embeddings - 1)\n\n        h = h + self.position_embeddings(positions).expand_as(h)\n        for layer in self.layers:\n            h = layer(h)\n\n        h = self.ln_f(h)\n        logits = self.head(h)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.720602Z","iopub.execute_input":"2025-02-11T14:18:43.720829Z","iopub.status.idle":"2025-02-11T14:18:43.734216Z","shell.execute_reply.started":"2025-02-11T14:18:43.720810Z","shell.execute_reply":"2025-02-11T14:18:43.733339Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def multiplication_mod_p_data(p, eq_token, op_token):\n    \"\"\"x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n    \"\"\"\n    x = torch.arange(p)\n    y = torch.arange(1, p)\n    x, y = torch.cartesian_prod(x, y).T\n\n    eq = torch.ones_like(x) * eq_token\n    op = torch.ones_like(x) * op_token\n    result = x * y % p\n\n    # Initialize num_updates and variance fields\n    num_updates = torch.zeros_like(x)\n    variance = torch.zeros_like(x)\n\n    return torch.stack([x, op, y, eq, num_updates, variance, result])\n","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1737367466051,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"RIifrNowR89e","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.735543Z","iopub.execute_input":"2025-02-11T14:18:43.735790Z","iopub.status.idle":"2025-02-11T14:18:43.749007Z","shell.execute_reply.started":"2025-02-11T14:18:43.735768Z","shell.execute_reply":"2025-02-11T14:18:43.748220Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def cycle(iterable):\n    while True:\n        for x in iterable:\n            yield x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.749800Z","iopub.execute_input":"2025-02-11T14:18:43.750080Z","iopub.status.idle":"2025-02-11T14:18:43.767605Z","shell.execute_reply.started":"2025-02-11T14:18:43.750057Z","shell.execute_reply":"2025-02-11T14:18:43.766867Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def custom_collate_fn_2(batch):\n    \"\"\"Custom collate function to handle extra fields in the dataset.\"\"\"\n    images, labels, _, _ = zip(*batch)  # Ignore the indices and extra_fields for loss computation\n    images = torch.stack(images)  # Stack images into a single tensor\n    labels = torch.tensor(labels)  # Convert labels to a tensor\n    return images, labels\n","metadata":{"executionInfo":{"elapsed":212,"status":"ok","timestamp":1737367551771,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"zlciE-2nKkLg","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.768515Z","iopub.execute_input":"2025-02-11T14:18:43.768765Z","iopub.status.idle":"2025-02-11T14:18:43.782180Z","shell.execute_reply.started":"2025-02-11T14:18:43.768743Z","shell.execute_reply":"2025-02-11T14:18:43.781246Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    x = torch.stack([item[0] for item in batch])\n    op = torch.stack([item[1] for item in batch])\n    y = torch.stack([item[2] for item in batch])\n    eq = torch.stack([item[3] for item in batch])\n    result = torch.stack([item[6] for item in batch])  # Only take the result field\n    num_updates = torch.stack([item[4] for item in batch])\n    variance = torch.stack([item[5] for item in batch])\n    \n    return x, op, y, eq, result, num_updates, variance","metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1737367559166,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"zif6Q-IEjFJ7","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.783020Z","iopub.execute_input":"2025-02-11T14:18:43.783223Z","iopub.status.idle":"2025-02-11T14:18:43.795193Z","shell.execute_reply.started":"2025-02-11T14:18:43.783206Z","shell.execute_reply":"2025-02-11T14:18:43.794544Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Needed for per sample gradient computations\ndef select_random_subset(tensor, percentage, seed=42):\n    \"\"\"\n    Flatten the parameter dimensions for each batch sample, select a percentage of elements,\n    and return a tensor with shape [batch_size, selected_elements].\n\n    Args:\n        tensor (torch.Tensor): The gradient tensor of shape [batch_size, *parameter_dims].\n        percentage (float): The percentage of elements to select.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        torch.Tensor: A tensor of shape [batch_size, selected_elements].\n    \"\"\"\n    batch_size, *param_dims = tensor.shape  # Extract batch and parameter dimensions\n    total_params = torch.prod(torch.tensor(param_dims))  # Total parameters per sample\n    subset_size = int(total_params * percentage)  # 20% of parameters\n\n    # Set seed for reproducibility\n    random.seed(seed)\n    indices = random.sample(range(total_params), subset_size)  # Random indices for selection\n\n    # Flatten parameter dimensions and select elements for each batch\n    flat_tensor = tensor.view(batch_size, -1)  # Flatten parameter dimensions for each sample\n    selected_subset = flat_tensor[:, indices]  # Select the same random indices across the batch\n\n    return selected_subset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.796012Z","iopub.execute_input":"2025-02-11T14:18:43.796255Z","iopub.status.idle":"2025-02-11T14:18:43.808846Z","shell.execute_reply.started":"2025-02-11T14:18:43.796223Z","shell.execute_reply":"2025-02-11T14:18:43.808121Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def rank_to_sampling_weights(my_dataset, top_k, top_k_sampling_prob, high_freq_better):\n    \"\"\"\n    Rank samples by variance and assign sampling weights.\n\n    Parameters:\n    - my_dataset: Dataset object (each sample has 7 elements: x, op, y, eq, result, num_updates, variance).\n    - top_k: Fraction of top samples to assign higher sampling probability.\n    - top_k_sampling_prob: Probability assigned to the top_k fraction of samples.\n    - high_freq_better: If True, higher variance samples are considered better.\n\n    Returns:\n    - new_weights: List of sampling weights for each sample.\n    \"\"\"\n    # Calculate the number of top_k samples\n    num_samples = my_dataset.shape[1]  # Assuming my_dataset is a tensor of shape [7, num_samples]\n    top_k_count = int(top_k * num_samples)\n\n    # Extract the variance values for each sample\n    variance_values = my_dataset[6]  # Variance is the 7th element in each sample\n\n    # Sort indices by variance in descending or ascending order\n    sorted_indices = sorted(\n        range(num_samples),\n        key=lambda idx: variance_values[idx],\n        reverse=high_freq_better,\n    )\n\n    # Initialize new_weights with zeros\n    new_weights = [0.0] * num_samples\n\n    # Assign weights to the top_k samples\n    for idx in sorted_indices[:top_k_count]:\n        new_weights[idx] = top_k_sampling_prob / top_k_count\n\n    # Assign weights to the rest of the samples\n    for idx in sorted_indices[top_k_count:]:\n        new_weights[idx] = (1 - top_k_sampling_prob) / (num_samples - top_k_count)\n\n    return new_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:18:43.809712Z","iopub.execute_input":"2025-02-11T14:18:43.810022Z","iopub.status.idle":"2025-02-11T14:18:43.826418Z","shell.execute_reply.started":"2025-02-11T14:18:43.809992Z","shell.execute_reply":"2025-02-11T14:18:43.825665Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## def main","metadata":{}},{"cell_type":"code","source":"def main(args):\n    torch.manual_seed(args.seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    eq_token = args.p\n    op_token = args.p + 1\n    \n    # Create model\n    model = Decoder(\n        dim=128, num_layers=2, num_heads=4, num_tokens=args.p + 2, seq_len=5\n    ).to(device)\n    print(model)\n    nparams = sum([p.numel() for p in model.parameters() if p.requires_grad])\n    print(f'Total number of parameters: {nparams}')\n    for name, param in model.named_parameters():\n        print(name)\n\n    # Create dataset\n    data = multiplication_mod_p_data(args.p, eq_token, op_token)\n    data = data.T \n    print(\"Dataset shape:\", data.shape)  # Debug dataset shape\n    \n    # Split the dataset into training and validation sets\n    train_idx, valid_idx = torch.randperm(data.shape[0]).split(data.shape[0] // 2)\n    train_data, valid_data = data[train_idx], data[valid_idx]\n    print(\"Train data shape:\", train_data.shape)  # Debug train data shape\n    print(\"Validation data shape:\", valid_data.shape)  # Debug validation data shape\n\n    # Create initial weights for uniform sampling\n    weights = [1.0] * len(train_data)\n    sampler = WeightedRandomSampler(weights, len(weights))\n\n    # DataLoader\n    train_loader = DataLoader(train_data, batch_size=args.batch_size, sampler=sampler, collate_fn=custom_collate_fn)\n    valid_loader = DataLoader(valid_data, batch_size=args.batch_size, collate_fn=custom_collate_fn)\n\n    data_iter = cycle(train_loader)\n    \n    # Optimizer\n    optimizer = getattr(torch.optim, args.optimizer)(\n        model.parameters(),\n        lr=args.lr,\n        weight_decay=args.weight_decay,\n        betas=(args.beta1, args.beta2),\n    )\n\n    # Scheduler\n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optimizer, lambda update: 1 if update > 10 else update / 10\n    )\n    \n    steps_per_epoch = math.ceil(data.shape[0] / args.batch_size)\n\n    # Start Training below\n    its, train_acc, val_acc, train_loss, val_loss = [], [], [], [], []\n    grads = None\n    i = 0\n\n    # For logging network weights.\n    net_its, nets = [], []\n\n    stable_steps = 0\n    stable_threshold = 10\n    reached_early_stop = False\n    steps_to_reach_val_acc = None\n\n    for e in tqdm(range(int(args.budget) // steps_per_epoch)):\n\n        # Update sampling distribution periodically\n        if args.appl_sampl_filter and e % args.sampling_distr_upd_freq == 0:\n            new_weights = rank_to_sampling_weights(train_data, args.top_k, args.top_k_sampling_prob, args.high_freq_better)\n            new_sampler = WeightedRandomSampler(new_weights, num_samples=len(train_data), replacement=True)\n            train_loader = DataLoader(\n                train_data,\n                batch_size=args.batch_size,\n                sampler=new_sampler,\n                collate_fn=custom_collate_fn\n            )\n            del data_iter\n            data_iter = cycle(train_loader)\n\n        # Training loop\n        model.train()\n        total_loss = 0\n        total_acc = 0\n        \n        for input in train_loader:\n            input = torch.stack(input, dim=1)  # Converts tuple of tensors to a single tensor (7, batch_size)\n            \n            x, op, y, eq, result, num_updates, variance = input[:, 0], input[:, 1], input[:, 2], input[:, 3], input[:, 4], input[:, 5], input[:, 6]\n            \n            # Move to device\n            x, op, y, eq, result = x.to(device), op.to(device), y.to(device), eq.to(device), result.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass: Pass only x, op, y, eq to the model\n            input_to_model = torch.stack([x, op, y, eq], dim=1) \n            \n            logits = model(input_to_model)  # Shape: [seq_len, batch_size, num_classes]\n            logits = logits[:, -1]  # Take the last token's output (shape: [batch_size, num_classes])\n            \n            # Compute loss: Use result as the target\n            loss = F.cross_entropy(logits, result.view(-1))\n            total_loss += loss.item() * input.shape[-1]\n            \n            model.zero_grad()\n            loss.backward()\n            \n            # Compute gradients and update num_updates and variance\n            with torch.no_grad():\n                variance = 0.0\n                for i, (name, param) in enumerate(model.named_parameters()):\n                    if param.grad is not None:\n                        grad = param.grad.view(-1)\n                        if i == 0:  # Only update for the first layer for simplicity\n                            variance += grad.var().item()\n                            num_updates += 1\n\n            trigger = i < 500 if args.two_stage else False\n\n            if args.filter == \"none\":\n                pass\n            elif args.filter == \"ma\":\n                grads = gradfilter_ma(model, grads=grads, window_size=args.window_size, lamb=args.lamb, trigger=trigger)\n            elif args.filter == \"ema\":\n                grads = gradfilter_ema(model, grads=grads, alpha=args.alpha, lamb=args.lamb)\n            else:\n                raise ValueError(f\"Invalid gradient filter type `{args.filter}`\")\n            \n            # Update model parameters\n            optimizer.step()\n            scheduler.step()\n            i += 1\n\n            # Compute accuracy\n            acc = (logits.argmax(-1) == result).float().mean()\n            total_acc += acc.item() * input.shape[-1]\n\n        avg_train_loss = total_loss / len(train_loader)\n        avg_train_acc = total_acc / len(train_loader)\n        train_acc.append(avg_train_acc)\n        train_loss.append(avg_train_loss)\n        its.append(i)\n\n            # Print training metrics\n        print(f\"Epoch {e + 1}/{int(args.budget) // steps_per_epoch}\")\n        print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_acc:.4f}\")\n\n\n        # Validation loop\n        model.eval()\n        total_val_loss = 0\n        total_val_acc = 0\n\n        for input in valid_loader:\n            input = torch.stack(input, dim=1)  # Converts tuple of tensors to a single tensor (7, batch_size)\n            \n            x, op, y, eq, result, num_updates, variance = input[:, 0], input[:, 1], input[:, 2], input[:, 3], input[:, 4], input[:, 5], input[:, 6]\n                \n            # Move to device\n            x, op, y, eq, result = x.to(device), op.to(device), y.to(device), eq.to(device), result.to(device)\n                \n            logits = model(torch.stack([x, op, y, eq], dim=1))  \n            logits = logits[:, -1]   # Take the last token's output (shape: [batch_size, num_classes])\n            batch_val_loss = F.cross_entropy(logits, result.view(-1))  # Use a new variable for the batch loss\n            total_val_loss += batch_val_loss.item() * input.shape[-1]\n                \n            batch_val_acc = (logits.argmax(-1) == result).float().mean()\n            total_val_acc += batch_val_acc.item() * input.shape[-1]\n\n        val_acc.append(total_val_acc / len(valid_loader))\n        val_loss.append(total_val_loss / len(valid_loader))\n        \n        # Early Stopping Logic\n        val_acc_last = val_acc[-1] if len(val_acc) > 0 else 0\n        if val_acc_last >= 0.92 and steps_to_reach_val_acc is None:\n            steps_to_reach_val_acc = i\n\n        # Check for stable performance\n        if val_acc_last > 0.9:\n            stable_steps += 1\n        else:\n            stable_steps = 0  # Reset counter if accuracy drops below 0.85\n\n        if stable_steps >= stable_threshold and val_acc_last >= 0.9 and steps_to_reach_val_acc is not None:\n            reached_early_stop = True\n            print(f\"Validation accuracy of 0.92 reached and remained > 0.9 for {stable_threshold} steps at step {i}\")\n\n        if reached_early_stop:\n            print(\"Early stopping triggered.\")\n            break\n\n    # Save results\n    specific_result_dir = f\"algo_{args.label}.pt\"\n    results_filename = os.path.join(results_dir, specific_result_dir)\n    torch.save(\n        {\n            \"its\": its,\n            \"train_acc\": train_acc,\n            \"train_loss\": train_loss,\n            \"val_acc\": val_acc,\n            \"val_loss\": val_loss,\n            \"steps_to_reach\": steps_to_reach_val_acc,\n            \"model_state_dict\": model.state_dict(),\n        },\n        results_filename,\n    )\n    print(f\"Saving to {results_filename}\")\n    print(f\"\\nTraining complete!\")\n    print(f\"Steps to reach 0.9 validation accuracy: {steps_to_reach_val_acc}\")","metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1737370352760,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"LWPwaBWpSF52","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:27:27.286878Z","iopub.execute_input":"2025-02-11T14:27:27.287162Z","iopub.status.idle":"2025-02-11T14:27:27.306376Z","shell.execute_reply.started":"2025-02-11T14:27:27.287141Z","shell.execute_reply":"2025-02-11T14:27:27.305400Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Remove the extra arguments passed by the Jupyter Notebook kernel\nsys.argv = [\"\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:27:28.336948Z","iopub.execute_input":"2025-02-11T14:27:28.337228Z","iopub.status.idle":"2025-02-11T14:27:28.340742Z","shell.execute_reply.started":"2025-02-11T14:27:28.337206Z","shell.execute_reply":"2025-02-11T14:27:28.339791Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analyze_results(label, results_dir=\"/kaggle/working/results/algo_online\"):\n    \"\"\"\n    Loads model results, extracts accuracy/loss data, and generates plots.\n\n    Args:\n        label (str): Label identifier for the results file.\n        results_dir (str): Directory where results are stored.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Define file paths\n    filename = f\"algo_{label}.pt\"  # Adjusted filename format\n    results_filename = os.path.join(results_dir, filename)\n\n    filename_plot_acc = f\"algo_{label}_acc.png\"\n    results_filename_plot_acc = os.path.join(results_dir, filename_plot_acc)\n\n    filename_plot_loss = f\"algo_{label}_loss.png\"\n    results_filename_plot_loss = os.path.join(results_dir, filename_plot_loss)\n\n    try:\n        # Load results\n        results = torch.load(results_filename)  # Removed invalid weights_only=True\n\n        # Extract data\n        its = results[\"its\"]  # Optimization steps\n        train_acc = results[\"train_acc\"]  # Training accuracy\n        val_acc = results[\"val_acc\"]  # Validation accuracy\n        train_loss = results[\"train_loss\"]  # Training loss\n        val_loss = results[\"val_loss\"]  # Validation loss\n        steps_to_reach = results.get(\"steps_to_reach\", None)  # Handle missing key\n\n        if steps_to_reach:\n            print(f\"Steps needed to reach 0.9 validation accuracy: {steps_to_reach}\")\n\n        # Plot Accuracy\n        plt.figure()\n        plt.plot(its, train_acc, label=\"Train Accuracy\", color=\"blue\")\n        plt.plot(its, val_acc, label=\"Validation Accuracy\", color=\"red\")\n\n        # Find and annotate the maximum validation accuracy\n        max_val_acc = max(val_acc)\n        max_val_idx = val_acc.index(max_val_acc)\n        plt.annotate(f\"Max Val Acc: {max_val_acc:.4f}\", \n                     (its[max_val_idx], max_val_acc), \n                     textcoords=\"offset points\", \n                     xytext=(0, 10), \n                     ha='center', \n                     fontsize=10, \n                     color='red')\n\n        plt.legend()\n        plt.title(f\"Accuracy - {label}\")\n        plt.xlabel(\"Optimization Steps\")\n        plt.ylabel(\"Accuracy\")\n        plt.xscale(\"log\")\n        plt.grid()\n        if steps_to_reach:\n            plt.figtext(0.5, -0.1, f\"Steps to reach val=0.9 = {steps_to_reach}\", \n                        ha=\"center\", fontsize=10, style=\"italic\")\n\n        plt.savefig(results_filename_plot_acc, dpi=150)\n        plt.show()\n        plt.close()\n\n        print(f\"Plots saved successfully at {results_filename_plot_acc}\")\n\n    except FileNotFoundError:\n        print(f\"Error: Results file {results_filename} not found.\")\n    except Exception as e:\n        print(f\"Error while processing {label}: {e}\")\n\ndef plot_all_experiments_together(labels, results_dir=\"/kaggle/working/results/algo_online\", show_only_val=False):\n    \"\"\"\n    Plots train and validation accuracy for multiple experiments in a single graph.\n    Allows showing only validation accuracy if `show_only_val=True`.\n\n    Args:\n        labels (list of str): List of labels corresponding to result files.\n        results_dir (str): Directory where results are stored.\n        show_only_val (bool): If True, only plots validation accuracy.\n\n    Returns:\n        None\n    \"\"\"\n    plt.figure(figsize=(10, 6))  # Set figure size\n\n    # Generate distinct colors for each experiment\n    base_colors = plt.cm.viridis(np.linspace(0, 1, len(labels)))  \n\n    for i, label in enumerate(labels):\n        results_filename = os.path.join(results_dir, f\"algo_{label}.pt\")\n\n        try:\n            # Load results\n            results = torch.load(results_filename)\n            its = results[\"its\"]\n            train_acc = results[\"train_acc\"]\n            val_acc = results[\"val_acc\"]\n            steps_to_reach = results.get(\"steps_to_reach\", None)  # Handle missing key\n\n            # Assign colors for train and validation curves\n            val_color = base_colors[i]  # Primary color for validation\n            train_color = tuple(c * 0.7 for c in base_colors[i])  # Slightly darker shade for train\n            \n            # Plot validation accuracy (always shown)\n            plt.plot(its, val_acc, label=f\"Validation ({label})\", color=val_color, linestyle=\"solid\")\n\n            if steps_to_reach:\n                plt.figtext(0.5, -0.1, f\"Steps to reach val=0.9 = {steps_to_reach}\", \n                            ha=\"center\", fontsize=10, style=\"italic\")\n        \n            # Plot train accuracy if `show_only_val` is False\n            if not show_only_val:\n                plt.plot(its, train_acc, label=f\"Train ({label})\", color=train_color, linestyle=\"dashed\")\n\n        except FileNotFoundError:\n            print(f\"Warning: Results file {results_filename} not found.\")\n        except Exception as e:\n            print(f\"Error while processing {label}: {e}\")\n\n    plt.legend()\n    plt.title(\"Train & Validation Accuracy for Multiple Experiments\" if not show_only_val else \"Validation Accuracy for Multiple Experiments\")\n    plt.xlabel(\"Optimization Steps\")\n    plt.ylabel(\"Accuracy\")\n    plt.xscale(\"log\")\n    plt.grid()\n    \n    # Save and show the plot\n    filename = \"combined_train_val_plot.png\" if not show_only_val else \"combined_val_plot.png\"\n    plot_path = os.path.join(results_dir, filename)\n    plt.savefig(plot_path, dpi=150)\n    plt.show()\n\n    print(f\"Combined plot saved at {plot_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:27:28.567443Z","iopub.execute_input":"2025-02-11T14:27:28.567741Z","iopub.status.idle":"2025-02-11T14:27:28.579893Z","shell.execute_reply.started":"2025-02-11T14:27:28.567719Z","shell.execute_reply":"2025-02-11T14:27:28.578809Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Execute training (by running main function)","metadata":{}},{"cell_type":"markdown","source":"From now on i just train networks with different configurations every timeand then I print their results after.","metadata":{}},{"cell_type":"markdown","source":"Below is Benchmark:\n\n    * no grokfast applied\n    * no filtering\n    * wd = 0\n\n\n\n            ","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Same as used in paper of Grokfast\n    parser = ArgumentParser(description=\"Train a model on Algorithmic Dataset without custom sampling\")\n    \n    parser.add_argument(\"--label\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--p\", type=int, default=97)\n    parser.add_argument(\"--budget\", type=int, default=3e5)\n    parser.add_argument(\"--train_points\", type=int, default=1000)\n    parser.add_argument(\"--optimization_steps\", type=int, default=10000)\n    parser.add_argument(\"--batch_size\", type=int, default=256)\n    parser.add_argument(\"--optimizer\", type=str, default=\"Adam\")\n    parser.add_argument(\"--beta1\", type=float, default=0.9)\n    parser.add_argument(\"--beta2\", type=float, default=0.98)\n    parser.add_argument(\"--weight_decay\", type=float, default=0)\n    parser.add_argument(\"--lr\", type=float, default=1e-3)\n    parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n    parser.add_argument(\"--download_directory\", type=str, default=\".\")\n    parser.add_argument(\"--depth\", type=int, default=3)\n    parser.add_argument(\"--width\", type=int, default=200)\n    parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n\n    # Grokfast\n    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"none\")\n    parser.add_argument(\"--alpha\", type=float, default=0.99)\n    parser.add_argument(\"--window_size\", type=int, default=100)\n    parser.add_argument(\"--lamb\", type=float, default=0.1)\n\n    # Ablation studies\n    parser.add_argument(\"--two_stage\", action='store_true')\n    parser.add_argument(\"--save_weights\", action='store_true')\n\n    # Samples ranking\n    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n\n    # Boolean arguements need this due to bad behavior of parser.parse_args\n    def boolean_string(s):\n        if s not in {\"False\", \"True\"}:\n            raise ValueError(\"Not a valid boolean string\")\n        return s == \"True\"\n\n    # These are the hyperparameters related to our online sampling filtering algorithm\n    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n\n    # -----------------------------------------------------------------\n    # Try different hyperparameter values for your grid search here\n    # -----------------------------------------------------------------\n    args = parser.parse_args(\n        [\n            \"--appl_sampl_filter\", \"False\", # booleans as non strings in order to work\n            \"--sampling_distr_upd_freq\", \"1\", # the rest as strings for some reason\n            \"--top_k\", \"0.1\",\n            \"--top_k_sampling_prob\", \"0.9\",\n            \"--high_freq_better\", \"True\",\n        ]\n    )\n    # -----------------------------------------------------------------\n    # -----------------------------------------------------------------\n\n    # Create arg.label for the filename of the saved results\n    if not args.appl_sampl_filter:\n        args.label = f\"filter{args.filter}_sampling_{args.appl_sampl_filter}\"\n    else:\n        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n\n    # Training with time recording\n\n    # Start the timer\n    start_time = time.time()\n\n    # Call your training function\n    main(args)\n\n    # End the timer\n    end_time = time.time()\n\n    # Calculate elapsed time\n    elapsed_time = end_time - start_time\n\n    # Convert to minutes and seconds (optional)\n    minutes, seconds = divmod(elapsed_time, 60)\n\n    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n    print(f\"label:{args.label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:27:29.929909Z","iopub.execute_input":"2025-02-11T14:27:29.930188Z","iopub.status.idle":"2025-02-11T14:50:53.401879Z","shell.execute_reply.started":"2025-02-11T14:27:29.930166Z","shell.execute_reply":"2025-02-11T14:50:53.400546Z"}},"outputs":[{"name":"stdout","text":"Decoder(\n  (token_embeddings): Embedding(99, 128)\n  (position_embeddings): Embedding(5, 128)\n  (layers): ModuleList(\n    (0-1): 2 x Block(\n      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (mlp): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n    )\n  )\n  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=128, out_features=99, bias=False)\n)\nTotal number of parameters: 422784\ntoken_embeddings.weight\nposition_embeddings.weight\nlayers.0.ln_1.weight\nlayers.0.ln_1.bias\nlayers.0.ln_2.weight\nlayers.0.ln_2.bias\nlayers.0.attn.in_proj_weight\nlayers.0.attn.in_proj_bias\nlayers.0.attn.out_proj.weight\nlayers.0.attn.out_proj.bias\nlayers.0.mlp.0.weight\nlayers.0.mlp.0.bias\nlayers.0.mlp.2.weight\nlayers.0.mlp.2.bias\nlayers.1.ln_1.weight\nlayers.1.ln_1.bias\nlayers.1.ln_2.weight\nlayers.1.ln_2.bias\nlayers.1.attn.in_proj_weight\nlayers.1.attn.in_proj_bias\nlayers.1.attn.out_proj.weight\nlayers.1.attn.out_proj.bias\nlayers.1.mlp.0.weight\nlayers.1.mlp.0.bias\nlayers.1.mlp.2.weight\nlayers.1.mlp.2.bias\nln_f.weight\nln_f.bias\nhead.weight\nDataset shape: torch.Size([9312, 7])\nTrain data shape: torch.Size([4656, 7])\nValidation data shape: torch.Size([4656, 7])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70005dc9c6f6419cbc5d21c7283ed4ab"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/8108\nTraining Loss: 32.5083, Training Accuracy: 0.0758\nEpoch 2/8108\nTraining Loss: 32.3157, Training Accuracy: 0.0604\nEpoch 3/8108\nTraining Loss: 32.1552, Training Accuracy: 0.0734\nEpoch 4/8108\nTraining Loss: 32.1773, Training Accuracy: 0.0921\nEpoch 5/8108\nTraining Loss: 32.1445, Training Accuracy: 0.0633\nEpoch 6/8108\nTraining Loss: 32.1890, Training Accuracy: 0.0652\nEpoch 7/8108\nTraining Loss: 32.1058, Training Accuracy: 0.0911\nEpoch 8/8108\nTraining Loss: 32.1210, Training Accuracy: 0.0662\nEpoch 9/8108\nTraining Loss: 32.1268, Training Accuracy: 0.0835\nEpoch 10/8108\nTraining Loss: 32.1499, Training Accuracy: 0.0792\nEpoch 11/8108\nTraining Loss: 32.1533, Training Accuracy: 0.0590\nEpoch 12/8108\nTraining Loss: 32.1262, Training Accuracy: 0.0720\nEpoch 13/8108\nTraining Loss: 32.1059, Training Accuracy: 0.0792\nEpoch 14/8108\nTraining Loss: 32.1367, Training Accuracy: 0.0696\nEpoch 15/8108\nTraining Loss: 32.0696, Training Accuracy: 0.0935\nEpoch 16/8108\nTraining Loss: 32.0703, Training Accuracy: 0.1190\nEpoch 17/8108\nTraining Loss: 32.0836, Training Accuracy: 0.0883\nEpoch 18/8108\nTraining Loss: 32.0960, Training Accuracy: 0.0835\nEpoch 19/8108\nTraining Loss: 32.0475, Training Accuracy: 0.0892\nEpoch 20/8108\nTraining Loss: 32.0837, Training Accuracy: 0.0633\nEpoch 21/8108\nTraining Loss: 32.0850, Training Accuracy: 0.0849\nEpoch 22/8108\nTraining Loss: 32.0915, Training Accuracy: 0.0676\nEpoch 23/8108\nTraining Loss: 32.0829, Training Accuracy: 0.0868\nEpoch 24/8108\nTraining Loss: 32.0971, Training Accuracy: 0.0907\nEpoch 25/8108\nTraining Loss: 32.0797, Training Accuracy: 0.0705\nEpoch 26/8108\nTraining Loss: 32.0799, Training Accuracy: 0.0676\nEpoch 27/8108\nTraining Loss: 32.0656, Training Accuracy: 0.0691\nEpoch 28/8108\nTraining Loss: 32.0603, Training Accuracy: 0.0878\nEpoch 29/8108\nTraining Loss: 32.0619, Training Accuracy: 0.0916\nEpoch 30/8108\nTraining Loss: 32.0924, Training Accuracy: 0.0863\nEpoch 31/8108\nTraining Loss: 32.0809, Training Accuracy: 0.0691\nEpoch 32/8108\nTraining Loss: 32.0404, Training Accuracy: 0.0720\nEpoch 33/8108\nTraining Loss: 32.0665, Training Accuracy: 0.0772\nEpoch 34/8108\nTraining Loss: 32.0939, Training Accuracy: 0.0734\nEpoch 35/8108\nTraining Loss: 32.0773, Training Accuracy: 0.0768\nEpoch 36/8108\nTraining Loss: 32.0715, Training Accuracy: 0.0796\nEpoch 37/8108\nTraining Loss: 32.0593, Training Accuracy: 0.0768\nEpoch 38/8108\nTraining Loss: 32.0568, Training Accuracy: 0.0974\nEpoch 39/8108\nTraining Loss: 32.0338, Training Accuracy: 0.0720\nEpoch 40/8108\nTraining Loss: 32.0919, Training Accuracy: 0.0576\nEpoch 41/8108\nTraining Loss: 32.0666, Training Accuracy: 0.1190\nEpoch 42/8108\nTraining Loss: 32.0747, Training Accuracy: 0.0763\nEpoch 43/8108\nTraining Loss: 32.0511, Training Accuracy: 0.0907\nEpoch 44/8108\nTraining Loss: 32.0620, Training Accuracy: 0.0868\nEpoch 45/8108\nTraining Loss: 32.0500, Training Accuracy: 0.0734\nEpoch 46/8108\nTraining Loss: 32.0391, Training Accuracy: 0.1041\nEpoch 47/8108\nTraining Loss: 32.0855, Training Accuracy: 0.0806\nEpoch 48/8108\nTraining Loss: 32.0281, Training Accuracy: 0.1113\nEpoch 49/8108\nTraining Loss: 32.0617, Training Accuracy: 0.0782\nEpoch 50/8108\nTraining Loss: 32.0246, Training Accuracy: 0.0763\nEpoch 51/8108\nTraining Loss: 32.0471, Training Accuracy: 0.0863\nEpoch 52/8108\nTraining Loss: 32.0449, Training Accuracy: 0.1036\nEpoch 53/8108\nTraining Loss: 32.0393, Training Accuracy: 0.1003\nEpoch 54/8108\nTraining Loss: 32.0643, Training Accuracy: 0.0863\nEpoch 55/8108\nTraining Loss: 32.0515, Training Accuracy: 0.0504\nEpoch 56/8108\nTraining Loss: 32.0423, Training Accuracy: 0.0782\nEpoch 57/8108\nTraining Loss: 32.0420, Training Accuracy: 0.0777\nEpoch 58/8108\nTraining Loss: 32.0267, Training Accuracy: 0.0681\nEpoch 59/8108\nTraining Loss: 32.0502, Training Accuracy: 0.1127\nEpoch 60/8108\nTraining Loss: 31.9963, Training Accuracy: 0.1060\nEpoch 61/8108\nTraining Loss: 32.0466, Training Accuracy: 0.0840\nEpoch 62/8108\nTraining Loss: 32.0539, Training Accuracy: 0.0768\nEpoch 63/8108\nTraining Loss: 32.0476, Training Accuracy: 0.0811\nEpoch 64/8108\nTraining Loss: 32.0325, Training Accuracy: 0.0768\nEpoch 65/8108\nTraining Loss: 32.0427, Training Accuracy: 0.0955\nEpoch 66/8108\nTraining Loss: 32.0193, Training Accuracy: 0.0950\nEpoch 67/8108\nTraining Loss: 32.0387, Training Accuracy: 0.0955\nEpoch 68/8108\nTraining Loss: 32.0274, Training Accuracy: 0.1127\nEpoch 69/8108\nTraining Loss: 32.0175, Training Accuracy: 0.1065\nEpoch 70/8108\nTraining Loss: 32.0332, Training Accuracy: 0.0883\nEpoch 71/8108\nTraining Loss: 32.0296, Training Accuracy: 0.0720\nEpoch 72/8108\nTraining Loss: 32.0495, Training Accuracy: 0.0748\nEpoch 73/8108\nTraining Loss: 32.0463, Training Accuracy: 0.0705\nEpoch 74/8108\nTraining Loss: 32.0566, Training Accuracy: 0.0604\nEpoch 75/8108\nTraining Loss: 32.0311, Training Accuracy: 0.0748\nEpoch 76/8108\nTraining Loss: 32.0644, Training Accuracy: 0.0715\nEpoch 77/8108\nTraining Loss: 32.0354, Training Accuracy: 0.0729\nEpoch 78/8108\nTraining Loss: 32.0726, Training Accuracy: 0.0820\nEpoch 79/8108\nTraining Loss: 32.0431, Training Accuracy: 0.0777\nEpoch 80/8108\nTraining Loss: 32.0385, Training Accuracy: 0.0811\nEpoch 81/8108\nTraining Loss: 32.0696, Training Accuracy: 0.0811\nEpoch 82/8108\nTraining Loss: 32.0044, Training Accuracy: 0.1012\nEpoch 83/8108\nTraining Loss: 32.0685, Training Accuracy: 0.0576\nEpoch 84/8108\nTraining Loss: 32.0481, Training Accuracy: 0.0878\nEpoch 85/8108\nTraining Loss: 32.0398, Training Accuracy: 0.0907\nEpoch 86/8108\nTraining Loss: 32.0204, Training Accuracy: 0.0830\nEpoch 87/8108\nTraining Loss: 32.0608, Training Accuracy: 0.0911\nEpoch 88/8108\nTraining Loss: 32.0403, Training Accuracy: 0.1137\nEpoch 89/8108\nTraining Loss: 32.0392, Training Accuracy: 0.0863\nEpoch 90/8108\nTraining Loss: 31.9865, Training Accuracy: 0.0907\nEpoch 91/8108\nTraining Loss: 32.0392, Training Accuracy: 0.0849\nEpoch 92/8108\nTraining Loss: 32.0167, Training Accuracy: 0.0820\nEpoch 93/8108\nTraining Loss: 32.0200, Training Accuracy: 0.0792\nEpoch 94/8108\nTraining Loss: 32.0369, Training Accuracy: 0.0811\nEpoch 95/8108\nTraining Loss: 32.0156, Training Accuracy: 0.0926\nEpoch 96/8108\nTraining Loss: 32.0544, Training Accuracy: 0.0705\nEpoch 97/8108\nTraining Loss: 32.0219, Training Accuracy: 0.0705\nEpoch 98/8108\nTraining Loss: 32.0364, Training Accuracy: 0.0887\nEpoch 99/8108\nTraining Loss: 32.0073, Training Accuracy: 0.1055\nEpoch 100/8108\nTraining Loss: 32.0168, Training Accuracy: 0.1094\nEpoch 101/8108\nTraining Loss: 32.0428, Training Accuracy: 0.1027\nEpoch 102/8108\nTraining Loss: 32.0521, Training Accuracy: 0.0863\nEpoch 103/8108\nTraining Loss: 32.0319, Training Accuracy: 0.0993\nEpoch 104/8108\nTraining Loss: 32.0342, Training Accuracy: 0.0983\nEpoch 105/8108\nTraining Loss: 32.0200, Training Accuracy: 0.0806\nEpoch 106/8108\nTraining Loss: 32.0418, Training Accuracy: 0.0720\nEpoch 107/8108\nTraining Loss: 32.0087, Training Accuracy: 0.0849\nEpoch 108/8108\nTraining Loss: 32.0245, Training Accuracy: 0.0907\nEpoch 109/8108\nTraining Loss: 32.0235, Training Accuracy: 0.0911\nEpoch 110/8108\nTraining Loss: 32.0197, Training Accuracy: 0.0830\nEpoch 111/8108\nTraining Loss: 32.0159, Training Accuracy: 0.0998\nEpoch 112/8108\nTraining Loss: 32.0174, Training Accuracy: 0.0854\nEpoch 113/8108\nTraining Loss: 31.9964, Training Accuracy: 0.1022\nEpoch 114/8108\nTraining Loss: 32.0082, Training Accuracy: 0.1007\nEpoch 115/8108\nTraining Loss: 32.0218, Training Accuracy: 0.0969\nEpoch 116/8108\nTraining Loss: 32.0317, Training Accuracy: 0.0907\nEpoch 117/8108\nTraining Loss: 31.9930, Training Accuracy: 0.1031\nEpoch 118/8108\nTraining Loss: 32.0239, Training Accuracy: 0.0897\nEpoch 119/8108\nTraining Loss: 32.0144, Training Accuracy: 0.0959\nEpoch 120/8108\nTraining Loss: 32.0083, Training Accuracy: 0.0854\nEpoch 121/8108\nTraining Loss: 32.0425, Training Accuracy: 0.0820\nEpoch 122/8108\nTraining Loss: 32.0341, Training Accuracy: 0.0820\nEpoch 123/8108\nTraining Loss: 32.0219, Training Accuracy: 0.0835\nEpoch 124/8108\nTraining Loss: 32.0101, Training Accuracy: 0.0782\nEpoch 125/8108\nTraining Loss: 32.0139, Training Accuracy: 0.0926\nEpoch 126/8108\nTraining Loss: 32.0360, Training Accuracy: 0.0676\nEpoch 127/8108\nTraining Loss: 32.0252, Training Accuracy: 0.0619\nEpoch 128/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0863\nEpoch 129/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0806\nEpoch 130/8108\nTraining Loss: 32.0253, Training Accuracy: 0.0801\nEpoch 131/8108\nTraining Loss: 32.0303, Training Accuracy: 0.0935\nEpoch 132/8108\nTraining Loss: 32.0056, Training Accuracy: 0.1151\nEpoch 133/8108\nTraining Loss: 32.0419, Training Accuracy: 0.0792\nEpoch 134/8108\nTraining Loss: 32.0204, Training Accuracy: 0.0691\nEpoch 135/8108\nTraining Loss: 32.0121, Training Accuracy: 0.0955\nEpoch 136/8108\nTraining Loss: 32.0184, Training Accuracy: 0.0892\nEpoch 137/8108\nTraining Loss: 32.0152, Training Accuracy: 0.0748\nEpoch 138/8108\nTraining Loss: 32.0101, Training Accuracy: 0.0955\nEpoch 139/8108\nTraining Loss: 32.0474, Training Accuracy: 0.0959\nEpoch 140/8108\nTraining Loss: 32.0227, Training Accuracy: 0.1257\nEpoch 141/8108\nTraining Loss: 31.9982, Training Accuracy: 0.1041\nEpoch 142/8108\nTraining Loss: 32.0192, Training Accuracy: 0.0840\nEpoch 143/8108\nTraining Loss: 32.0217, Training Accuracy: 0.0720\nEpoch 144/8108\nTraining Loss: 32.0295, Training Accuracy: 0.0532\nEpoch 145/8108\nTraining Loss: 32.0337, Training Accuracy: 0.0825\nEpoch 146/8108\nTraining Loss: 32.0147, Training Accuracy: 0.0959\nEpoch 147/8108\nTraining Loss: 32.0158, Training Accuracy: 0.0777\nEpoch 148/8108\nTraining Loss: 32.0116, Training Accuracy: 0.0696\nEpoch 149/8108\nTraining Loss: 32.0094, Training Accuracy: 0.0720\nEpoch 150/8108\nTraining Loss: 32.0060, Training Accuracy: 0.1027\nEpoch 151/8108\nTraining Loss: 32.0301, Training Accuracy: 0.0998\nEpoch 152/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0955\nEpoch 153/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0892\nEpoch 154/8108\nTraining Loss: 32.0320, Training Accuracy: 0.0907\nEpoch 155/8108\nTraining Loss: 32.0153, Training Accuracy: 0.0705\nEpoch 156/8108\nTraining Loss: 32.0123, Training Accuracy: 0.0863\nEpoch 157/8108\nTraining Loss: 32.0103, Training Accuracy: 0.0748\nEpoch 158/8108\nTraining Loss: 32.0159, Training Accuracy: 0.0619\nEpoch 159/8108\nTraining Loss: 32.0279, Training Accuracy: 0.0720\nEpoch 160/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0811\nEpoch 161/8108\nTraining Loss: 32.0246, Training Accuracy: 0.0835\nEpoch 162/8108\nTraining Loss: 32.0300, Training Accuracy: 0.0691\nEpoch 163/8108\nTraining Loss: 32.0082, Training Accuracy: 0.0806\nEpoch 164/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0676\nEpoch 165/8108\nTraining Loss: 32.0263, Training Accuracy: 0.0926\nEpoch 166/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0633\nEpoch 167/8108\nTraining Loss: 32.0218, Training Accuracy: 0.0777\nEpoch 168/8108\nTraining Loss: 32.0089, Training Accuracy: 0.0763\nEpoch 169/8108\nTraining Loss: 32.0209, Training Accuracy: 0.0955\nEpoch 170/8108\nTraining Loss: 32.0109, Training Accuracy: 0.1108\nEpoch 171/8108\nTraining Loss: 32.0082, Training Accuracy: 0.0974\nEpoch 172/8108\nTraining Loss: 32.0141, Training Accuracy: 0.0825\nEpoch 173/8108\nTraining Loss: 31.9916, Training Accuracy: 0.1099\nEpoch 174/8108\nTraining Loss: 32.0245, Training Accuracy: 0.0840\nEpoch 175/8108\nTraining Loss: 32.0046, Training Accuracy: 0.1055\nEpoch 176/8108\nTraining Loss: 32.0275, Training Accuracy: 0.0748\nEpoch 177/8108\nTraining Loss: 32.0183, Training Accuracy: 0.0892\nEpoch 178/8108\nTraining Loss: 32.0263, Training Accuracy: 0.0902\nEpoch 179/8108\nTraining Loss: 32.0101, Training Accuracy: 0.0792\nEpoch 180/8108\nTraining Loss: 32.0263, Training Accuracy: 0.0825\nEpoch 181/8108\nTraining Loss: 32.0268, Training Accuracy: 0.0532\nEpoch 182/8108\nTraining Loss: 32.0224, Training Accuracy: 0.0796\nEpoch 183/8108\nTraining Loss: 32.0185, Training Accuracy: 0.0561\nEpoch 184/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0811\nEpoch 185/8108\nTraining Loss: 32.0216, Training Accuracy: 0.1041\nEpoch 186/8108\nTraining Loss: 32.0066, Training Accuracy: 0.0763\nEpoch 187/8108\nTraining Loss: 32.0098, Training Accuracy: 0.1007\nEpoch 188/8108\nTraining Loss: 32.0050, Training Accuracy: 0.0969\nEpoch 189/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0883\nEpoch 190/8108\nTraining Loss: 32.0169, Training Accuracy: 0.1012\nEpoch 191/8108\nTraining Loss: 32.0167, Training Accuracy: 0.0849\nEpoch 192/8108\nTraining Loss: 32.0243, Training Accuracy: 0.0883\nEpoch 193/8108\nTraining Loss: 32.0037, Training Accuracy: 0.0979\nEpoch 194/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0935\nEpoch 195/8108\nTraining Loss: 32.0217, Training Accuracy: 0.0604\nEpoch 196/8108\nTraining Loss: 32.0129, Training Accuracy: 0.0835\nEpoch 197/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0792\nEpoch 198/8108\nTraining Loss: 32.0182, Training Accuracy: 0.0811\nEpoch 199/8108\nTraining Loss: 32.0178, Training Accuracy: 0.0806\nEpoch 200/8108\nTraining Loss: 31.9829, Training Accuracy: 0.0892\nEpoch 201/8108\nTraining Loss: 32.0125, Training Accuracy: 0.0792\nEpoch 202/8108\nTraining Loss: 32.0141, Training Accuracy: 0.0518\nEpoch 203/8108\nTraining Loss: 32.0083, Training Accuracy: 0.0935\nEpoch 204/8108\nTraining Loss: 32.0194, Training Accuracy: 0.0811\nEpoch 205/8108\nTraining Loss: 32.0196, Training Accuracy: 0.0830\nEpoch 206/8108\nTraining Loss: 32.0118, Training Accuracy: 0.0959\nEpoch 207/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0868\nEpoch 208/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0998\nEpoch 209/8108\nTraining Loss: 32.0391, Training Accuracy: 0.0806\nEpoch 210/8108\nTraining Loss: 31.9949, Training Accuracy: 0.1012\nEpoch 211/8108\nTraining Loss: 31.9933, Training Accuracy: 0.1055\nEpoch 212/8108\nTraining Loss: 32.0126, Training Accuracy: 0.0868\nEpoch 213/8108\nTraining Loss: 32.0252, Training Accuracy: 0.0820\nEpoch 214/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0868\nEpoch 215/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0796\nEpoch 216/8108\nTraining Loss: 32.0190, Training Accuracy: 0.0883\nEpoch 217/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0792\nEpoch 218/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0720\nEpoch 219/8108\nTraining Loss: 32.0125, Training Accuracy: 0.0921\nEpoch 220/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0940\nEpoch 221/8108\nTraining Loss: 32.0071, Training Accuracy: 0.0806\nEpoch 222/8108\nTraining Loss: 32.0238, Training Accuracy: 0.0883\nEpoch 223/8108\nTraining Loss: 32.0047, Training Accuracy: 0.0921\nEpoch 224/8108\nTraining Loss: 31.9984, Training Accuracy: 0.1055\nEpoch 225/8108\nTraining Loss: 32.0068, Training Accuracy: 0.1079\nEpoch 226/8108\nTraining Loss: 32.0003, Training Accuracy: 0.0720\nEpoch 227/8108\nTraining Loss: 31.9889, Training Accuracy: 0.1031\nEpoch 228/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0854\nEpoch 229/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0897\nEpoch 230/8108\nTraining Loss: 32.0112, Training Accuracy: 0.0681\nEpoch 231/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0863\nEpoch 232/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0926\nEpoch 233/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0840\nEpoch 234/8108\nTraining Loss: 31.9773, Training Accuracy: 0.1017\nEpoch 235/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0964\nEpoch 236/8108\nTraining Loss: 31.9776, Training Accuracy: 0.0863\nEpoch 237/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0945\nEpoch 238/8108\nTraining Loss: 32.0037, Training Accuracy: 0.0921\nEpoch 239/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0849\nEpoch 240/8108\nTraining Loss: 32.0084, Training Accuracy: 0.0863\nEpoch 241/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0820\nEpoch 242/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0676\nEpoch 243/8108\nTraining Loss: 32.0015, Training Accuracy: 0.0921\nEpoch 244/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0782\nEpoch 245/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0897\nEpoch 246/8108\nTraining Loss: 32.0056, Training Accuracy: 0.0681\nEpoch 247/8108\nTraining Loss: 32.0299, Training Accuracy: 0.0878\nEpoch 248/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0576\nEpoch 249/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0979\nEpoch 250/8108\nTraining Loss: 31.9924, Training Accuracy: 0.1022\nEpoch 251/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0897\nEpoch 252/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0863\nEpoch 253/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0883\nEpoch 254/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0892\nEpoch 255/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0777\nEpoch 256/8108\nTraining Loss: 32.0192, Training Accuracy: 0.0796\nEpoch 257/8108\nTraining Loss: 32.0056, Training Accuracy: 0.0806\nEpoch 258/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0796\nEpoch 259/8108\nTraining Loss: 32.0135, Training Accuracy: 0.0806\nEpoch 260/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0935\nEpoch 261/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0950\nEpoch 262/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0820\nEpoch 263/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0911\nEpoch 264/8108\nTraining Loss: 31.9792, Training Accuracy: 0.0979\nEpoch 265/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0796\nEpoch 266/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0892\nEpoch 267/8108\nTraining Loss: 32.0108, Training Accuracy: 0.0792\nEpoch 268/8108\nTraining Loss: 31.9825, Training Accuracy: 0.0940\nEpoch 269/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0748\nEpoch 270/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0878\nEpoch 271/8108\nTraining Loss: 31.9810, Training Accuracy: 0.0993\nEpoch 272/8108\nTraining Loss: 32.0185, Training Accuracy: 0.0820\nEpoch 273/8108\nTraining Loss: 31.9971, Training Accuracy: 0.0868\nEpoch 274/8108\nTraining Loss: 31.9963, Training Accuracy: 0.0763\nEpoch 275/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0950\nEpoch 276/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0724\nEpoch 277/8108\nTraining Loss: 32.0009, Training Accuracy: 0.0998\nEpoch 278/8108\nTraining Loss: 32.0190, Training Accuracy: 0.0835\nEpoch 279/8108\nTraining Loss: 32.0025, Training Accuracy: 0.1031\nEpoch 280/8108\nTraining Loss: 31.9917, Training Accuracy: 0.1075\nEpoch 281/8108\nTraining Loss: 31.9843, Training Accuracy: 0.0935\nEpoch 282/8108\nTraining Loss: 32.0280, Training Accuracy: 0.0662\nEpoch 283/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0840\nEpoch 284/8108\nTraining Loss: 31.9764, Training Accuracy: 0.0835\nEpoch 285/8108\nTraining Loss: 32.0148, Training Accuracy: 0.0835\nEpoch 286/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0796\nEpoch 287/8108\nTraining Loss: 31.9880, Training Accuracy: 0.0921\nEpoch 288/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0863\nEpoch 289/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0959\nEpoch 290/8108\nTraining Loss: 32.0149, Training Accuracy: 0.1175\nEpoch 291/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0935\nEpoch 292/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0763\nEpoch 293/8108\nTraining Loss: 32.0026, Training Accuracy: 0.0720\nEpoch 294/8108\nTraining Loss: 31.9887, Training Accuracy: 0.1079\nEpoch 295/8108\nTraining Loss: 32.0146, Training Accuracy: 0.0739\nEpoch 296/8108\nTraining Loss: 32.0058, Training Accuracy: 0.1127\nEpoch 297/8108\nTraining Loss: 32.0105, Training Accuracy: 0.0868\nEpoch 298/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0902\nEpoch 299/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0969\nEpoch 300/8108\nTraining Loss: 32.0216, Training Accuracy: 0.0705\nEpoch 301/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0705\nEpoch 302/8108\nTraining Loss: 32.0026, Training Accuracy: 0.0950\nEpoch 303/8108\nTraining Loss: 32.0122, Training Accuracy: 0.0561\nEpoch 304/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0935\nEpoch 305/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0825\nEpoch 306/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0825\nEpoch 307/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0705\nEpoch 308/8108\nTraining Loss: 32.0041, Training Accuracy: 0.1094\nEpoch 309/8108\nTraining Loss: 32.0028, Training Accuracy: 0.0955\nEpoch 310/8108\nTraining Loss: 32.0100, Training Accuracy: 0.0998\nEpoch 311/8108\nTraining Loss: 32.0083, Training Accuracy: 0.0892\nEpoch 312/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0820\nEpoch 313/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0662\nEpoch 314/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0863\nEpoch 315/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0840\nEpoch 316/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0840\nEpoch 317/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0998\nEpoch 318/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0691\nEpoch 319/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0849\nEpoch 320/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0792\nEpoch 321/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0926\nEpoch 322/8108\nTraining Loss: 31.9986, Training Accuracy: 0.1017\nEpoch 323/8108\nTraining Loss: 32.0093, Training Accuracy: 0.0820\nEpoch 324/8108\nTraining Loss: 31.9842, Training Accuracy: 0.0983\nEpoch 325/8108\nTraining Loss: 32.0112, Training Accuracy: 0.0763\nEpoch 326/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0883\nEpoch 327/8108\nTraining Loss: 31.9886, Training Accuracy: 0.0897\nEpoch 328/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0840\nEpoch 329/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0859\nEpoch 330/8108\nTraining Loss: 32.0067, Training Accuracy: 0.0820\nEpoch 331/8108\nTraining Loss: 32.0062, Training Accuracy: 0.1055\nEpoch 332/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0950\nEpoch 333/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0935\nEpoch 334/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0993\nEpoch 335/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0691\nEpoch 336/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0907\nEpoch 337/8108\nTraining Loss: 31.9881, Training Accuracy: 0.1017\nEpoch 338/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0820\nEpoch 339/8108\nTraining Loss: 32.0140, Training Accuracy: 0.0849\nEpoch 340/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0763\nEpoch 341/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0849\nEpoch 342/8108\nTraining Loss: 32.0197, Training Accuracy: 0.0796\nEpoch 343/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0892\nEpoch 344/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0892\nEpoch 345/8108\nTraining Loss: 32.0065, Training Accuracy: 0.1137\nEpoch 346/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0724\nEpoch 347/8108\nTraining Loss: 32.0019, Training Accuracy: 0.1051\nEpoch 348/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0892\nEpoch 349/8108\nTraining Loss: 32.0074, Training Accuracy: 0.0662\nEpoch 350/8108\nTraining Loss: 32.0137, Training Accuracy: 0.0796\nEpoch 351/8108\nTraining Loss: 32.0070, Training Accuracy: 0.0950\nEpoch 352/8108\nTraining Loss: 32.0133, Training Accuracy: 0.0739\nEpoch 353/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0854\nEpoch 354/8108\nTraining Loss: 32.0027, Training Accuracy: 0.1017\nEpoch 355/8108\nTraining Loss: 32.0083, Training Accuracy: 0.0796\nEpoch 356/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0935\nEpoch 357/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0931\nEpoch 358/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0763\nEpoch 359/8108\nTraining Loss: 32.0094, Training Accuracy: 0.0763\nEpoch 360/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0964\nEpoch 361/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0763\nEpoch 362/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0911\nEpoch 363/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0878\nEpoch 364/8108\nTraining Loss: 31.9996, Training Accuracy: 0.1079\nEpoch 365/8108\nTraining Loss: 31.9847, Training Accuracy: 0.1055\nEpoch 366/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0763\nEpoch 367/8108\nTraining Loss: 32.0033, Training Accuracy: 0.0753\nEpoch 368/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0878\nEpoch 369/8108\nTraining Loss: 32.0171, Training Accuracy: 0.0720\nEpoch 370/8108\nTraining Loss: 31.9828, Training Accuracy: 0.1065\nEpoch 371/8108\nTraining Loss: 31.9690, Training Accuracy: 0.0940\nEpoch 372/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0792\nEpoch 373/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0576\nEpoch 374/8108\nTraining Loss: 31.9824, Training Accuracy: 0.0887\nEpoch 375/8108\nTraining Loss: 31.9886, Training Accuracy: 0.1166\nEpoch 376/8108\nTraining Loss: 32.0131, Training Accuracy: 0.0998\nEpoch 377/8108\nTraining Loss: 31.9891, Training Accuracy: 0.0782\nEpoch 378/8108\nTraining Loss: 31.9870, Training Accuracy: 0.0748\nEpoch 379/8108\nTraining Loss: 32.0127, Training Accuracy: 0.0926\nEpoch 380/8108\nTraining Loss: 32.0118, Training Accuracy: 0.0806\nEpoch 381/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0878\nEpoch 382/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0753\nEpoch 383/8108\nTraining Loss: 32.0156, Training Accuracy: 0.0720\nEpoch 384/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0796\nEpoch 385/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0748\nEpoch 386/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0979\nEpoch 387/8108\nTraining Loss: 31.9928, Training Accuracy: 0.0935\nEpoch 388/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0806\nEpoch 389/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0662\nEpoch 390/8108\nTraining Loss: 31.9828, Training Accuracy: 0.0863\nEpoch 391/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0993\nEpoch 392/8108\nTraining Loss: 32.0002, Training Accuracy: 0.0878\nEpoch 393/8108\nTraining Loss: 31.9856, Training Accuracy: 0.1031\nEpoch 394/8108\nTraining Loss: 32.0020, Training Accuracy: 0.0854\nEpoch 395/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0945\nEpoch 396/8108\nTraining Loss: 32.0018, Training Accuracy: 0.0763\nEpoch 397/8108\nTraining Loss: 32.0084, Training Accuracy: 0.0777\nEpoch 398/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0782\nEpoch 399/8108\nTraining Loss: 31.9859, Training Accuracy: 0.0854\nEpoch 400/8108\nTraining Loss: 32.0180, Training Accuracy: 0.0748\nEpoch 401/8108\nTraining Loss: 31.9982, Training Accuracy: 0.0983\nEpoch 402/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0835\nEpoch 403/8108\nTraining Loss: 31.9813, Training Accuracy: 0.0931\nEpoch 404/8108\nTraining Loss: 31.9852, Training Accuracy: 0.0969\nEpoch 405/8108\nTraining Loss: 31.9924, Training Accuracy: 0.0955\nEpoch 406/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0705\nEpoch 407/8108\nTraining Loss: 31.9899, Training Accuracy: 0.1118\nEpoch 408/8108\nTraining Loss: 31.9808, Training Accuracy: 0.0988\nEpoch 409/8108\nTraining Loss: 32.0160, Training Accuracy: 0.0825\nEpoch 410/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0964\nEpoch 411/8108\nTraining Loss: 31.9867, Training Accuracy: 0.0825\nEpoch 412/8108\nTraining Loss: 32.0158, Training Accuracy: 0.0935\nEpoch 413/8108\nTraining Loss: 32.0093, Training Accuracy: 0.0863\nEpoch 414/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0825\nEpoch 415/8108\nTraining Loss: 32.0142, Training Accuracy: 0.0648\nEpoch 416/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0892\nEpoch 417/8108\nTraining Loss: 32.0077, Training Accuracy: 0.0696\nEpoch 418/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0868\nEpoch 419/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0979\nEpoch 420/8108\nTraining Loss: 31.9817, Training Accuracy: 0.1099\nEpoch 421/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0763\nEpoch 422/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0926\nEpoch 423/8108\nTraining Loss: 32.0097, Training Accuracy: 0.0897\nEpoch 424/8108\nTraining Loss: 31.9850, Training Accuracy: 0.1089\nEpoch 425/8108\nTraining Loss: 32.0012, Training Accuracy: 0.0993\nEpoch 426/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0926\nEpoch 427/8108\nTraining Loss: 32.0018, Training Accuracy: 0.1238\nEpoch 428/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0849\nEpoch 429/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0792\nEpoch 430/8108\nTraining Loss: 31.9958, Training Accuracy: 0.1075\nEpoch 431/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0863\nEpoch 432/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0926\nEpoch 433/8108\nTraining Loss: 32.0038, Training Accuracy: 0.1099\nEpoch 434/8108\nTraining Loss: 31.9852, Training Accuracy: 0.1012\nEpoch 435/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0830\nEpoch 436/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0959\nEpoch 437/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0863\nEpoch 438/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0763\nEpoch 439/8108\nTraining Loss: 32.0072, Training Accuracy: 0.0907\nEpoch 440/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0777\nEpoch 441/8108\nTraining Loss: 31.9843, Training Accuracy: 0.1060\nEpoch 442/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0835\nEpoch 443/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0720\nEpoch 444/8108\nTraining Loss: 31.9980, Training Accuracy: 0.1156\nEpoch 445/8108\nTraining Loss: 31.9975, Training Accuracy: 0.1123\nEpoch 446/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0748\nEpoch 447/8108\nTraining Loss: 31.9939, Training Accuracy: 0.0840\nEpoch 448/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0835\nEpoch 449/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0792\nEpoch 450/8108\nTraining Loss: 31.9790, Training Accuracy: 0.1166\nEpoch 451/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0854\nEpoch 452/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0720\nEpoch 453/8108\nTraining Loss: 31.9807, Training Accuracy: 0.0863\nEpoch 454/8108\nTraining Loss: 31.9977, Training Accuracy: 0.1031\nEpoch 455/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0911\nEpoch 456/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0720\nEpoch 457/8108\nTraining Loss: 31.9867, Training Accuracy: 0.0926\nEpoch 458/8108\nTraining Loss: 31.9891, Training Accuracy: 0.0662\nEpoch 459/8108\nTraining Loss: 32.0188, Training Accuracy: 0.0792\nEpoch 460/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0792\nEpoch 461/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0931\nEpoch 462/8108\nTraining Loss: 32.0095, Training Accuracy: 0.0897\nEpoch 463/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0863\nEpoch 464/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0969\nEpoch 465/8108\nTraining Loss: 31.9826, Training Accuracy: 0.0840\nEpoch 466/8108\nTraining Loss: 32.0072, Training Accuracy: 0.0724\nEpoch 467/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0854\nEpoch 468/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0945\nEpoch 469/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0806\nEpoch 470/8108\nTraining Loss: 31.9955, Training Accuracy: 0.0820\nEpoch 471/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0916\nEpoch 472/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0796\nEpoch 473/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0691\nEpoch 474/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0950\nEpoch 475/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0921\nEpoch 476/8108\nTraining Loss: 32.0077, Training Accuracy: 0.0796\nEpoch 477/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0926\nEpoch 478/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0681\nEpoch 479/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0676\nEpoch 480/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0696\nEpoch 481/8108\nTraining Loss: 31.9893, Training Accuracy: 0.0705\nEpoch 482/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0849\nEpoch 483/8108\nTraining Loss: 31.9822, Training Accuracy: 0.0955\nEpoch 484/8108\nTraining Loss: 32.0133, Training Accuracy: 0.1084\nEpoch 485/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0878\nEpoch 486/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0734\nEpoch 487/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0892\nEpoch 488/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0816\nEpoch 489/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0648\nEpoch 490/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0811\nEpoch 491/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0863\nEpoch 492/8108\nTraining Loss: 31.9687, Training Accuracy: 0.1041\nEpoch 493/8108\nTraining Loss: 32.0121, Training Accuracy: 0.0878\nEpoch 494/8108\nTraining Loss: 31.9880, Training Accuracy: 0.1012\nEpoch 495/8108\nTraining Loss: 32.0112, Training Accuracy: 0.0777\nEpoch 496/8108\nTraining Loss: 32.0066, Training Accuracy: 0.0806\nEpoch 497/8108\nTraining Loss: 31.9775, Training Accuracy: 0.0911\nEpoch 498/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0892\nEpoch 499/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0763\nEpoch 500/8108\nTraining Loss: 32.0126, Training Accuracy: 0.0820\nEpoch 501/8108\nTraining Loss: 32.0173, Training Accuracy: 0.0792\nEpoch 502/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0955\nEpoch 503/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0734\nEpoch 504/8108\nTraining Loss: 31.9840, Training Accuracy: 0.0806\nEpoch 505/8108\nTraining Loss: 31.9920, Training Accuracy: 0.1180\nEpoch 506/8108\nTraining Loss: 32.0015, Training Accuracy: 0.0691\nEpoch 507/8108\nTraining Loss: 31.9899, Training Accuracy: 0.0710\nEpoch 508/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0777\nEpoch 509/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0734\nEpoch 510/8108\nTraining Loss: 31.9777, Training Accuracy: 0.0892\nEpoch 511/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0806\nEpoch 512/8108\nTraining Loss: 31.9708, Training Accuracy: 0.0648\nEpoch 513/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0950\nEpoch 514/8108\nTraining Loss: 31.9870, Training Accuracy: 0.0777\nEpoch 515/8108\nTraining Loss: 32.0096, Training Accuracy: 0.0792\nEpoch 516/8108\nTraining Loss: 31.9815, Training Accuracy: 0.0748\nEpoch 517/8108\nTraining Loss: 32.0143, Training Accuracy: 0.0825\nEpoch 518/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0983\nEpoch 519/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0993\nEpoch 520/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0820\nEpoch 521/8108\nTraining Loss: 31.9919, Training Accuracy: 0.1190\nEpoch 522/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0835\nEpoch 523/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0983\nEpoch 524/8108\nTraining Loss: 32.0170, Training Accuracy: 0.0883\nEpoch 525/8108\nTraining Loss: 32.0112, Training Accuracy: 0.0897\nEpoch 526/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0801\nEpoch 527/8108\nTraining Loss: 32.0123, Training Accuracy: 0.0787\nEpoch 528/8108\nTraining Loss: 31.9984, Training Accuracy: 0.0863\nEpoch 529/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0926\nEpoch 530/8108\nTraining Loss: 31.9848, Training Accuracy: 0.0907\nEpoch 531/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0849\nEpoch 532/8108\nTraining Loss: 31.9894, Training Accuracy: 0.0825\nEpoch 533/8108\nTraining Loss: 32.0042, Training Accuracy: 0.0935\nEpoch 534/8108\nTraining Loss: 32.0191, Training Accuracy: 0.0926\nEpoch 535/8108\nTraining Loss: 32.0139, Training Accuracy: 0.0734\nEpoch 536/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0705\nEpoch 537/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0806\nEpoch 538/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0931\nEpoch 539/8108\nTraining Loss: 31.9856, Training Accuracy: 0.0988\nEpoch 540/8108\nTraining Loss: 32.0152, Training Accuracy: 0.0811\nEpoch 541/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0969\nEpoch 542/8108\nTraining Loss: 31.9811, Training Accuracy: 0.0691\nEpoch 543/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0974\nEpoch 544/8108\nTraining Loss: 32.0046, Training Accuracy: 0.0883\nEpoch 545/8108\nTraining Loss: 31.9787, Training Accuracy: 0.0782\nEpoch 546/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0777\nEpoch 547/8108\nTraining Loss: 32.0099, Training Accuracy: 0.1022\nEpoch 548/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0969\nEpoch 549/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0734\nEpoch 550/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0825\nEpoch 551/8108\nTraining Loss: 32.0084, Training Accuracy: 0.0897\nEpoch 552/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0998\nEpoch 553/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0940\nEpoch 554/8108\nTraining Loss: 31.9780, Training Accuracy: 0.0547\nEpoch 555/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0619\nEpoch 556/8108\nTraining Loss: 32.0149, Training Accuracy: 0.0648\nEpoch 557/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0696\nEpoch 558/8108\nTraining Loss: 31.9757, Training Accuracy: 0.0983\nEpoch 559/8108\nTraining Loss: 32.0081, Training Accuracy: 0.0734\nEpoch 560/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0926\nEpoch 561/8108\nTraining Loss: 31.9816, Training Accuracy: 0.0792\nEpoch 562/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0931\nEpoch 563/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0892\nEpoch 564/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0835\nEpoch 565/8108\nTraining Loss: 31.9985, Training Accuracy: 0.0705\nEpoch 566/8108\nTraining Loss: 31.9916, Training Accuracy: 0.1147\nEpoch 567/8108\nTraining Loss: 31.9957, Training Accuracy: 0.1012\nEpoch 568/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0825\nEpoch 569/8108\nTraining Loss: 32.0091, Training Accuracy: 0.0763\nEpoch 570/8108\nTraining Loss: 31.9895, Training Accuracy: 0.0763\nEpoch 571/8108\nTraining Loss: 32.0166, Training Accuracy: 0.0844\nEpoch 572/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0959\nEpoch 573/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0777\nEpoch 574/8108\nTraining Loss: 32.0066, Training Accuracy: 0.0921\nEpoch 575/8108\nTraining Loss: 32.0033, Training Accuracy: 0.1089\nEpoch 576/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0748\nEpoch 577/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0734\nEpoch 578/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0863\nEpoch 579/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0907\nEpoch 580/8108\nTraining Loss: 32.0012, Training Accuracy: 0.0806\nEpoch 581/8108\nTraining Loss: 31.9858, Training Accuracy: 0.1031\nEpoch 582/8108\nTraining Loss: 32.0028, Training Accuracy: 0.0902\nEpoch 583/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0705\nEpoch 584/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0806\nEpoch 585/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0935\nEpoch 586/8108\nTraining Loss: 31.9859, Training Accuracy: 0.0907\nEpoch 587/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0940\nEpoch 588/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0940\nEpoch 589/8108\nTraining Loss: 31.9939, Training Accuracy: 0.0811\nEpoch 590/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0859\nEpoch 591/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0897\nEpoch 592/8108\nTraining Loss: 32.0090, Training Accuracy: 0.1099\nEpoch 593/8108\nTraining Loss: 31.9933, Training Accuracy: 0.1084\nEpoch 594/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0868\nEpoch 595/8108\nTraining Loss: 31.9778, Training Accuracy: 0.0926\nEpoch 596/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0940\nEpoch 597/8108\nTraining Loss: 32.0020, Training Accuracy: 0.0604\nEpoch 598/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0777\nEpoch 599/8108\nTraining Loss: 32.0067, Training Accuracy: 0.0633\nEpoch 600/8108\nTraining Loss: 32.0114, Training Accuracy: 0.0911\nEpoch 601/8108\nTraining Loss: 32.0026, Training Accuracy: 0.0720\nEpoch 602/8108\nTraining Loss: 31.9807, Training Accuracy: 0.0993\nEpoch 603/8108\nTraining Loss: 31.9892, Training Accuracy: 0.1070\nEpoch 604/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0921\nEpoch 605/8108\nTraining Loss: 32.0146, Training Accuracy: 0.0739\nEpoch 606/8108\nTraining Loss: 31.9962, Training Accuracy: 0.1142\nEpoch 607/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0993\nEpoch 608/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0734\nEpoch 609/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0768\nEpoch 610/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0792\nEpoch 611/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0998\nEpoch 612/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0681\nEpoch 613/8108\nTraining Loss: 31.9806, Training Accuracy: 0.0734\nEpoch 614/8108\nTraining Loss: 32.0108, Training Accuracy: 0.0796\nEpoch 615/8108\nTraining Loss: 32.0038, Training Accuracy: 0.1084\nEpoch 616/8108\nTraining Loss: 32.0003, Training Accuracy: 0.1161\nEpoch 617/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0792\nEpoch 618/8108\nTraining Loss: 31.9883, Training Accuracy: 0.1099\nEpoch 619/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0892\nEpoch 620/8108\nTraining Loss: 31.9806, Training Accuracy: 0.1017\nEpoch 621/8108\nTraining Loss: 31.9916, Training Accuracy: 0.1017\nEpoch 622/8108\nTraining Loss: 31.9879, Training Accuracy: 0.1060\nEpoch 623/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0911\nEpoch 624/8108\nTraining Loss: 32.0112, Training Accuracy: 0.0883\nEpoch 625/8108\nTraining Loss: 32.0145, Training Accuracy: 0.0734\nEpoch 626/8108\nTraining Loss: 31.9846, Training Accuracy: 0.0998\nEpoch 627/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0849\nEpoch 628/8108\nTraining Loss: 31.9799, Training Accuracy: 0.1142\nEpoch 629/8108\nTraining Loss: 31.9902, Training Accuracy: 0.1012\nEpoch 630/8108\nTraining Loss: 32.0121, Training Accuracy: 0.0907\nEpoch 631/8108\nTraining Loss: 32.0094, Training Accuracy: 0.0907\nEpoch 632/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0907\nEpoch 633/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0907\nEpoch 634/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0820\nEpoch 635/8108\nTraining Loss: 32.0058, Training Accuracy: 0.0782\nEpoch 636/8108\nTraining Loss: 32.0088, Training Accuracy: 0.1070\nEpoch 637/8108\nTraining Loss: 31.9962, Training Accuracy: 0.0854\nEpoch 638/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0863\nEpoch 639/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0748\nEpoch 640/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0892\nEpoch 641/8108\nTraining Loss: 31.9824, Training Accuracy: 0.0964\nEpoch 642/8108\nTraining Loss: 31.9734, Training Accuracy: 0.0883\nEpoch 643/8108\nTraining Loss: 31.9696, Training Accuracy: 0.0792\nEpoch 644/8108\nTraining Loss: 32.0190, Training Accuracy: 0.0835\nEpoch 645/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0863\nEpoch 646/8108\nTraining Loss: 32.0086, Training Accuracy: 0.0763\nEpoch 647/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0878\nEpoch 648/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0863\nEpoch 649/8108\nTraining Loss: 32.0101, Training Accuracy: 0.0753\nEpoch 650/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0777\nEpoch 651/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0950\nEpoch 652/8108\nTraining Loss: 31.9899, Training Accuracy: 0.0892\nEpoch 653/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0792\nEpoch 654/8108\nTraining Loss: 31.9696, Training Accuracy: 0.1147\nEpoch 655/8108\nTraining Loss: 31.9876, Training Accuracy: 0.1012\nEpoch 656/8108\nTraining Loss: 32.0199, Training Accuracy: 0.0763\nEpoch 657/8108\nTraining Loss: 31.9851, Training Accuracy: 0.1127\nEpoch 658/8108\nTraining Loss: 32.0164, Training Accuracy: 0.0676\nEpoch 659/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0768\nEpoch 660/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0935\nEpoch 661/8108\nTraining Loss: 32.0002, Training Accuracy: 0.0633\nEpoch 662/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0883\nEpoch 663/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0753\nEpoch 664/8108\nTraining Loss: 31.9796, Training Accuracy: 0.0863\nEpoch 665/8108\nTraining Loss: 31.9971, Training Accuracy: 0.0940\nEpoch 666/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0969\nEpoch 667/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0964\nEpoch 668/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0935\nEpoch 669/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0955\nEpoch 670/8108\nTraining Loss: 31.9820, Training Accuracy: 0.1022\nEpoch 671/8108\nTraining Loss: 31.9982, Training Accuracy: 0.0763\nEpoch 672/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0734\nEpoch 673/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0720\nEpoch 674/8108\nTraining Loss: 31.9893, Training Accuracy: 0.0950\nEpoch 675/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0825\nEpoch 676/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0911\nEpoch 677/8108\nTraining Loss: 31.9924, Training Accuracy: 0.0633\nEpoch 678/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0897\nEpoch 679/8108\nTraining Loss: 31.9870, Training Accuracy: 0.0955\nEpoch 680/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0988\nEpoch 681/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0835\nEpoch 682/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0648\nEpoch 683/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0907\nEpoch 684/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0734\nEpoch 685/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0604\nEpoch 686/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0724\nEpoch 687/8108\nTraining Loss: 31.9695, Training Accuracy: 0.1046\nEpoch 688/8108\nTraining Loss: 32.0087, Training Accuracy: 0.0705\nEpoch 689/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0768\nEpoch 690/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0820\nEpoch 691/8108\nTraining Loss: 32.0103, Training Accuracy: 0.0849\nEpoch 692/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0911\nEpoch 693/8108\nTraining Loss: 31.9985, Training Accuracy: 0.0840\nEpoch 694/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0940\nEpoch 695/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0849\nEpoch 696/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0859\nEpoch 697/8108\nTraining Loss: 31.9780, Training Accuracy: 0.0825\nEpoch 698/8108\nTraining Loss: 31.9841, Training Accuracy: 0.1094\nEpoch 699/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0883\nEpoch 700/8108\nTraining Loss: 31.9738, Training Accuracy: 0.0806\nEpoch 701/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0691\nEpoch 702/8108\nTraining Loss: 31.9782, Training Accuracy: 0.0835\nEpoch 703/8108\nTraining Loss: 31.9817, Training Accuracy: 0.0863\nEpoch 704/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0892\nEpoch 705/8108\nTraining Loss: 32.0068, Training Accuracy: 0.1171\nEpoch 706/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0935\nEpoch 707/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0907\nEpoch 708/8108\nTraining Loss: 31.9892, Training Accuracy: 0.0921\nEpoch 709/8108\nTraining Loss: 32.0047, Training Accuracy: 0.0676\nEpoch 710/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0849\nEpoch 711/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0868\nEpoch 712/8108\nTraining Loss: 31.9894, Training Accuracy: 0.0705\nEpoch 713/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0974\nEpoch 714/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0897\nEpoch 715/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0777\nEpoch 716/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0696\nEpoch 717/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0561\nEpoch 718/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0806\nEpoch 719/8108\nTraining Loss: 32.0037, Training Accuracy: 0.0892\nEpoch 720/8108\nTraining Loss: 31.9999, Training Accuracy: 0.1113\nEpoch 721/8108\nTraining Loss: 32.0125, Training Accuracy: 0.0619\nEpoch 722/8108\nTraining Loss: 32.0050, Training Accuracy: 0.0590\nEpoch 723/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0911\nEpoch 724/8108\nTraining Loss: 31.9913, Training Accuracy: 0.1113\nEpoch 725/8108\nTraining Loss: 31.9890, Training Accuracy: 0.0825\nEpoch 726/8108\nTraining Loss: 32.0056, Training Accuracy: 0.0518\nEpoch 727/8108\nTraining Loss: 31.9872, Training Accuracy: 0.1003\nEpoch 728/8108\nTraining Loss: 32.0050, Training Accuracy: 0.0969\nEpoch 729/8108\nTraining Loss: 32.0001, Training Accuracy: 0.1046\nEpoch 730/8108\nTraining Loss: 31.9777, Training Accuracy: 0.0892\nEpoch 731/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0983\nEpoch 732/8108\nTraining Loss: 31.9946, Training Accuracy: 0.1156\nEpoch 733/8108\nTraining Loss: 31.9645, Training Accuracy: 0.0983\nEpoch 734/8108\nTraining Loss: 31.9838, Training Accuracy: 0.0911\nEpoch 735/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0911\nEpoch 736/8108\nTraining Loss: 31.9868, Training Accuracy: 0.0921\nEpoch 737/8108\nTraining Loss: 32.0032, Training Accuracy: 0.1027\nEpoch 738/8108\nTraining Loss: 32.0074, Training Accuracy: 0.0863\nEpoch 739/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0734\nEpoch 740/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0806\nEpoch 741/8108\nTraining Loss: 31.9939, Training Accuracy: 0.0897\nEpoch 742/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0792\nEpoch 743/8108\nTraining Loss: 31.9699, Training Accuracy: 0.0748\nEpoch 744/8108\nTraining Loss: 32.0166, Training Accuracy: 0.0897\nEpoch 745/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0897\nEpoch 746/8108\nTraining Loss: 32.0058, Training Accuracy: 0.0710\nEpoch 747/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0921\nEpoch 748/8108\nTraining Loss: 31.9695, Training Accuracy: 0.1118\nEpoch 749/8108\nTraining Loss: 31.9928, Training Accuracy: 0.0849\nEpoch 750/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0806\nEpoch 751/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0734\nEpoch 752/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0840\nEpoch 753/8108\nTraining Loss: 31.9854, Training Accuracy: 0.0907\nEpoch 754/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0849\nEpoch 755/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0811\nEpoch 756/8108\nTraining Loss: 32.0095, Training Accuracy: 0.0676\nEpoch 757/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0825\nEpoch 758/8108\nTraining Loss: 32.0062, Training Accuracy: 0.0935\nEpoch 759/8108\nTraining Loss: 32.0132, Training Accuracy: 0.0705\nEpoch 760/8108\nTraining Loss: 31.9769, Training Accuracy: 0.0854\nEpoch 761/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0897\nEpoch 762/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0955\nEpoch 763/8108\nTraining Loss: 31.9866, Training Accuracy: 0.0887\nEpoch 764/8108\nTraining Loss: 31.9892, Training Accuracy: 0.0974\nEpoch 765/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0916\nEpoch 766/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0892\nEpoch 767/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0902\nEpoch 768/8108\nTraining Loss: 31.9831, Training Accuracy: 0.0878\nEpoch 769/8108\nTraining Loss: 32.0015, Training Accuracy: 0.1031\nEpoch 770/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0811\nEpoch 771/8108\nTraining Loss: 31.9866, Training Accuracy: 0.0830\nEpoch 772/8108\nTraining Loss: 31.9830, Training Accuracy: 0.1041\nEpoch 773/8108\nTraining Loss: 32.0056, Training Accuracy: 0.1036\nEpoch 774/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0849\nEpoch 775/8108\nTraining Loss: 32.0136, Training Accuracy: 0.0974\nEpoch 776/8108\nTraining Loss: 31.9909, Training Accuracy: 0.0907\nEpoch 777/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0768\nEpoch 778/8108\nTraining Loss: 31.9971, Training Accuracy: 0.0792\nEpoch 779/8108\nTraining Loss: 31.9942, Training Accuracy: 0.1051\nEpoch 780/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0806\nEpoch 781/8108\nTraining Loss: 31.9958, Training Accuracy: 0.1099\nEpoch 782/8108\nTraining Loss: 32.0027, Training Accuracy: 0.1113\nEpoch 783/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0964\nEpoch 784/8108\nTraining Loss: 32.0054, Training Accuracy: 0.0806\nEpoch 785/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0748\nEpoch 786/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0883\nEpoch 787/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0820\nEpoch 788/8108\nTraining Loss: 31.9952, Training Accuracy: 0.1017\nEpoch 789/8108\nTraining Loss: 31.9732, Training Accuracy: 0.0955\nEpoch 790/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0892\nEpoch 791/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0811\nEpoch 792/8108\nTraining Loss: 31.9955, Training Accuracy: 0.0811\nEpoch 793/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0811\nEpoch 794/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0897\nEpoch 795/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0950\nEpoch 796/8108\nTraining Loss: 32.0037, Training Accuracy: 0.0897\nEpoch 797/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0911\nEpoch 798/8108\nTraining Loss: 31.9894, Training Accuracy: 0.0806\nEpoch 799/8108\nTraining Loss: 31.9963, Training Accuracy: 0.0950\nEpoch 800/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0979\nEpoch 801/8108\nTraining Loss: 32.0067, Training Accuracy: 0.0825\nEpoch 802/8108\nTraining Loss: 32.0241, Training Accuracy: 0.0748\nEpoch 803/8108\nTraining Loss: 31.9963, Training Accuracy: 0.0734\nEpoch 804/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0854\nEpoch 805/8108\nTraining Loss: 31.9967, Training Accuracy: 0.1027\nEpoch 806/8108\nTraining Loss: 32.0081, Training Accuracy: 0.0887\nEpoch 807/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0868\nEpoch 808/8108\nTraining Loss: 32.0012, Training Accuracy: 0.0935\nEpoch 809/8108\nTraining Loss: 31.9985, Training Accuracy: 0.0945\nEpoch 810/8108\nTraining Loss: 31.9949, Training Accuracy: 0.0648\nEpoch 811/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0820\nEpoch 812/8108\nTraining Loss: 31.9855, Training Accuracy: 0.1108\nEpoch 813/8108\nTraining Loss: 31.9796, Training Accuracy: 0.0854\nEpoch 814/8108\nTraining Loss: 31.9886, Training Accuracy: 0.0806\nEpoch 815/8108\nTraining Loss: 31.9784, Training Accuracy: 0.1046\nEpoch 816/8108\nTraining Loss: 31.9861, Training Accuracy: 0.1007\nEpoch 817/8108\nTraining Loss: 31.9780, Training Accuracy: 0.0892\nEpoch 818/8108\nTraining Loss: 31.9905, Training Accuracy: 0.1132\nEpoch 819/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0691\nEpoch 820/8108\nTraining Loss: 31.9969, Training Accuracy: 0.1007\nEpoch 821/8108\nTraining Loss: 32.0121, Training Accuracy: 0.0648\nEpoch 822/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0792\nEpoch 823/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0532\nEpoch 824/8108\nTraining Loss: 32.0036, Training Accuracy: 0.0849\nEpoch 825/8108\nTraining Loss: 31.9842, Training Accuracy: 0.1036\nEpoch 826/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0796\nEpoch 827/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0849\nEpoch 828/8108\nTraining Loss: 32.0099, Training Accuracy: 0.0878\nEpoch 829/8108\nTraining Loss: 31.9871, Training Accuracy: 0.0840\nEpoch 830/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0739\nEpoch 831/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0897\nEpoch 832/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0892\nEpoch 833/8108\nTraining Loss: 32.0197, Training Accuracy: 0.0662\nEpoch 834/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0734\nEpoch 835/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0820\nEpoch 836/8108\nTraining Loss: 31.9995, Training Accuracy: 0.1108\nEpoch 837/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0696\nEpoch 838/8108\nTraining Loss: 31.9791, Training Accuracy: 0.0825\nEpoch 839/8108\nTraining Loss: 31.9949, Training Accuracy: 0.0935\nEpoch 840/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0863\nEpoch 841/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0969\nEpoch 842/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0830\nEpoch 843/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0830\nEpoch 844/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0921\nEpoch 845/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0748\nEpoch 846/8108\nTraining Loss: 32.0086, Training Accuracy: 0.0897\nEpoch 847/8108\nTraining Loss: 31.9803, Training Accuracy: 0.0935\nEpoch 848/8108\nTraining Loss: 31.9759, Training Accuracy: 0.0935\nEpoch 849/8108\nTraining Loss: 32.0163, Training Accuracy: 0.0921\nEpoch 850/8108\nTraining Loss: 31.9602, Training Accuracy: 0.0988\nEpoch 851/8108\nTraining Loss: 32.0092, Training Accuracy: 0.0806\nEpoch 852/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0892\nEpoch 853/8108\nTraining Loss: 31.9854, Training Accuracy: 0.0883\nEpoch 854/8108\nTraining Loss: 31.9889, Training Accuracy: 0.1007\nEpoch 855/8108\nTraining Loss: 32.0157, Training Accuracy: 0.0988\nEpoch 856/8108\nTraining Loss: 31.9957, Training Accuracy: 0.1012\nEpoch 857/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0696\nEpoch 858/8108\nTraining Loss: 31.9962, Training Accuracy: 0.0993\nEpoch 859/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0863\nEpoch 860/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0782\nEpoch 861/8108\nTraining Loss: 32.0042, Training Accuracy: 0.0720\nEpoch 862/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0825\nEpoch 863/8108\nTraining Loss: 31.9843, Training Accuracy: 0.1175\nEpoch 864/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0835\nEpoch 865/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0720\nEpoch 866/8108\nTraining Loss: 31.9786, Training Accuracy: 0.0935\nEpoch 867/8108\nTraining Loss: 31.9899, Training Accuracy: 0.0835\nEpoch 868/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0988\nEpoch 869/8108\nTraining Loss: 31.9853, Training Accuracy: 0.0993\nEpoch 870/8108\nTraining Loss: 31.9967, Training Accuracy: 0.1022\nEpoch 871/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0835\nEpoch 872/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0705\nEpoch 873/8108\nTraining Loss: 31.9847, Training Accuracy: 0.0863\nEpoch 874/8108\nTraining Loss: 31.9826, Training Accuracy: 0.0849\nEpoch 875/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0955\nEpoch 876/8108\nTraining Loss: 31.9888, Training Accuracy: 0.1055\nEpoch 877/8108\nTraining Loss: 31.9767, Training Accuracy: 0.1051\nEpoch 878/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0691\nEpoch 879/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0945\nEpoch 880/8108\nTraining Loss: 31.9871, Training Accuracy: 0.0748\nEpoch 881/8108\nTraining Loss: 32.0127, Training Accuracy: 0.0897\nEpoch 882/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0974\nEpoch 883/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0777\nEpoch 884/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0883\nEpoch 885/8108\nTraining Loss: 31.9976, Training Accuracy: 0.0940\nEpoch 886/8108\nTraining Loss: 32.0058, Training Accuracy: 0.1060\nEpoch 887/8108\nTraining Loss: 32.0069, Training Accuracy: 0.0931\nEpoch 888/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0959\nEpoch 889/8108\nTraining Loss: 31.9875, Training Accuracy: 0.0863\nEpoch 890/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0648\nEpoch 891/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0782\nEpoch 892/8108\nTraining Loss: 31.9726, Training Accuracy: 0.0825\nEpoch 893/8108\nTraining Loss: 31.9827, Training Accuracy: 0.1175\nEpoch 894/8108\nTraining Loss: 31.9984, Training Accuracy: 0.0806\nEpoch 895/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0835\nEpoch 896/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0734\nEpoch 897/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0748\nEpoch 898/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0720\nEpoch 899/8108\nTraining Loss: 32.0155, Training Accuracy: 0.0648\nEpoch 900/8108\nTraining Loss: 31.9883, Training Accuracy: 0.0753\nEpoch 901/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0892\nEpoch 902/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0720\nEpoch 903/8108\nTraining Loss: 32.0087, Training Accuracy: 0.0820\nEpoch 904/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0739\nEpoch 905/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0863\nEpoch 906/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0734\nEpoch 907/8108\nTraining Loss: 31.9948, Training Accuracy: 0.0911\nEpoch 908/8108\nTraining Loss: 32.0084, Training Accuracy: 0.0859\nEpoch 909/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0854\nEpoch 910/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0792\nEpoch 911/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0955\nEpoch 912/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0825\nEpoch 913/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0907\nEpoch 914/8108\nTraining Loss: 32.0046, Training Accuracy: 0.0662\nEpoch 915/8108\nTraining Loss: 31.9837, Training Accuracy: 0.0969\nEpoch 916/8108\nTraining Loss: 32.0109, Training Accuracy: 0.0763\nEpoch 917/8108\nTraining Loss: 32.0018, Training Accuracy: 0.0988\nEpoch 918/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0907\nEpoch 919/8108\nTraining Loss: 32.0019, Training Accuracy: 0.0840\nEpoch 920/8108\nTraining Loss: 31.9833, Training Accuracy: 0.1147\nEpoch 921/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0720\nEpoch 922/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0950\nEpoch 923/8108\nTraining Loss: 32.0000, Training Accuracy: 0.1137\nEpoch 924/8108\nTraining Loss: 32.0074, Training Accuracy: 0.0816\nEpoch 925/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0979\nEpoch 926/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0792\nEpoch 927/8108\nTraining Loss: 32.0030, Training Accuracy: 0.1079\nEpoch 928/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0863\nEpoch 929/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0792\nEpoch 930/8108\nTraining Loss: 31.9870, Training Accuracy: 0.1041\nEpoch 931/8108\nTraining Loss: 32.0012, Training Accuracy: 0.0892\nEpoch 932/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0935\nEpoch 933/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0739\nEpoch 934/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0878\nEpoch 935/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0835\nEpoch 936/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0993\nEpoch 937/8108\nTraining Loss: 31.9996, Training Accuracy: 0.0820\nEpoch 938/8108\nTraining Loss: 31.9795, Training Accuracy: 0.1166\nEpoch 939/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0854\nEpoch 940/8108\nTraining Loss: 32.0108, Training Accuracy: 0.0935\nEpoch 941/8108\nTraining Loss: 32.0046, Training Accuracy: 0.0911\nEpoch 942/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0820\nEpoch 943/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0998\nEpoch 944/8108\nTraining Loss: 31.9955, Training Accuracy: 0.0969\nEpoch 945/8108\nTraining Loss: 31.9928, Training Accuracy: 0.0777\nEpoch 946/8108\nTraining Loss: 31.9837, Training Accuracy: 0.0863\nEpoch 947/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0811\nEpoch 948/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0662\nEpoch 949/8108\nTraining Loss: 32.0050, Training Accuracy: 0.0911\nEpoch 950/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0720\nEpoch 951/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0950\nEpoch 952/8108\nTraining Loss: 31.9875, Training Accuracy: 0.0911\nEpoch 953/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0926\nEpoch 954/8108\nTraining Loss: 31.9776, Training Accuracy: 0.1007\nEpoch 955/8108\nTraining Loss: 32.0029, Training Accuracy: 0.1017\nEpoch 956/8108\nTraining Loss: 32.0148, Training Accuracy: 0.0796\nEpoch 957/8108\nTraining Loss: 32.0176, Training Accuracy: 0.0648\nEpoch 958/8108\nTraining Loss: 32.0018, Training Accuracy: 0.0892\nEpoch 959/8108\nTraining Loss: 31.9825, Training Accuracy: 0.0926\nEpoch 960/8108\nTraining Loss: 31.9868, Training Accuracy: 0.1161\nEpoch 961/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0768\nEpoch 962/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0734\nEpoch 963/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0720\nEpoch 964/8108\nTraining Loss: 31.9894, Training Accuracy: 0.0902\nEpoch 965/8108\nTraining Loss: 31.9856, Training Accuracy: 0.0955\nEpoch 966/8108\nTraining Loss: 31.9895, Training Accuracy: 0.0863\nEpoch 967/8108\nTraining Loss: 31.9742, Training Accuracy: 0.0878\nEpoch 968/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0863\nEpoch 969/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0691\nEpoch 970/8108\nTraining Loss: 32.0202, Training Accuracy: 0.0705\nEpoch 971/8108\nTraining Loss: 32.0041, Training Accuracy: 0.0734\nEpoch 972/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0964\nEpoch 973/8108\nTraining Loss: 31.9867, Training Accuracy: 0.0854\nEpoch 974/8108\nTraining Loss: 32.0054, Training Accuracy: 0.0624\nEpoch 975/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0955\nEpoch 976/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0926\nEpoch 977/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0777\nEpoch 978/8108\nTraining Loss: 32.0065, Training Accuracy: 0.0820\nEpoch 979/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0849\nEpoch 980/8108\nTraining Loss: 32.0134, Training Accuracy: 0.0691\nEpoch 981/8108\nTraining Loss: 31.9875, Training Accuracy: 0.0748\nEpoch 982/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0892\nEpoch 983/8108\nTraining Loss: 31.9992, Training Accuracy: 0.1022\nEpoch 984/8108\nTraining Loss: 31.9828, Training Accuracy: 0.1099\nEpoch 985/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0883\nEpoch 986/8108\nTraining Loss: 32.0255, Training Accuracy: 0.0724\nEpoch 987/8108\nTraining Loss: 31.9846, Training Accuracy: 0.1123\nEpoch 988/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0959\nEpoch 989/8108\nTraining Loss: 32.0158, Training Accuracy: 0.0796\nEpoch 990/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0835\nEpoch 991/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0792\nEpoch 992/8108\nTraining Loss: 32.0082, Training Accuracy: 0.0887\nEpoch 993/8108\nTraining Loss: 31.9880, Training Accuracy: 0.0763\nEpoch 994/8108\nTraining Loss: 31.9799, Training Accuracy: 0.0835\nEpoch 995/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0849\nEpoch 996/8108\nTraining Loss: 32.0113, Training Accuracy: 0.0777\nEpoch 997/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0768\nEpoch 998/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0777\nEpoch 999/8108\nTraining Loss: 31.9802, Training Accuracy: 0.1036\nEpoch 1000/8108\nTraining Loss: 32.0209, Training Accuracy: 0.0863\nEpoch 1001/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0935\nEpoch 1002/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0974\nEpoch 1003/8108\nTraining Loss: 31.9959, Training Accuracy: 0.1142\nEpoch 1004/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0748\nEpoch 1005/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0921\nEpoch 1006/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0993\nEpoch 1007/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0724\nEpoch 1008/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0792\nEpoch 1009/8108\nTraining Loss: 31.9855, Training Accuracy: 0.1084\nEpoch 1010/8108\nTraining Loss: 32.0019, Training Accuracy: 0.0935\nEpoch 1011/8108\nTraining Loss: 31.9904, Training Accuracy: 0.1036\nEpoch 1012/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0691\nEpoch 1013/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0974\nEpoch 1014/8108\nTraining Loss: 31.9943, Training Accuracy: 0.1046\nEpoch 1015/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0897\nEpoch 1016/8108\nTraining Loss: 31.9738, Training Accuracy: 0.1065\nEpoch 1017/8108\nTraining Loss: 32.0000, Training Accuracy: 0.1031\nEpoch 1018/8108\nTraining Loss: 32.0121, Training Accuracy: 0.1007\nEpoch 1019/8108\nTraining Loss: 31.9848, Training Accuracy: 0.0979\nEpoch 1020/8108\nTraining Loss: 31.9675, Training Accuracy: 0.0854\nEpoch 1021/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0691\nEpoch 1022/8108\nTraining Loss: 31.9948, Training Accuracy: 0.1046\nEpoch 1023/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0921\nEpoch 1024/8108\nTraining Loss: 32.0153, Training Accuracy: 0.0720\nEpoch 1025/8108\nTraining Loss: 32.0103, Training Accuracy: 0.0806\nEpoch 1026/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0878\nEpoch 1027/8108\nTraining Loss: 32.0026, Training Accuracy: 0.0720\nEpoch 1028/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0926\nEpoch 1029/8108\nTraining Loss: 32.0077, Training Accuracy: 0.0964\nEpoch 1030/8108\nTraining Loss: 31.9850, Training Accuracy: 0.0849\nEpoch 1031/8108\nTraining Loss: 31.9927, Training Accuracy: 0.0648\nEpoch 1032/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0792\nEpoch 1033/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0825\nEpoch 1034/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0806\nEpoch 1035/8108\nTraining Loss: 31.9821, Training Accuracy: 0.0969\nEpoch 1036/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0782\nEpoch 1037/8108\nTraining Loss: 32.0123, Training Accuracy: 0.0806\nEpoch 1038/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0777\nEpoch 1039/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0998\nEpoch 1040/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0782\nEpoch 1041/8108\nTraining Loss: 31.9743, Training Accuracy: 0.0878\nEpoch 1042/8108\nTraining Loss: 32.0142, Training Accuracy: 0.0849\nEpoch 1043/8108\nTraining Loss: 31.9792, Training Accuracy: 0.1065\nEpoch 1044/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0768\nEpoch 1045/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0720\nEpoch 1046/8108\nTraining Loss: 32.0095, Training Accuracy: 0.0720\nEpoch 1047/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0921\nEpoch 1048/8108\nTraining Loss: 31.9874, Training Accuracy: 0.1055\nEpoch 1049/8108\nTraining Loss: 31.9935, Training Accuracy: 0.1022\nEpoch 1050/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0950\nEpoch 1051/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0859\nEpoch 1052/8108\nTraining Loss: 32.0033, Training Accuracy: 0.0590\nEpoch 1053/8108\nTraining Loss: 31.9765, Training Accuracy: 0.0926\nEpoch 1054/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0979\nEpoch 1055/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0979\nEpoch 1056/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0792\nEpoch 1057/8108\nTraining Loss: 31.9860, Training Accuracy: 0.0720\nEpoch 1058/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0792\nEpoch 1059/8108\nTraining Loss: 31.9837, Training Accuracy: 0.0835\nEpoch 1060/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0878\nEpoch 1061/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0710\nEpoch 1062/8108\nTraining Loss: 32.0088, Training Accuracy: 0.0950\nEpoch 1063/8108\nTraining Loss: 31.9927, Training Accuracy: 0.1007\nEpoch 1064/8108\nTraining Loss: 31.9990, Training Accuracy: 0.1031\nEpoch 1065/8108\nTraining Loss: 32.0042, Training Accuracy: 0.0796\nEpoch 1066/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0878\nEpoch 1067/8108\nTraining Loss: 31.9971, Training Accuracy: 0.0777\nEpoch 1068/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0748\nEpoch 1069/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0710\nEpoch 1070/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0911\nEpoch 1071/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0897\nEpoch 1072/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0835\nEpoch 1073/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0883\nEpoch 1074/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0748\nEpoch 1075/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0849\nEpoch 1076/8108\nTraining Loss: 31.9815, Training Accuracy: 0.0792\nEpoch 1077/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0854\nEpoch 1078/8108\nTraining Loss: 32.0056, Training Accuracy: 0.0691\nEpoch 1079/8108\nTraining Loss: 32.0108, Training Accuracy: 0.0878\nEpoch 1080/8108\nTraining Loss: 32.0062, Training Accuracy: 0.0753\nEpoch 1081/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0820\nEpoch 1082/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0863\nEpoch 1083/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0835\nEpoch 1084/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0840\nEpoch 1085/8108\nTraining Loss: 31.9823, Training Accuracy: 0.1003\nEpoch 1086/8108\nTraining Loss: 31.9850, Training Accuracy: 0.0945\nEpoch 1087/8108\nTraining Loss: 31.9809, Training Accuracy: 0.0854\nEpoch 1088/8108\nTraining Loss: 31.9558, Training Accuracy: 0.0825\nEpoch 1089/8108\nTraining Loss: 31.9665, Training Accuracy: 0.0907\nEpoch 1090/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0633\nEpoch 1091/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0878\nEpoch 1092/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0792\nEpoch 1093/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0662\nEpoch 1094/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0940\nEpoch 1095/8108\nTraining Loss: 32.0091, Training Accuracy: 0.0720\nEpoch 1096/8108\nTraining Loss: 31.9797, Training Accuracy: 0.0979\nEpoch 1097/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0863\nEpoch 1098/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0911\nEpoch 1099/8108\nTraining Loss: 31.9984, Training Accuracy: 0.0835\nEpoch 1100/8108\nTraining Loss: 31.9849, Training Accuracy: 0.0926\nEpoch 1101/8108\nTraining Loss: 31.9786, Training Accuracy: 0.0950\nEpoch 1102/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0902\nEpoch 1103/8108\nTraining Loss: 31.9846, Training Accuracy: 0.0691\nEpoch 1104/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0768\nEpoch 1105/8108\nTraining Loss: 31.9973, Training Accuracy: 0.1036\nEpoch 1106/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0734\nEpoch 1107/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0907\nEpoch 1108/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0926\nEpoch 1109/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0863\nEpoch 1110/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0911\nEpoch 1111/8108\nTraining Loss: 32.0028, Training Accuracy: 0.1041\nEpoch 1112/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0969\nEpoch 1113/8108\nTraining Loss: 31.9670, Training Accuracy: 0.1041\nEpoch 1114/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0691\nEpoch 1115/8108\nTraining Loss: 31.9949, Training Accuracy: 0.1166\nEpoch 1116/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0734\nEpoch 1117/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0840\nEpoch 1118/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0825\nEpoch 1119/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0931\nEpoch 1120/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0892\nEpoch 1121/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0777\nEpoch 1122/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0796\nEpoch 1123/8108\nTraining Loss: 31.9777, Training Accuracy: 0.0964\nEpoch 1124/8108\nTraining Loss: 31.9838, Training Accuracy: 0.0763\nEpoch 1125/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0892\nEpoch 1126/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0854\nEpoch 1127/8108\nTraining Loss: 31.9902, Training Accuracy: 0.1142\nEpoch 1128/8108\nTraining Loss: 31.9945, Training Accuracy: 0.1175\nEpoch 1129/8108\nTraining Loss: 31.9849, Training Accuracy: 0.0907\nEpoch 1130/8108\nTraining Loss: 31.9787, Training Accuracy: 0.0748\nEpoch 1131/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0979\nEpoch 1132/8108\nTraining Loss: 31.9856, Training Accuracy: 0.0777\nEpoch 1133/8108\nTraining Loss: 32.0122, Training Accuracy: 0.0835\nEpoch 1134/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0710\nEpoch 1135/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0983\nEpoch 1136/8108\nTraining Loss: 31.9832, Training Accuracy: 0.0907\nEpoch 1137/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0801\nEpoch 1138/8108\nTraining Loss: 31.9797, Training Accuracy: 0.0710\nEpoch 1139/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0854\nEpoch 1140/8108\nTraining Loss: 32.0194, Training Accuracy: 0.0777\nEpoch 1141/8108\nTraining Loss: 31.9794, Training Accuracy: 0.0854\nEpoch 1142/8108\nTraining Loss: 32.0037, Training Accuracy: 0.0916\nEpoch 1143/8108\nTraining Loss: 32.0013, Training Accuracy: 0.1103\nEpoch 1144/8108\nTraining Loss: 31.9808, Training Accuracy: 0.1027\nEpoch 1145/8108\nTraining Loss: 31.9787, Training Accuracy: 0.0806\nEpoch 1146/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0777\nEpoch 1147/8108\nTraining Loss: 32.0020, Training Accuracy: 0.0983\nEpoch 1148/8108\nTraining Loss: 32.0084, Training Accuracy: 0.0652\nEpoch 1149/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0782\nEpoch 1150/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0840\nEpoch 1151/8108\nTraining Loss: 31.9923, Training Accuracy: 0.1084\nEpoch 1152/8108\nTraining Loss: 32.0106, Training Accuracy: 0.0854\nEpoch 1153/8108\nTraining Loss: 31.9871, Training Accuracy: 0.1022\nEpoch 1154/8108\nTraining Loss: 32.0129, Training Accuracy: 0.0926\nEpoch 1155/8108\nTraining Loss: 31.9750, Training Accuracy: 0.0792\nEpoch 1156/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0768\nEpoch 1157/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0734\nEpoch 1158/8108\nTraining Loss: 31.9909, Training Accuracy: 0.0796\nEpoch 1159/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0878\nEpoch 1160/8108\nTraining Loss: 31.9774, Training Accuracy: 0.0868\nEpoch 1161/8108\nTraining Loss: 32.0066, Training Accuracy: 0.1084\nEpoch 1162/8108\nTraining Loss: 31.9890, Training Accuracy: 0.0969\nEpoch 1163/8108\nTraining Loss: 32.0092, Training Accuracy: 0.0979\nEpoch 1164/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0979\nEpoch 1165/8108\nTraining Loss: 31.9939, Training Accuracy: 0.0796\nEpoch 1166/8108\nTraining Loss: 31.9861, Training Accuracy: 0.0921\nEpoch 1167/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0935\nEpoch 1168/8108\nTraining Loss: 31.9839, Training Accuracy: 0.0883\nEpoch 1169/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0835\nEpoch 1170/8108\nTraining Loss: 31.9906, Training Accuracy: 0.1027\nEpoch 1171/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0935\nEpoch 1172/8108\nTraining Loss: 32.0121, Training Accuracy: 0.0835\nEpoch 1173/8108\nTraining Loss: 31.9902, Training Accuracy: 0.1075\nEpoch 1174/8108\nTraining Loss: 31.9787, Training Accuracy: 0.0811\nEpoch 1175/8108\nTraining Loss: 31.9955, Training Accuracy: 0.1103\nEpoch 1176/8108\nTraining Loss: 31.9997, Training Accuracy: 0.1017\nEpoch 1177/8108\nTraining Loss: 31.9852, Training Accuracy: 0.0926\nEpoch 1178/8108\nTraining Loss: 31.9835, Training Accuracy: 0.1257\nEpoch 1179/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0950\nEpoch 1180/8108\nTraining Loss: 32.0143, Training Accuracy: 0.0892\nEpoch 1181/8108\nTraining Loss: 31.9843, Training Accuracy: 0.0768\nEpoch 1182/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0763\nEpoch 1183/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0734\nEpoch 1184/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0748\nEpoch 1185/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0940\nEpoch 1186/8108\nTraining Loss: 32.0165, Training Accuracy: 0.0691\nEpoch 1187/8108\nTraining Loss: 31.9866, Training Accuracy: 0.0926\nEpoch 1188/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0652\nEpoch 1189/8108\nTraining Loss: 32.0108, Training Accuracy: 0.1070\nEpoch 1190/8108\nTraining Loss: 31.9851, Training Accuracy: 0.0964\nEpoch 1191/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0969\nEpoch 1192/8108\nTraining Loss: 31.9863, Training Accuracy: 0.1166\nEpoch 1193/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0811\nEpoch 1194/8108\nTraining Loss: 32.0068, Training Accuracy: 0.1007\nEpoch 1195/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0849\nEpoch 1196/8108\nTraining Loss: 31.9856, Training Accuracy: 0.0993\nEpoch 1197/8108\nTraining Loss: 31.9723, Training Accuracy: 0.1065\nEpoch 1198/8108\nTraining Loss: 32.0129, Training Accuracy: 0.0724\nEpoch 1199/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0676\nEpoch 1200/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0825\nEpoch 1201/8108\nTraining Loss: 31.9901, Training Accuracy: 0.1089\nEpoch 1202/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0777\nEpoch 1203/8108\nTraining Loss: 32.0068, Training Accuracy: 0.0796\nEpoch 1204/8108\nTraining Loss: 31.9996, Training Accuracy: 0.0868\nEpoch 1205/8108\nTraining Loss: 32.0093, Training Accuracy: 0.0863\nEpoch 1206/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0897\nEpoch 1207/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0935\nEpoch 1208/8108\nTraining Loss: 31.9925, Training Accuracy: 0.1046\nEpoch 1209/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0768\nEpoch 1210/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0911\nEpoch 1211/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0806\nEpoch 1212/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0907\nEpoch 1213/8108\nTraining Loss: 32.0063, Training Accuracy: 0.0854\nEpoch 1214/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0955\nEpoch 1215/8108\nTraining Loss: 31.9803, Training Accuracy: 0.0969\nEpoch 1216/8108\nTraining Loss: 32.0092, Training Accuracy: 0.0926\nEpoch 1217/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0710\nEpoch 1218/8108\nTraining Loss: 31.9771, Training Accuracy: 0.1051\nEpoch 1219/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0926\nEpoch 1220/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0878\nEpoch 1221/8108\nTraining Loss: 31.9807, Training Accuracy: 0.0835\nEpoch 1222/8108\nTraining Loss: 31.9717, Training Accuracy: 0.1022\nEpoch 1223/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0849\nEpoch 1224/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0806\nEpoch 1225/8108\nTraining Loss: 32.0026, Training Accuracy: 0.0734\nEpoch 1226/8108\nTraining Loss: 31.9906, Training Accuracy: 0.0878\nEpoch 1227/8108\nTraining Loss: 32.0035, Training Accuracy: 0.0868\nEpoch 1228/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0820\nEpoch 1229/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0940\nEpoch 1230/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0782\nEpoch 1231/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0777\nEpoch 1232/8108\nTraining Loss: 31.9807, Training Accuracy: 0.0724\nEpoch 1233/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0868\nEpoch 1234/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0935\nEpoch 1235/8108\nTraining Loss: 31.9894, Training Accuracy: 0.1003\nEpoch 1236/8108\nTraining Loss: 32.0002, Training Accuracy: 0.1055\nEpoch 1237/8108\nTraining Loss: 31.9890, Training Accuracy: 0.1070\nEpoch 1238/8108\nTraining Loss: 31.9932, Training Accuracy: 0.1031\nEpoch 1239/8108\nTraining Loss: 32.0023, Training Accuracy: 0.0840\nEpoch 1240/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0748\nEpoch 1241/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0969\nEpoch 1242/8108\nTraining Loss: 31.9839, Training Accuracy: 0.1055\nEpoch 1243/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0734\nEpoch 1244/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0897\nEpoch 1245/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0940\nEpoch 1246/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0959\nEpoch 1247/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0988\nEpoch 1248/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0907\nEpoch 1249/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0911\nEpoch 1250/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0907\nEpoch 1251/8108\nTraining Loss: 31.9817, Training Accuracy: 0.0849\nEpoch 1252/8108\nTraining Loss: 31.9760, Training Accuracy: 0.0897\nEpoch 1253/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0854\nEpoch 1254/8108\nTraining Loss: 31.9870, Training Accuracy: 0.0955\nEpoch 1255/8108\nTraining Loss: 32.0156, Training Accuracy: 0.0561\nEpoch 1256/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0576\nEpoch 1257/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0878\nEpoch 1258/8108\nTraining Loss: 31.9787, Training Accuracy: 0.0734\nEpoch 1259/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0835\nEpoch 1260/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0662\nEpoch 1261/8108\nTraining Loss: 32.0304, Training Accuracy: 0.0633\nEpoch 1262/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0734\nEpoch 1263/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0868\nEpoch 1264/8108\nTraining Loss: 31.9764, Training Accuracy: 0.0849\nEpoch 1265/8108\nTraining Loss: 31.9962, Training Accuracy: 0.0892\nEpoch 1266/8108\nTraining Loss: 31.9786, Training Accuracy: 0.0926\nEpoch 1267/8108\nTraining Loss: 31.9993, Training Accuracy: 0.1007\nEpoch 1268/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0969\nEpoch 1269/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0576\nEpoch 1270/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0724\nEpoch 1271/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0940\nEpoch 1272/8108\nTraining Loss: 32.0002, Training Accuracy: 0.0945\nEpoch 1273/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0974\nEpoch 1274/8108\nTraining Loss: 32.0029, Training Accuracy: 0.0892\nEpoch 1275/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0792\nEpoch 1276/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0863\nEpoch 1277/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0734\nEpoch 1278/8108\nTraining Loss: 31.9828, Training Accuracy: 0.1007\nEpoch 1279/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0739\nEpoch 1280/8108\nTraining Loss: 32.0025, Training Accuracy: 0.1084\nEpoch 1281/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0792\nEpoch 1282/8108\nTraining Loss: 31.9844, Training Accuracy: 0.1113\nEpoch 1283/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0777\nEpoch 1284/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0840\nEpoch 1285/8108\nTraining Loss: 32.0118, Training Accuracy: 0.0902\nEpoch 1286/8108\nTraining Loss: 32.0042, Training Accuracy: 0.0955\nEpoch 1287/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0868\nEpoch 1288/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0835\nEpoch 1289/8108\nTraining Loss: 31.9785, Training Accuracy: 0.1007\nEpoch 1290/8108\nTraining Loss: 31.9710, Training Accuracy: 0.0955\nEpoch 1291/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0820\nEpoch 1292/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0835\nEpoch 1293/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0849\nEpoch 1294/8108\nTraining Loss: 32.0087, Training Accuracy: 0.0796\nEpoch 1295/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0907\nEpoch 1296/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0705\nEpoch 1297/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0782\nEpoch 1298/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0835\nEpoch 1299/8108\nTraining Loss: 31.9856, Training Accuracy: 0.0897\nEpoch 1300/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0763\nEpoch 1301/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0792\nEpoch 1302/8108\nTraining Loss: 32.0071, Training Accuracy: 0.0825\nEpoch 1303/8108\nTraining Loss: 31.9911, Training Accuracy: 0.1012\nEpoch 1304/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0921\nEpoch 1305/8108\nTraining Loss: 31.9725, Training Accuracy: 0.0873\nEpoch 1306/8108\nTraining Loss: 31.9903, Training Accuracy: 0.1089\nEpoch 1307/8108\nTraining Loss: 31.9804, Training Accuracy: 0.0983\nEpoch 1308/8108\nTraining Loss: 31.9894, Training Accuracy: 0.0806\nEpoch 1309/8108\nTraining Loss: 32.0169, Training Accuracy: 0.0854\nEpoch 1310/8108\nTraining Loss: 32.0114, Training Accuracy: 0.0806\nEpoch 1311/8108\nTraining Loss: 31.9866, Training Accuracy: 0.0739\nEpoch 1312/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0748\nEpoch 1313/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0892\nEpoch 1314/8108\nTraining Loss: 31.9888, Training Accuracy: 0.1041\nEpoch 1315/8108\nTraining Loss: 32.0136, Training Accuracy: 0.0820\nEpoch 1316/8108\nTraining Loss: 32.0171, Training Accuracy: 0.0739\nEpoch 1317/8108\nTraining Loss: 31.9839, Training Accuracy: 0.0964\nEpoch 1318/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0974\nEpoch 1319/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0820\nEpoch 1320/8108\nTraining Loss: 32.0091, Training Accuracy: 0.0883\nEpoch 1321/8108\nTraining Loss: 32.0096, Training Accuracy: 0.0748\nEpoch 1322/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0777\nEpoch 1323/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0753\nEpoch 1324/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0734\nEpoch 1325/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0883\nEpoch 1326/8108\nTraining Loss: 31.9746, Training Accuracy: 0.0854\nEpoch 1327/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0720\nEpoch 1328/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0854\nEpoch 1329/8108\nTraining Loss: 32.0131, Training Accuracy: 0.0835\nEpoch 1330/8108\nTraining Loss: 31.9784, Training Accuracy: 0.0849\nEpoch 1331/8108\nTraining Loss: 32.0079, Training Accuracy: 0.0820\nEpoch 1332/8108\nTraining Loss: 31.9960, Training Accuracy: 0.1099\nEpoch 1333/8108\nTraining Loss: 32.0094, Training Accuracy: 0.0753\nEpoch 1334/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0816\nEpoch 1335/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0734\nEpoch 1336/8108\nTraining Loss: 31.9924, Training Accuracy: 0.0777\nEpoch 1337/8108\nTraining Loss: 31.9922, Training Accuracy: 0.0849\nEpoch 1338/8108\nTraining Loss: 32.0084, Training Accuracy: 0.0792\nEpoch 1339/8108\nTraining Loss: 31.9822, Training Accuracy: 0.1017\nEpoch 1340/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0849\nEpoch 1341/8108\nTraining Loss: 31.9846, Training Accuracy: 0.0969\nEpoch 1342/8108\nTraining Loss: 31.9924, Training Accuracy: 0.0849\nEpoch 1343/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0792\nEpoch 1344/8108\nTraining Loss: 32.0141, Training Accuracy: 0.1007\nEpoch 1345/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0705\nEpoch 1346/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0748\nEpoch 1347/8108\nTraining Loss: 31.9868, Training Accuracy: 0.1266\nEpoch 1348/8108\nTraining Loss: 31.9802, Training Accuracy: 0.0907\nEpoch 1349/8108\nTraining Loss: 32.0051, Training Accuracy: 0.0964\nEpoch 1350/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0863\nEpoch 1351/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0835\nEpoch 1352/8108\nTraining Loss: 32.0036, Training Accuracy: 0.1012\nEpoch 1353/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0950\nEpoch 1354/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0691\nEpoch 1355/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0633\nEpoch 1356/8108\nTraining Loss: 32.0028, Training Accuracy: 0.0633\nEpoch 1357/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0676\nEpoch 1358/8108\nTraining Loss: 31.9939, Training Accuracy: 0.0878\nEpoch 1359/8108\nTraining Loss: 31.9859, Training Accuracy: 0.0748\nEpoch 1360/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0897\nEpoch 1361/8108\nTraining Loss: 31.9976, Training Accuracy: 0.0907\nEpoch 1362/8108\nTraining Loss: 31.9747, Training Accuracy: 0.0911\nEpoch 1363/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0921\nEpoch 1364/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0940\nEpoch 1365/8108\nTraining Loss: 31.9684, Training Accuracy: 0.0883\nEpoch 1366/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0782\nEpoch 1367/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0753\nEpoch 1368/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0926\nEpoch 1369/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0940\nEpoch 1370/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0863\nEpoch 1371/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0849\nEpoch 1372/8108\nTraining Loss: 31.9811, Training Accuracy: 0.0883\nEpoch 1373/8108\nTraining Loss: 32.0047, Training Accuracy: 0.0792\nEpoch 1374/8108\nTraining Loss: 32.0015, Training Accuracy: 0.0979\nEpoch 1375/8108\nTraining Loss: 32.0174, Training Accuracy: 0.0739\nEpoch 1376/8108\nTraining Loss: 31.9740, Training Accuracy: 0.0835\nEpoch 1377/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0907\nEpoch 1378/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0950\nEpoch 1379/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0907\nEpoch 1380/8108\nTraining Loss: 32.0102, Training Accuracy: 0.0897\nEpoch 1381/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0964\nEpoch 1382/8108\nTraining Loss: 32.0110, Training Accuracy: 0.0792\nEpoch 1383/8108\nTraining Loss: 32.0002, Training Accuracy: 0.0969\nEpoch 1384/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0892\nEpoch 1385/8108\nTraining Loss: 32.0021, Training Accuracy: 0.1027\nEpoch 1386/8108\nTraining Loss: 32.0040, Training Accuracy: 0.1108\nEpoch 1387/8108\nTraining Loss: 31.9968, Training Accuracy: 0.1223\nEpoch 1388/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0883\nEpoch 1389/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0907\nEpoch 1390/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0911\nEpoch 1391/8108\nTraining Loss: 31.9849, Training Accuracy: 0.0734\nEpoch 1392/8108\nTraining Loss: 31.9801, Training Accuracy: 0.0955\nEpoch 1393/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0806\nEpoch 1394/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0979\nEpoch 1395/8108\nTraining Loss: 31.9828, Training Accuracy: 0.1127\nEpoch 1396/8108\nTraining Loss: 32.0134, Training Accuracy: 0.0907\nEpoch 1397/8108\nTraining Loss: 31.9861, Training Accuracy: 0.0868\nEpoch 1398/8108\nTraining Loss: 31.9809, Training Accuracy: 0.1194\nEpoch 1399/8108\nTraining Loss: 32.0185, Training Accuracy: 0.0734\nEpoch 1400/8108\nTraining Loss: 31.9802, Training Accuracy: 0.0792\nEpoch 1401/8108\nTraining Loss: 32.0156, Training Accuracy: 0.0768\nEpoch 1402/8108\nTraining Loss: 32.0007, Training Accuracy: 0.1103\nEpoch 1403/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0763\nEpoch 1404/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0849\nEpoch 1405/8108\nTraining Loss: 31.9879, Training Accuracy: 0.1089\nEpoch 1406/8108\nTraining Loss: 32.0045, Training Accuracy: 0.0820\nEpoch 1407/8108\nTraining Loss: 31.9928, Training Accuracy: 0.0868\nEpoch 1408/8108\nTraining Loss: 32.0012, Training Accuracy: 0.0748\nEpoch 1409/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0811\nEpoch 1410/8108\nTraining Loss: 31.9803, Training Accuracy: 0.1003\nEpoch 1411/8108\nTraining Loss: 32.0125, Training Accuracy: 0.0950\nEpoch 1412/8108\nTraining Loss: 31.9670, Training Accuracy: 0.1027\nEpoch 1413/8108\nTraining Loss: 31.9834, Training Accuracy: 0.0777\nEpoch 1414/8108\nTraining Loss: 32.0125, Training Accuracy: 0.0768\nEpoch 1415/8108\nTraining Loss: 32.0114, Training Accuracy: 0.0935\nEpoch 1416/8108\nTraining Loss: 32.0089, Training Accuracy: 0.0604\nEpoch 1417/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0806\nEpoch 1418/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0921\nEpoch 1419/8108\nTraining Loss: 31.9821, Training Accuracy: 0.0748\nEpoch 1420/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0739\nEpoch 1421/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0911\nEpoch 1422/8108\nTraining Loss: 31.9825, Training Accuracy: 0.0724\nEpoch 1423/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0763\nEpoch 1424/8108\nTraining Loss: 31.9985, Training Accuracy: 0.0907\nEpoch 1425/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0576\nEpoch 1426/8108\nTraining Loss: 31.9905, Training Accuracy: 0.1012\nEpoch 1427/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0902\nEpoch 1428/8108\nTraining Loss: 32.0102, Training Accuracy: 0.1003\nEpoch 1429/8108\nTraining Loss: 31.9971, Training Accuracy: 0.0849\nEpoch 1430/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0825\nEpoch 1431/8108\nTraining Loss: 31.9821, Training Accuracy: 0.0907\nEpoch 1432/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0964\nEpoch 1433/8108\nTraining Loss: 31.9831, Training Accuracy: 0.0849\nEpoch 1434/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0907\nEpoch 1435/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0959\nEpoch 1436/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0825\nEpoch 1437/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0734\nEpoch 1438/8108\nTraining Loss: 31.9976, Training Accuracy: 0.0604\nEpoch 1439/8108\nTraining Loss: 31.9940, Training Accuracy: 0.1065\nEpoch 1440/8108\nTraining Loss: 31.9810, Training Accuracy: 0.0806\nEpoch 1441/8108\nTraining Loss: 32.0106, Training Accuracy: 0.0748\nEpoch 1442/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0897\nEpoch 1443/8108\nTraining Loss: 31.9879, Training Accuracy: 0.1099\nEpoch 1444/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0969\nEpoch 1445/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0983\nEpoch 1446/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0806\nEpoch 1447/8108\nTraining Loss: 31.9782, Training Accuracy: 0.1099\nEpoch 1448/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0835\nEpoch 1449/8108\nTraining Loss: 32.0124, Training Accuracy: 0.0907\nEpoch 1450/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0959\nEpoch 1451/8108\nTraining Loss: 31.9901, Training Accuracy: 0.1079\nEpoch 1452/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0729\nEpoch 1453/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0792\nEpoch 1454/8108\nTraining Loss: 31.9806, Training Accuracy: 0.0950\nEpoch 1455/8108\nTraining Loss: 32.0187, Training Accuracy: 0.1027\nEpoch 1456/8108\nTraining Loss: 31.9790, Training Accuracy: 0.1103\nEpoch 1457/8108\nTraining Loss: 31.9821, Training Accuracy: 0.0911\nEpoch 1458/8108\nTraining Loss: 31.9951, Training Accuracy: 0.1055\nEpoch 1459/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0935\nEpoch 1460/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0710\nEpoch 1461/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0835\nEpoch 1462/8108\nTraining Loss: 31.9801, Training Accuracy: 0.1031\nEpoch 1463/8108\nTraining Loss: 32.0117, Training Accuracy: 0.0959\nEpoch 1464/8108\nTraining Loss: 31.9948, Training Accuracy: 0.0768\nEpoch 1465/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0624\nEpoch 1466/8108\nTraining Loss: 31.9890, Training Accuracy: 0.0667\nEpoch 1467/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0619\nEpoch 1468/8108\nTraining Loss: 32.0035, Training Accuracy: 0.0604\nEpoch 1469/8108\nTraining Loss: 31.9982, Training Accuracy: 0.0921\nEpoch 1470/8108\nTraining Loss: 31.9892, Training Accuracy: 0.0830\nEpoch 1471/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0892\nEpoch 1472/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0892\nEpoch 1473/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0648\nEpoch 1474/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0993\nEpoch 1475/8108\nTraining Loss: 31.9976, Training Accuracy: 0.1051\nEpoch 1476/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0739\nEpoch 1477/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0691\nEpoch 1478/8108\nTraining Loss: 31.9840, Training Accuracy: 0.0945\nEpoch 1479/8108\nTraining Loss: 31.9964, Training Accuracy: 0.1046\nEpoch 1480/8108\nTraining Loss: 31.9708, Training Accuracy: 0.1051\nEpoch 1481/8108\nTraining Loss: 31.9841, Training Accuracy: 0.0911\nEpoch 1482/8108\nTraining Loss: 32.0166, Training Accuracy: 0.0878\nEpoch 1483/8108\nTraining Loss: 32.0050, Training Accuracy: 0.0792\nEpoch 1484/8108\nTraining Loss: 31.9871, Training Accuracy: 0.0868\nEpoch 1485/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0998\nEpoch 1486/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0868\nEpoch 1487/8108\nTraining Loss: 32.0023, Training Accuracy: 0.1022\nEpoch 1488/8108\nTraining Loss: 31.9896, Training Accuracy: 0.1012\nEpoch 1489/8108\nTraining Loss: 31.9848, Training Accuracy: 0.0782\nEpoch 1490/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0820\nEpoch 1491/8108\nTraining Loss: 32.0067, Training Accuracy: 0.0806\nEpoch 1492/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0849\nEpoch 1493/8108\nTraining Loss: 31.9848, Training Accuracy: 0.0897\nEpoch 1494/8108\nTraining Loss: 32.0020, Training Accuracy: 0.0892\nEpoch 1495/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0950\nEpoch 1496/8108\nTraining Loss: 31.9984, Training Accuracy: 0.1051\nEpoch 1497/8108\nTraining Loss: 31.9956, Training Accuracy: 0.1036\nEpoch 1498/8108\nTraining Loss: 32.0016, Training Accuracy: 0.1079\nEpoch 1499/8108\nTraining Loss: 31.9889, Training Accuracy: 0.1094\nEpoch 1500/8108\nTraining Loss: 31.9982, Training Accuracy: 0.0748\nEpoch 1501/8108\nTraining Loss: 31.9866, Training Accuracy: 0.0945\nEpoch 1502/8108\nTraining Loss: 31.9853, Training Accuracy: 0.0806\nEpoch 1503/8108\nTraining Loss: 32.0131, Training Accuracy: 0.1022\nEpoch 1504/8108\nTraining Loss: 31.9808, Training Accuracy: 0.0820\nEpoch 1505/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0792\nEpoch 1506/8108\nTraining Loss: 31.9927, Training Accuracy: 0.0950\nEpoch 1507/8108\nTraining Loss: 31.9827, Training Accuracy: 0.0926\nEpoch 1508/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0935\nEpoch 1509/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0955\nEpoch 1510/8108\nTraining Loss: 31.9830, Training Accuracy: 0.1305\nEpoch 1511/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0969\nEpoch 1512/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0724\nEpoch 1513/8108\nTraining Loss: 31.9866, Training Accuracy: 0.0878\nEpoch 1514/8108\nTraining Loss: 31.9832, Training Accuracy: 0.0911\nEpoch 1515/8108\nTraining Loss: 31.9995, Training Accuracy: 0.1079\nEpoch 1516/8108\nTraining Loss: 31.9839, Training Accuracy: 0.0734\nEpoch 1517/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0983\nEpoch 1518/8108\nTraining Loss: 31.9709, Training Accuracy: 0.0777\nEpoch 1519/8108\nTraining Loss: 31.9897, Training Accuracy: 0.1099\nEpoch 1520/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0748\nEpoch 1521/8108\nTraining Loss: 32.0023, Training Accuracy: 0.0748\nEpoch 1522/8108\nTraining Loss: 31.9909, Training Accuracy: 0.0940\nEpoch 1523/8108\nTraining Loss: 32.0019, Training Accuracy: 0.0849\nEpoch 1524/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0792\nEpoch 1525/8108\nTraining Loss: 31.9901, Training Accuracy: 0.1099\nEpoch 1526/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0676\nEpoch 1527/8108\nTraining Loss: 31.9996, Training Accuracy: 0.0705\nEpoch 1528/8108\nTraining Loss: 31.9845, Training Accuracy: 0.0782\nEpoch 1529/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0792\nEpoch 1530/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0849\nEpoch 1531/8108\nTraining Loss: 31.9840, Training Accuracy: 0.0849\nEpoch 1532/8108\nTraining Loss: 31.9928, Training Accuracy: 0.1075\nEpoch 1533/8108\nTraining Loss: 31.9793, Training Accuracy: 0.1204\nEpoch 1534/8108\nTraining Loss: 31.9842, Training Accuracy: 0.0691\nEpoch 1535/8108\nTraining Loss: 32.0178, Training Accuracy: 0.0648\nEpoch 1536/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0878\nEpoch 1537/8108\nTraining Loss: 31.9730, Training Accuracy: 0.0907\nEpoch 1538/8108\nTraining Loss: 32.0036, Training Accuracy: 0.0739\nEpoch 1539/8108\nTraining Loss: 31.9915, Training Accuracy: 0.1103\nEpoch 1540/8108\nTraining Loss: 32.0035, Training Accuracy: 0.0748\nEpoch 1541/8108\nTraining Loss: 32.0081, Training Accuracy: 0.0648\nEpoch 1542/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0811\nEpoch 1543/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0734\nEpoch 1544/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0720\nEpoch 1545/8108\nTraining Loss: 31.9802, Training Accuracy: 0.0926\nEpoch 1546/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0897\nEpoch 1547/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0763\nEpoch 1548/8108\nTraining Loss: 32.0026, Training Accuracy: 0.0691\nEpoch 1549/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0633\nEpoch 1550/8108\nTraining Loss: 31.9801, Training Accuracy: 0.0979\nEpoch 1551/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0907\nEpoch 1552/8108\nTraining Loss: 31.9740, Training Accuracy: 0.0854\nEpoch 1553/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0576\nEpoch 1554/8108\nTraining Loss: 31.9809, Training Accuracy: 0.0849\nEpoch 1555/8108\nTraining Loss: 32.0045, Training Accuracy: 0.0940\nEpoch 1556/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0705\nEpoch 1557/8108\nTraining Loss: 32.0020, Training Accuracy: 0.0792\nEpoch 1558/8108\nTraining Loss: 32.0097, Training Accuracy: 0.0676\nEpoch 1559/8108\nTraining Loss: 31.9923, Training Accuracy: 0.1161\nEpoch 1560/8108\nTraining Loss: 31.9960, Training Accuracy: 0.1012\nEpoch 1561/8108\nTraining Loss: 31.9823, Training Accuracy: 0.0840\nEpoch 1562/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0878\nEpoch 1563/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0854\nEpoch 1564/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0691\nEpoch 1565/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0883\nEpoch 1566/8108\nTraining Loss: 31.9890, Training Accuracy: 0.0691\nEpoch 1567/8108\nTraining Loss: 32.0011, Training Accuracy: 0.1118\nEpoch 1568/8108\nTraining Loss: 32.0015, Training Accuracy: 0.0907\nEpoch 1569/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0825\nEpoch 1570/8108\nTraining Loss: 31.9784, Training Accuracy: 0.0863\nEpoch 1571/8108\nTraining Loss: 32.0174, Training Accuracy: 0.0926\nEpoch 1572/8108\nTraining Loss: 32.0080, Training Accuracy: 0.0777\nEpoch 1573/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0835\nEpoch 1574/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0964\nEpoch 1575/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0849\nEpoch 1576/8108\nTraining Loss: 31.9784, Training Accuracy: 0.1046\nEpoch 1577/8108\nTraining Loss: 31.9981, Training Accuracy: 0.1055\nEpoch 1578/8108\nTraining Loss: 31.9820, Training Accuracy: 0.0777\nEpoch 1579/8108\nTraining Loss: 32.0003, Training Accuracy: 0.0955\nEpoch 1580/8108\nTraining Loss: 31.9766, Training Accuracy: 0.0820\nEpoch 1581/8108\nTraining Loss: 32.0037, Training Accuracy: 0.0940\nEpoch 1582/8108\nTraining Loss: 32.0122, Training Accuracy: 0.0825\nEpoch 1583/8108\nTraining Loss: 31.9696, Training Accuracy: 0.1041\nEpoch 1584/8108\nTraining Loss: 32.0203, Training Accuracy: 0.0840\nEpoch 1585/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0792\nEpoch 1586/8108\nTraining Loss: 31.9935, Training Accuracy: 0.1084\nEpoch 1587/8108\nTraining Loss: 31.9790, Training Accuracy: 0.0921\nEpoch 1588/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0753\nEpoch 1589/8108\nTraining Loss: 32.0080, Training Accuracy: 0.0979\nEpoch 1590/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0950\nEpoch 1591/8108\nTraining Loss: 32.0070, Training Accuracy: 0.0969\nEpoch 1592/8108\nTraining Loss: 31.9714, Training Accuracy: 0.0748\nEpoch 1593/8108\nTraining Loss: 32.0031, Training Accuracy: 0.0796\nEpoch 1594/8108\nTraining Loss: 31.9723, Training Accuracy: 0.1089\nEpoch 1595/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0768\nEpoch 1596/8108\nTraining Loss: 32.0050, Training Accuracy: 0.1027\nEpoch 1597/8108\nTraining Loss: 31.9916, Training Accuracy: 0.1017\nEpoch 1598/8108\nTraining Loss: 32.0105, Training Accuracy: 0.0820\nEpoch 1599/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0748\nEpoch 1600/8108\nTraining Loss: 31.9843, Training Accuracy: 0.0840\nEpoch 1601/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0662\nEpoch 1602/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0748\nEpoch 1603/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0792\nEpoch 1604/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0940\nEpoch 1605/8108\nTraining Loss: 32.0035, Training Accuracy: 0.0878\nEpoch 1606/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0840\nEpoch 1607/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0748\nEpoch 1608/8108\nTraining Loss: 32.0123, Training Accuracy: 0.0820\nEpoch 1609/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0921\nEpoch 1610/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0796\nEpoch 1611/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0979\nEpoch 1612/8108\nTraining Loss: 31.9890, Training Accuracy: 0.1099\nEpoch 1613/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0825\nEpoch 1614/8108\nTraining Loss: 31.9880, Training Accuracy: 0.0863\nEpoch 1615/8108\nTraining Loss: 31.9891, Training Accuracy: 0.0863\nEpoch 1616/8108\nTraining Loss: 31.9834, Training Accuracy: 0.1027\nEpoch 1617/8108\nTraining Loss: 31.9890, Training Accuracy: 0.0763\nEpoch 1618/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0777\nEpoch 1619/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0940\nEpoch 1620/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0897\nEpoch 1621/8108\nTraining Loss: 31.9737, Training Accuracy: 0.0676\nEpoch 1622/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0811\nEpoch 1623/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0868\nEpoch 1624/8108\nTraining Loss: 32.0112, Training Accuracy: 0.0840\nEpoch 1625/8108\nTraining Loss: 31.9845, Training Accuracy: 0.0883\nEpoch 1626/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0705\nEpoch 1627/8108\nTraining Loss: 32.0170, Training Accuracy: 0.0835\nEpoch 1628/8108\nTraining Loss: 31.9905, Training Accuracy: 0.1012\nEpoch 1629/8108\nTraining Loss: 31.9950, Training Accuracy: 0.1041\nEpoch 1630/8108\nTraining Loss: 32.0116, Training Accuracy: 0.0792\nEpoch 1631/8108\nTraining Loss: 32.0050, Training Accuracy: 0.0935\nEpoch 1632/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0907\nEpoch 1633/8108\nTraining Loss: 31.9916, Training Accuracy: 0.1185\nEpoch 1634/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0676\nEpoch 1635/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0940\nEpoch 1636/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0811\nEpoch 1637/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0907\nEpoch 1638/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0955\nEpoch 1639/8108\nTraining Loss: 31.9899, Training Accuracy: 0.0806\nEpoch 1640/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0561\nEpoch 1641/8108\nTraining Loss: 31.9886, Training Accuracy: 0.0883\nEpoch 1642/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0892\nEpoch 1643/8108\nTraining Loss: 31.9768, Training Accuracy: 0.1046\nEpoch 1644/8108\nTraining Loss: 32.0083, Training Accuracy: 0.0811\nEpoch 1645/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0911\nEpoch 1646/8108\nTraining Loss: 31.9986, Training Accuracy: 0.1161\nEpoch 1647/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0710\nEpoch 1648/8108\nTraining Loss: 32.0035, Training Accuracy: 0.0792\nEpoch 1649/8108\nTraining Loss: 31.9827, Training Accuracy: 0.0950\nEpoch 1650/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0868\nEpoch 1651/8108\nTraining Loss: 31.9843, Training Accuracy: 0.0907\nEpoch 1652/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0926\nEpoch 1653/8108\nTraining Loss: 32.0058, Training Accuracy: 0.0782\nEpoch 1654/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0883\nEpoch 1655/8108\nTraining Loss: 31.9919, Training Accuracy: 0.1147\nEpoch 1656/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0854\nEpoch 1657/8108\nTraining Loss: 32.0078, Training Accuracy: 0.0897\nEpoch 1658/8108\nTraining Loss: 32.0095, Training Accuracy: 0.0782\nEpoch 1659/8108\nTraining Loss: 32.0081, Training Accuracy: 0.0878\nEpoch 1660/8108\nTraining Loss: 32.0029, Training Accuracy: 0.0772\nEpoch 1661/8108\nTraining Loss: 32.0062, Training Accuracy: 0.0662\nEpoch 1662/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0878\nEpoch 1663/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0988\nEpoch 1664/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0926\nEpoch 1665/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0748\nEpoch 1666/8108\nTraining Loss: 31.9785, Training Accuracy: 0.0878\nEpoch 1667/8108\nTraining Loss: 32.0174, Training Accuracy: 0.0820\nEpoch 1668/8108\nTraining Loss: 32.0142, Training Accuracy: 0.0811\nEpoch 1669/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0849\nEpoch 1670/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0883\nEpoch 1671/8108\nTraining Loss: 31.9800, Training Accuracy: 0.0983\nEpoch 1672/8108\nTraining Loss: 32.0012, Training Accuracy: 0.0720\nEpoch 1673/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0878\nEpoch 1674/8108\nTraining Loss: 32.0123, Training Accuracy: 0.0998\nEpoch 1675/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0825\nEpoch 1676/8108\nTraining Loss: 31.9835, Training Accuracy: 0.0720\nEpoch 1677/8108\nTraining Loss: 32.0119, Training Accuracy: 0.0916\nEpoch 1678/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0734\nEpoch 1679/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0739\nEpoch 1680/8108\nTraining Loss: 31.9777, Training Accuracy: 0.0748\nEpoch 1681/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0768\nEpoch 1682/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0849\nEpoch 1683/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0720\nEpoch 1684/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0720\nEpoch 1685/8108\nTraining Loss: 32.0065, Training Accuracy: 0.0648\nEpoch 1686/8108\nTraining Loss: 31.9927, Training Accuracy: 0.0705\nEpoch 1687/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0964\nEpoch 1688/8108\nTraining Loss: 32.0188, Training Accuracy: 0.0705\nEpoch 1689/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0777\nEpoch 1690/8108\nTraining Loss: 31.9875, Training Accuracy: 0.0748\nEpoch 1691/8108\nTraining Loss: 31.9742, Training Accuracy: 0.0878\nEpoch 1692/8108\nTraining Loss: 32.0112, Training Accuracy: 0.0863\nEpoch 1693/8108\nTraining Loss: 31.9854, Training Accuracy: 0.0940\nEpoch 1694/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0720\nEpoch 1695/8108\nTraining Loss: 31.9742, Training Accuracy: 0.1185\nEpoch 1696/8108\nTraining Loss: 31.9893, Training Accuracy: 0.1003\nEpoch 1697/8108\nTraining Loss: 31.9880, Training Accuracy: 0.0921\nEpoch 1698/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0772\nEpoch 1699/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0907\nEpoch 1700/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0979\nEpoch 1701/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0782\nEpoch 1702/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0935\nEpoch 1703/8108\nTraining Loss: 31.9745, Training Accuracy: 0.1041\nEpoch 1704/8108\nTraining Loss: 31.9857, Training Accuracy: 0.0748\nEpoch 1705/8108\nTraining Loss: 32.0023, Training Accuracy: 0.0998\nEpoch 1706/8108\nTraining Loss: 32.0015, Training Accuracy: 0.0777\nEpoch 1707/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0854\nEpoch 1708/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0806\nEpoch 1709/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0811\nEpoch 1710/8108\nTraining Loss: 32.0237, Training Accuracy: 0.0792\nEpoch 1711/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0590\nEpoch 1712/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0863\nEpoch 1713/8108\nTraining Loss: 32.0088, Training Accuracy: 0.0820\nEpoch 1714/8108\nTraining Loss: 31.9877, Training Accuracy: 0.0849\nEpoch 1715/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0883\nEpoch 1716/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0926\nEpoch 1717/8108\nTraining Loss: 32.0126, Training Accuracy: 0.0796\nEpoch 1718/8108\nTraining Loss: 31.9900, Training Accuracy: 0.1003\nEpoch 1719/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0892\nEpoch 1720/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0811\nEpoch 1721/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0806\nEpoch 1722/8108\nTraining Loss: 31.9862, Training Accuracy: 0.1041\nEpoch 1723/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0849\nEpoch 1724/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0777\nEpoch 1725/8108\nTraining Loss: 31.9981, Training Accuracy: 0.1075\nEpoch 1726/8108\nTraining Loss: 32.0045, Training Accuracy: 0.0662\nEpoch 1727/8108\nTraining Loss: 31.9846, Training Accuracy: 0.0863\nEpoch 1728/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0897\nEpoch 1729/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0979\nEpoch 1730/8108\nTraining Loss: 31.9883, Training Accuracy: 0.0796\nEpoch 1731/8108\nTraining Loss: 31.9949, Training Accuracy: 0.0863\nEpoch 1732/8108\nTraining Loss: 32.0134, Training Accuracy: 0.0964\nEpoch 1733/8108\nTraining Loss: 31.9963, Training Accuracy: 0.0921\nEpoch 1734/8108\nTraining Loss: 32.0069, Training Accuracy: 0.0878\nEpoch 1735/8108\nTraining Loss: 31.9772, Training Accuracy: 0.0868\nEpoch 1736/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0868\nEpoch 1737/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0768\nEpoch 1738/8108\nTraining Loss: 32.0072, Training Accuracy: 0.0561\nEpoch 1739/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0940\nEpoch 1740/8108\nTraining Loss: 31.9670, Training Accuracy: 0.0820\nEpoch 1741/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0950\nEpoch 1742/8108\nTraining Loss: 32.0088, Training Accuracy: 0.0863\nEpoch 1743/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0532\nEpoch 1744/8108\nTraining Loss: 32.0018, Training Accuracy: 0.0624\nEpoch 1745/8108\nTraining Loss: 32.0050, Training Accuracy: 0.0868\nEpoch 1746/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0983\nEpoch 1747/8108\nTraining Loss: 31.9892, Training Accuracy: 0.0998\nEpoch 1748/8108\nTraining Loss: 32.0101, Training Accuracy: 0.0648\nEpoch 1749/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0734\nEpoch 1750/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0911\nEpoch 1751/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0777\nEpoch 1752/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0825\nEpoch 1753/8108\nTraining Loss: 31.9740, Training Accuracy: 0.0840\nEpoch 1754/8108\nTraining Loss: 32.0158, Training Accuracy: 0.0940\nEpoch 1755/8108\nTraining Loss: 31.9835, Training Accuracy: 0.0959\nEpoch 1756/8108\nTraining Loss: 31.9838, Training Accuracy: 0.0940\nEpoch 1757/8108\nTraining Loss: 32.0147, Training Accuracy: 0.0763\nEpoch 1758/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0820\nEpoch 1759/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0763\nEpoch 1760/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0868\nEpoch 1761/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0897\nEpoch 1762/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0792\nEpoch 1763/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0950\nEpoch 1764/8108\nTraining Loss: 31.9820, Training Accuracy: 0.0782\nEpoch 1765/8108\nTraining Loss: 32.0201, Training Accuracy: 0.0969\nEpoch 1766/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0955\nEpoch 1767/8108\nTraining Loss: 31.9697, Training Accuracy: 0.0734\nEpoch 1768/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0993\nEpoch 1769/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0820\nEpoch 1770/8108\nTraining Loss: 31.9833, Training Accuracy: 0.1046\nEpoch 1771/8108\nTraining Loss: 32.0048, Training Accuracy: 0.1031\nEpoch 1772/8108\nTraining Loss: 31.9972, Training Accuracy: 0.1012\nEpoch 1773/8108\nTraining Loss: 31.9895, Training Accuracy: 0.0854\nEpoch 1774/8108\nTraining Loss: 32.0139, Training Accuracy: 0.0576\nEpoch 1775/8108\nTraining Loss: 31.9856, Training Accuracy: 0.0734\nEpoch 1776/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0897\nEpoch 1777/8108\nTraining Loss: 31.9819, Training Accuracy: 0.1027\nEpoch 1778/8108\nTraining Loss: 31.9694, Training Accuracy: 0.1075\nEpoch 1779/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0835\nEpoch 1780/8108\nTraining Loss: 31.9823, Training Accuracy: 0.0662\nEpoch 1781/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0705\nEpoch 1782/8108\nTraining Loss: 31.9788, Training Accuracy: 0.1041\nEpoch 1783/8108\nTraining Loss: 32.0084, Training Accuracy: 0.0940\nEpoch 1784/8108\nTraining Loss: 32.0029, Training Accuracy: 0.1017\nEpoch 1785/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0835\nEpoch 1786/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0926\nEpoch 1787/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0935\nEpoch 1788/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0806\nEpoch 1789/8108\nTraining Loss: 31.9799, Training Accuracy: 0.0993\nEpoch 1790/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0820\nEpoch 1791/8108\nTraining Loss: 31.9825, Training Accuracy: 0.0902\nEpoch 1792/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0940\nEpoch 1793/8108\nTraining Loss: 31.9792, Training Accuracy: 0.0820\nEpoch 1794/8108\nTraining Loss: 31.9941, Training Accuracy: 0.1031\nEpoch 1795/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0748\nEpoch 1796/8108\nTraining Loss: 32.0146, Training Accuracy: 0.0878\nEpoch 1797/8108\nTraining Loss: 31.9827, Training Accuracy: 0.1003\nEpoch 1798/8108\nTraining Loss: 31.9908, Training Accuracy: 0.1041\nEpoch 1799/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0969\nEpoch 1800/8108\nTraining Loss: 31.9875, Training Accuracy: 0.0811\nEpoch 1801/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0763\nEpoch 1802/8108\nTraining Loss: 32.0128, Training Accuracy: 0.0897\nEpoch 1803/8108\nTraining Loss: 31.9848, Training Accuracy: 0.0835\nEpoch 1804/8108\nTraining Loss: 31.9797, Training Accuracy: 0.0590\nEpoch 1805/8108\nTraining Loss: 31.9985, Training Accuracy: 0.1012\nEpoch 1806/8108\nTraining Loss: 31.9891, Training Accuracy: 0.0863\nEpoch 1807/8108\nTraining Loss: 31.9770, Training Accuracy: 0.1103\nEpoch 1808/8108\nTraining Loss: 31.9756, Training Accuracy: 0.0830\nEpoch 1809/8108\nTraining Loss: 31.9976, Training Accuracy: 0.1007\nEpoch 1810/8108\nTraining Loss: 32.0045, Training Accuracy: 0.0811\nEpoch 1811/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0945\nEpoch 1812/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0782\nEpoch 1813/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0983\nEpoch 1814/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0806\nEpoch 1815/8108\nTraining Loss: 31.9890, Training Accuracy: 0.1065\nEpoch 1816/8108\nTraining Loss: 31.9827, Training Accuracy: 0.0849\nEpoch 1817/8108\nTraining Loss: 32.0146, Training Accuracy: 0.0590\nEpoch 1818/8108\nTraining Loss: 31.9815, Training Accuracy: 0.1123\nEpoch 1819/8108\nTraining Loss: 31.9893, Training Accuracy: 0.0883\nEpoch 1820/8108\nTraining Loss: 32.0090, Training Accuracy: 0.0897\nEpoch 1821/8108\nTraining Loss: 32.0163, Training Accuracy: 0.0883\nEpoch 1822/8108\nTraining Loss: 31.9779, Training Accuracy: 0.0897\nEpoch 1823/8108\nTraining Loss: 32.0137, Training Accuracy: 0.0945\nEpoch 1824/8108\nTraining Loss: 31.9924, Training Accuracy: 0.0883\nEpoch 1825/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0763\nEpoch 1826/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0705\nEpoch 1827/8108\nTraining Loss: 31.9828, Training Accuracy: 0.0835\nEpoch 1828/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0916\nEpoch 1829/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0911\nEpoch 1830/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0969\nEpoch 1831/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0806\nEpoch 1832/8108\nTraining Loss: 31.9867, Training Accuracy: 0.0777\nEpoch 1833/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0940\nEpoch 1834/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0734\nEpoch 1835/8108\nTraining Loss: 31.9762, Training Accuracy: 0.1012\nEpoch 1836/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0863\nEpoch 1837/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0883\nEpoch 1838/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0840\nEpoch 1839/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0907\nEpoch 1840/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0705\nEpoch 1841/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0998\nEpoch 1842/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0854\nEpoch 1843/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0806\nEpoch 1844/8108\nTraining Loss: 32.0106, Training Accuracy: 0.0950\nEpoch 1845/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0835\nEpoch 1846/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0863\nEpoch 1847/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0907\nEpoch 1848/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0720\nEpoch 1849/8108\nTraining Loss: 31.9975, Training Accuracy: 0.1031\nEpoch 1850/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0662\nEpoch 1851/8108\nTraining Loss: 31.9871, Training Accuracy: 0.0792\nEpoch 1852/8108\nTraining Loss: 32.0127, Training Accuracy: 0.0806\nEpoch 1853/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0777\nEpoch 1854/8108\nTraining Loss: 32.0252, Training Accuracy: 0.0926\nEpoch 1855/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0859\nEpoch 1856/8108\nTraining Loss: 32.0002, Training Accuracy: 0.0883\nEpoch 1857/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0878\nEpoch 1858/8108\nTraining Loss: 32.0020, Training Accuracy: 0.1012\nEpoch 1859/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0777\nEpoch 1860/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0859\nEpoch 1861/8108\nTraining Loss: 31.9890, Training Accuracy: 0.1007\nEpoch 1862/8108\nTraining Loss: 32.0010, Training Accuracy: 0.1041\nEpoch 1863/8108\nTraining Loss: 31.9718, Training Accuracy: 0.0921\nEpoch 1864/8108\nTraining Loss: 32.0055, Training Accuracy: 0.0820\nEpoch 1865/8108\nTraining Loss: 32.0119, Training Accuracy: 0.0907\nEpoch 1866/8108\nTraining Loss: 32.0121, Training Accuracy: 0.0734\nEpoch 1867/8108\nTraining Loss: 32.0079, Training Accuracy: 0.0883\nEpoch 1868/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0691\nEpoch 1869/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0897\nEpoch 1870/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0935\nEpoch 1871/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0835\nEpoch 1872/8108\nTraining Loss: 31.9846, Training Accuracy: 0.0676\nEpoch 1873/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0835\nEpoch 1874/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0979\nEpoch 1875/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0979\nEpoch 1876/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0969\nEpoch 1877/8108\nTraining Loss: 31.9986, Training Accuracy: 0.1113\nEpoch 1878/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0897\nEpoch 1879/8108\nTraining Loss: 31.9864, Training Accuracy: 0.1041\nEpoch 1880/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0748\nEpoch 1881/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0892\nEpoch 1882/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0844\nEpoch 1883/8108\nTraining Loss: 31.9924, Training Accuracy: 0.0792\nEpoch 1884/8108\nTraining Loss: 32.0067, Training Accuracy: 0.0950\nEpoch 1885/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0993\nEpoch 1886/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0561\nEpoch 1887/8108\nTraining Loss: 31.9755, Training Accuracy: 0.0974\nEpoch 1888/8108\nTraining Loss: 31.9860, Training Accuracy: 0.0911\nEpoch 1889/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0892\nEpoch 1890/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0696\nEpoch 1891/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0820\nEpoch 1892/8108\nTraining Loss: 32.0038, Training Accuracy: 0.0763\nEpoch 1893/8108\nTraining Loss: 32.0086, Training Accuracy: 0.0705\nEpoch 1894/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0993\nEpoch 1895/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0820\nEpoch 1896/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0724\nEpoch 1897/8108\nTraining Loss: 31.9884, Training Accuracy: 0.1046\nEpoch 1898/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0748\nEpoch 1899/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0955\nEpoch 1900/8108\nTraining Loss: 31.9859, Training Accuracy: 0.1070\nEpoch 1901/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0849\nEpoch 1902/8108\nTraining Loss: 31.9938, Training Accuracy: 0.1214\nEpoch 1903/8108\nTraining Loss: 32.0076, Training Accuracy: 0.0748\nEpoch 1904/8108\nTraining Loss: 32.0088, Training Accuracy: 0.0892\nEpoch 1905/8108\nTraining Loss: 32.0119, Training Accuracy: 0.0705\nEpoch 1906/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0734\nEpoch 1907/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0993\nEpoch 1908/8108\nTraining Loss: 31.9965, Training Accuracy: 0.1007\nEpoch 1909/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0676\nEpoch 1910/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0667\nEpoch 1911/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0854\nEpoch 1912/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0868\nEpoch 1913/8108\nTraining Loss: 32.0087, Training Accuracy: 0.1075\nEpoch 1914/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0734\nEpoch 1915/8108\nTraining Loss: 32.0162, Training Accuracy: 0.0691\nEpoch 1916/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0849\nEpoch 1917/8108\nTraining Loss: 32.0002, Training Accuracy: 0.0840\nEpoch 1918/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0777\nEpoch 1919/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0648\nEpoch 1920/8108\nTraining Loss: 31.9902, Training Accuracy: 0.0897\nEpoch 1921/8108\nTraining Loss: 32.0140, Training Accuracy: 0.0854\nEpoch 1922/8108\nTraining Loss: 31.9883, Training Accuracy: 0.0825\nEpoch 1923/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0983\nEpoch 1924/8108\nTraining Loss: 31.9875, Training Accuracy: 0.0940\nEpoch 1925/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0796\nEpoch 1926/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0926\nEpoch 1927/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0911\nEpoch 1928/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0940\nEpoch 1929/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0983\nEpoch 1930/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0676\nEpoch 1931/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0878\nEpoch 1932/8108\nTraining Loss: 31.9707, Training Accuracy: 0.0897\nEpoch 1933/8108\nTraining Loss: 32.0122, Training Accuracy: 0.0955\nEpoch 1934/8108\nTraining Loss: 31.9985, Training Accuracy: 0.1041\nEpoch 1935/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0926\nEpoch 1936/8108\nTraining Loss: 31.9662, Training Accuracy: 0.1017\nEpoch 1937/8108\nTraining Loss: 31.9629, Training Accuracy: 0.1003\nEpoch 1938/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0806\nEpoch 1939/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0868\nEpoch 1940/8108\nTraining Loss: 31.9873, Training Accuracy: 0.1084\nEpoch 1941/8108\nTraining Loss: 31.9849, Training Accuracy: 0.1089\nEpoch 1942/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0907\nEpoch 1943/8108\nTraining Loss: 31.9963, Training Accuracy: 0.0811\nEpoch 1944/8108\nTraining Loss: 32.0083, Training Accuracy: 0.0950\nEpoch 1945/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0969\nEpoch 1946/8108\nTraining Loss: 31.9904, Training Accuracy: 0.1022\nEpoch 1947/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0782\nEpoch 1948/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0768\nEpoch 1949/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0748\nEpoch 1950/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0878\nEpoch 1951/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0868\nEpoch 1952/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0820\nEpoch 1953/8108\nTraining Loss: 31.9996, Training Accuracy: 0.0705\nEpoch 1954/8108\nTraining Loss: 31.9826, Training Accuracy: 0.1022\nEpoch 1955/8108\nTraining Loss: 31.9893, Training Accuracy: 0.1075\nEpoch 1956/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0825\nEpoch 1957/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0806\nEpoch 1958/8108\nTraining Loss: 31.9953, Training Accuracy: 0.1003\nEpoch 1959/8108\nTraining Loss: 31.9906, Training Accuracy: 0.0940\nEpoch 1960/8108\nTraining Loss: 31.9833, Training Accuracy: 0.0983\nEpoch 1961/8108\nTraining Loss: 31.9849, Training Accuracy: 0.0648\nEpoch 1962/8108\nTraining Loss: 32.0130, Training Accuracy: 0.0777\nEpoch 1963/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0863\nEpoch 1964/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0748\nEpoch 1965/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0840\nEpoch 1966/8108\nTraining Loss: 32.0031, Training Accuracy: 0.0676\nEpoch 1967/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0648\nEpoch 1968/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0935\nEpoch 1969/8108\nTraining Loss: 31.9717, Training Accuracy: 0.0950\nEpoch 1970/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0892\nEpoch 1971/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0983\nEpoch 1972/8108\nTraining Loss: 32.0023, Training Accuracy: 0.1060\nEpoch 1973/8108\nTraining Loss: 31.9839, Training Accuracy: 0.0734\nEpoch 1974/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0777\nEpoch 1975/8108\nTraining Loss: 31.9722, Training Accuracy: 0.0935\nEpoch 1976/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0705\nEpoch 1977/8108\nTraining Loss: 31.9852, Training Accuracy: 0.1137\nEpoch 1978/8108\nTraining Loss: 32.0200, Training Accuracy: 0.0806\nEpoch 1979/8108\nTraining Loss: 31.9922, Training Accuracy: 0.0863\nEpoch 1980/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0964\nEpoch 1981/8108\nTraining Loss: 31.9716, Training Accuracy: 0.0902\nEpoch 1982/8108\nTraining Loss: 31.9635, Training Accuracy: 0.1113\nEpoch 1983/8108\nTraining Loss: 32.0074, Training Accuracy: 0.0753\nEpoch 1984/8108\nTraining Loss: 31.9948, Training Accuracy: 0.0935\nEpoch 1985/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0892\nEpoch 1986/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0806\nEpoch 1987/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0748\nEpoch 1988/8108\nTraining Loss: 31.9918, Training Accuracy: 0.1036\nEpoch 1989/8108\nTraining Loss: 32.0057, Training Accuracy: 0.1171\nEpoch 1990/8108\nTraining Loss: 31.9775, Training Accuracy: 0.0849\nEpoch 1991/8108\nTraining Loss: 32.0003, Training Accuracy: 0.0691\nEpoch 1992/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0849\nEpoch 1993/8108\nTraining Loss: 31.9798, Training Accuracy: 0.1036\nEpoch 1994/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0849\nEpoch 1995/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0983\nEpoch 1996/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0931\nEpoch 1997/8108\nTraining Loss: 32.0038, Training Accuracy: 0.0792\nEpoch 1998/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0940\nEpoch 1999/8108\nTraining Loss: 31.9893, Training Accuracy: 0.0964\nEpoch 2000/8108\nTraining Loss: 31.9855, Training Accuracy: 0.1046\nEpoch 2001/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0844\nEpoch 2002/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0806\nEpoch 2003/8108\nTraining Loss: 31.9796, Training Accuracy: 0.0835\nEpoch 2004/8108\nTraining Loss: 32.0116, Training Accuracy: 0.0892\nEpoch 2005/8108\nTraining Loss: 32.0106, Training Accuracy: 0.1022\nEpoch 2006/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0849\nEpoch 2007/8108\nTraining Loss: 31.9984, Training Accuracy: 0.1070\nEpoch 2008/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0854\nEpoch 2009/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0763\nEpoch 2010/8108\nTraining Loss: 31.9918, Training Accuracy: 0.1055\nEpoch 2011/8108\nTraining Loss: 31.9785, Training Accuracy: 0.0792\nEpoch 2012/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0921\nEpoch 2013/8108\nTraining Loss: 32.0093, Training Accuracy: 0.0863\nEpoch 2014/8108\nTraining Loss: 31.9860, Training Accuracy: 0.0883\nEpoch 2015/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0964\nEpoch 2016/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0863\nEpoch 2017/8108\nTraining Loss: 31.9917, Training Accuracy: 0.1012\nEpoch 2018/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0897\nEpoch 2019/8108\nTraining Loss: 32.0121, Training Accuracy: 0.1075\nEpoch 2020/8108\nTraining Loss: 31.9764, Training Accuracy: 0.0835\nEpoch 2021/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0820\nEpoch 2022/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0792\nEpoch 2023/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0897\nEpoch 2024/8108\nTraining Loss: 31.9865, Training Accuracy: 0.0792\nEpoch 2025/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0969\nEpoch 2026/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0863\nEpoch 2027/8108\nTraining Loss: 31.9927, Training Accuracy: 0.0662\nEpoch 2028/8108\nTraining Loss: 32.0225, Training Accuracy: 0.0782\nEpoch 2029/8108\nTraining Loss: 31.9923, Training Accuracy: 0.1055\nEpoch 2030/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0753\nEpoch 2031/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0940\nEpoch 2032/8108\nTraining Loss: 32.0109, Training Accuracy: 0.0926\nEpoch 2033/8108\nTraining Loss: 32.0107, Training Accuracy: 0.0840\nEpoch 2034/8108\nTraining Loss: 31.9724, Training Accuracy: 0.1266\nEpoch 2035/8108\nTraining Loss: 31.9849, Training Accuracy: 0.0902\nEpoch 2036/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0748\nEpoch 2037/8108\nTraining Loss: 31.9701, Training Accuracy: 0.0993\nEpoch 2038/8108\nTraining Loss: 32.0166, Training Accuracy: 0.0753\nEpoch 2039/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0763\nEpoch 2040/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0619\nEpoch 2041/8108\nTraining Loss: 31.9838, Training Accuracy: 0.0940\nEpoch 2042/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0878\nEpoch 2043/8108\nTraining Loss: 32.0115, Training Accuracy: 0.0619\nEpoch 2044/8108\nTraining Loss: 31.9971, Training Accuracy: 0.0926\nEpoch 2045/8108\nTraining Loss: 32.0015, Training Accuracy: 0.0945\nEpoch 2046/8108\nTraining Loss: 32.0018, Training Accuracy: 0.0705\nEpoch 2047/8108\nTraining Loss: 31.9815, Training Accuracy: 0.0820\nEpoch 2048/8108\nTraining Loss: 31.9851, Training Accuracy: 0.0926\nEpoch 2049/8108\nTraining Loss: 32.0038, Training Accuracy: 0.0820\nEpoch 2050/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0561\nEpoch 2051/8108\nTraining Loss: 31.9759, Training Accuracy: 0.1118\nEpoch 2052/8108\nTraining Loss: 31.9688, Training Accuracy: 0.0950\nEpoch 2053/8108\nTraining Loss: 31.9810, Training Accuracy: 0.0897\nEpoch 2054/8108\nTraining Loss: 32.0188, Training Accuracy: 0.0633\nEpoch 2055/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0806\nEpoch 2056/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0806\nEpoch 2057/8108\nTraining Loss: 31.9836, Training Accuracy: 0.0835\nEpoch 2058/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0883\nEpoch 2059/8108\nTraining Loss: 31.9814, Training Accuracy: 0.1060\nEpoch 2060/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0840\nEpoch 2061/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0806\nEpoch 2062/8108\nTraining Loss: 32.0003, Training Accuracy: 0.1103\nEpoch 2063/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0950\nEpoch 2064/8108\nTraining Loss: 31.9974, Training Accuracy: 0.1041\nEpoch 2065/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0907\nEpoch 2066/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0820\nEpoch 2067/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0820\nEpoch 2068/8108\nTraining Loss: 31.9912, Training Accuracy: 0.1022\nEpoch 2069/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0748\nEpoch 2070/8108\nTraining Loss: 31.9962, Training Accuracy: 0.0878\nEpoch 2071/8108\nTraining Loss: 31.9939, Training Accuracy: 0.0820\nEpoch 2072/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0878\nEpoch 2073/8108\nTraining Loss: 31.9851, Training Accuracy: 0.0983\nEpoch 2074/8108\nTraining Loss: 31.9894, Training Accuracy: 0.0792\nEpoch 2075/8108\nTraining Loss: 31.9961, Training Accuracy: 0.1084\nEpoch 2076/8108\nTraining Loss: 31.9880, Training Accuracy: 0.0796\nEpoch 2077/8108\nTraining Loss: 32.0070, Training Accuracy: 0.0921\nEpoch 2078/8108\nTraining Loss: 32.0110, Training Accuracy: 0.0801\nEpoch 2079/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0820\nEpoch 2080/8108\nTraining Loss: 31.9814, Training Accuracy: 0.0806\nEpoch 2081/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0950\nEpoch 2082/8108\nTraining Loss: 31.9895, Training Accuracy: 0.1017\nEpoch 2083/8108\nTraining Loss: 31.9787, Training Accuracy: 0.0969\nEpoch 2084/8108\nTraining Loss: 32.0078, Training Accuracy: 0.0907\nEpoch 2085/8108\nTraining Loss: 31.9852, Training Accuracy: 0.0662\nEpoch 2086/8108\nTraining Loss: 31.9842, Training Accuracy: 0.0868\nEpoch 2087/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0691\nEpoch 2088/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0768\nEpoch 2089/8108\nTraining Loss: 31.9756, Training Accuracy: 0.0974\nEpoch 2090/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0806\nEpoch 2091/8108\nTraining Loss: 32.0110, Training Accuracy: 0.0748\nEpoch 2092/8108\nTraining Loss: 31.9844, Training Accuracy: 0.1046\nEpoch 2093/8108\nTraining Loss: 31.9962, Training Accuracy: 0.0998\nEpoch 2094/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0820\nEpoch 2095/8108\nTraining Loss: 31.9883, Training Accuracy: 0.0911\nEpoch 2096/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0835\nEpoch 2097/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0676\nEpoch 2098/8108\nTraining Loss: 31.9833, Training Accuracy: 0.1055\nEpoch 2099/8108\nTraining Loss: 31.9861, Training Accuracy: 0.0883\nEpoch 2100/8108\nTraining Loss: 32.0009, Training Accuracy: 0.0964\nEpoch 2101/8108\nTraining Loss: 31.9748, Training Accuracy: 0.0969\nEpoch 2102/8108\nTraining Loss: 31.9806, Training Accuracy: 0.0763\nEpoch 2103/8108\nTraining Loss: 32.0028, Training Accuracy: 0.0820\nEpoch 2104/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0907\nEpoch 2105/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0739\nEpoch 2106/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0820\nEpoch 2107/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0959\nEpoch 2108/8108\nTraining Loss: 31.9728, Training Accuracy: 0.0681\nEpoch 2109/8108\nTraining Loss: 32.0136, Training Accuracy: 0.0619\nEpoch 2110/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0863\nEpoch 2111/8108\nTraining Loss: 31.9806, Training Accuracy: 0.0979\nEpoch 2112/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0983\nEpoch 2113/8108\nTraining Loss: 31.9927, Training Accuracy: 0.0921\nEpoch 2114/8108\nTraining Loss: 32.0009, Training Accuracy: 0.0854\nEpoch 2115/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0720\nEpoch 2116/8108\nTraining Loss: 31.9963, Training Accuracy: 0.0748\nEpoch 2117/8108\nTraining Loss: 32.0066, Training Accuracy: 0.0863\nEpoch 2118/8108\nTraining Loss: 32.0087, Training Accuracy: 0.0959\nEpoch 2119/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0777\nEpoch 2120/8108\nTraining Loss: 31.9897, Training Accuracy: 0.0916\nEpoch 2121/8108\nTraining Loss: 31.9785, Training Accuracy: 0.0931\nEpoch 2122/8108\nTraining Loss: 31.9777, Training Accuracy: 0.0840\nEpoch 2123/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0840\nEpoch 2124/8108\nTraining Loss: 31.9886, Training Accuracy: 0.0844\nEpoch 2125/8108\nTraining Loss: 32.0030, Training Accuracy: 0.1055\nEpoch 2126/8108\nTraining Loss: 32.0174, Training Accuracy: 0.0705\nEpoch 2127/8108\nTraining Loss: 32.0196, Training Accuracy: 0.0806\nEpoch 2128/8108\nTraining Loss: 32.0069, Training Accuracy: 0.0777\nEpoch 2129/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0691\nEpoch 2130/8108\nTraining Loss: 32.0063, Training Accuracy: 0.0883\nEpoch 2131/8108\nTraining Loss: 32.0148, Training Accuracy: 0.0633\nEpoch 2132/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0849\nEpoch 2133/8108\nTraining Loss: 31.9657, Training Accuracy: 0.0806\nEpoch 2134/8108\nTraining Loss: 31.9782, Training Accuracy: 0.1022\nEpoch 2135/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0633\nEpoch 2136/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0676\nEpoch 2137/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0926\nEpoch 2138/8108\nTraining Loss: 31.9860, Training Accuracy: 0.0955\nEpoch 2139/8108\nTraining Loss: 31.9695, Training Accuracy: 0.0835\nEpoch 2140/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0662\nEpoch 2141/8108\nTraining Loss: 31.9807, Training Accuracy: 0.0911\nEpoch 2142/8108\nTraining Loss: 31.9902, Training Accuracy: 0.0926\nEpoch 2143/8108\nTraining Loss: 31.9833, Training Accuracy: 0.0868\nEpoch 2144/8108\nTraining Loss: 31.9716, Training Accuracy: 0.0873\nEpoch 2145/8108\nTraining Loss: 32.0082, Training Accuracy: 0.0840\nEpoch 2146/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0763\nEpoch 2147/8108\nTraining Loss: 31.9800, Training Accuracy: 0.0902\nEpoch 2148/8108\nTraining Loss: 31.9687, Training Accuracy: 0.0878\nEpoch 2149/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0907\nEpoch 2150/8108\nTraining Loss: 31.9598, Training Accuracy: 0.1041\nEpoch 2151/8108\nTraining Loss: 32.0080, Training Accuracy: 0.0849\nEpoch 2152/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0892\nEpoch 2153/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0854\nEpoch 2154/8108\nTraining Loss: 32.0034, Training Accuracy: 0.1127\nEpoch 2155/8108\nTraining Loss: 31.9847, Training Accuracy: 0.0883\nEpoch 2156/8108\nTraining Loss: 31.9816, Training Accuracy: 0.0892\nEpoch 2157/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0705\nEpoch 2158/8108\nTraining Loss: 32.0108, Training Accuracy: 0.0849\nEpoch 2159/8108\nTraining Loss: 31.9749, Training Accuracy: 0.1017\nEpoch 2160/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0935\nEpoch 2161/8108\nTraining Loss: 31.9760, Training Accuracy: 0.1055\nEpoch 2162/8108\nTraining Loss: 32.0083, Training Accuracy: 0.1065\nEpoch 2163/8108\nTraining Loss: 31.9915, Training Accuracy: 0.1252\nEpoch 2164/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0691\nEpoch 2165/8108\nTraining Loss: 32.0065, Training Accuracy: 0.0619\nEpoch 2166/8108\nTraining Loss: 32.0080, Training Accuracy: 0.0940\nEpoch 2167/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0892\nEpoch 2168/8108\nTraining Loss: 32.0005, Training Accuracy: 0.0921\nEpoch 2169/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0998\nEpoch 2170/8108\nTraining Loss: 31.9892, Training Accuracy: 0.0902\nEpoch 2171/8108\nTraining Loss: 31.9777, Training Accuracy: 0.0863\nEpoch 2172/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0633\nEpoch 2173/8108\nTraining Loss: 31.9928, Training Accuracy: 0.1041\nEpoch 2174/8108\nTraining Loss: 31.9798, Training Accuracy: 0.0796\nEpoch 2175/8108\nTraining Loss: 31.9801, Training Accuracy: 0.0863\nEpoch 2176/8108\nTraining Loss: 32.0065, Training Accuracy: 0.0748\nEpoch 2177/8108\nTraining Loss: 31.9916, Training Accuracy: 0.1041\nEpoch 2178/8108\nTraining Loss: 31.9893, Training Accuracy: 0.0676\nEpoch 2179/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0921\nEpoch 2180/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0873\nEpoch 2181/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0998\nEpoch 2182/8108\nTraining Loss: 32.0056, Training Accuracy: 0.0734\nEpoch 2183/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0710\nEpoch 2184/8108\nTraining Loss: 32.0009, Training Accuracy: 0.0691\nEpoch 2185/8108\nTraining Loss: 31.9839, Training Accuracy: 0.1233\nEpoch 2186/8108\nTraining Loss: 31.9750, Training Accuracy: 0.0902\nEpoch 2187/8108\nTraining Loss: 31.9976, Training Accuracy: 0.0835\nEpoch 2188/8108\nTraining Loss: 32.0110, Training Accuracy: 0.0969\nEpoch 2189/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0983\nEpoch 2190/8108\nTraining Loss: 32.0034, Training Accuracy: 0.1012\nEpoch 2191/8108\nTraining Loss: 31.9839, Training Accuracy: 0.1084\nEpoch 2192/8108\nTraining Loss: 31.9794, Training Accuracy: 0.0892\nEpoch 2193/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0868\nEpoch 2194/8108\nTraining Loss: 32.0035, Training Accuracy: 0.1147\nEpoch 2195/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0902\nEpoch 2196/8108\nTraining Loss: 31.9856, Training Accuracy: 0.1099\nEpoch 2197/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0892\nEpoch 2198/8108\nTraining Loss: 32.0016, Training Accuracy: 0.1012\nEpoch 2199/8108\nTraining Loss: 32.0298, Training Accuracy: 0.0849\nEpoch 2200/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0796\nEpoch 2201/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0748\nEpoch 2202/8108\nTraining Loss: 31.9868, Training Accuracy: 0.0940\nEpoch 2203/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0868\nEpoch 2204/8108\nTraining Loss: 31.9843, Training Accuracy: 0.0921\nEpoch 2205/8108\nTraining Loss: 31.9776, Training Accuracy: 0.1027\nEpoch 2206/8108\nTraining Loss: 32.0163, Training Accuracy: 0.0734\nEpoch 2207/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0840\nEpoch 2208/8108\nTraining Loss: 31.9890, Training Accuracy: 0.0911\nEpoch 2209/8108\nTraining Loss: 31.9867, Training Accuracy: 0.0887\nEpoch 2210/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0878\nEpoch 2211/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0854\nEpoch 2212/8108\nTraining Loss: 31.9881, Training Accuracy: 0.1007\nEpoch 2213/8108\nTraining Loss: 31.9806, Training Accuracy: 0.0825\nEpoch 2214/8108\nTraining Loss: 31.9797, Training Accuracy: 0.0835\nEpoch 2215/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0849\nEpoch 2216/8108\nTraining Loss: 31.9897, Training Accuracy: 0.0940\nEpoch 2217/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0935\nEpoch 2218/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0724\nEpoch 2219/8108\nTraining Loss: 32.0186, Training Accuracy: 0.1027\nEpoch 2220/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0777\nEpoch 2221/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0945\nEpoch 2222/8108\nTraining Loss: 31.9824, Training Accuracy: 0.0792\nEpoch 2223/8108\nTraining Loss: 31.9934, Training Accuracy: 0.1175\nEpoch 2224/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0892\nEpoch 2225/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0921\nEpoch 2226/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0964\nEpoch 2227/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0921\nEpoch 2228/8108\nTraining Loss: 32.0009, Training Accuracy: 0.1046\nEpoch 2229/8108\nTraining Loss: 32.0074, Training Accuracy: 0.0763\nEpoch 2230/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0792\nEpoch 2231/8108\nTraining Loss: 31.9836, Training Accuracy: 0.0720\nEpoch 2232/8108\nTraining Loss: 32.0187, Training Accuracy: 0.0676\nEpoch 2233/8108\nTraining Loss: 31.9948, Training Accuracy: 0.0911\nEpoch 2234/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0820\nEpoch 2235/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0907\nEpoch 2236/8108\nTraining Loss: 32.0141, Training Accuracy: 0.1027\nEpoch 2237/8108\nTraining Loss: 32.0072, Training Accuracy: 0.0935\nEpoch 2238/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0777\nEpoch 2239/8108\nTraining Loss: 32.0047, Training Accuracy: 0.0892\nEpoch 2240/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0878\nEpoch 2241/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0849\nEpoch 2242/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0926\nEpoch 2243/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0604\nEpoch 2244/8108\nTraining Loss: 32.0097, Training Accuracy: 0.0897\nEpoch 2245/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0911\nEpoch 2246/8108\nTraining Loss: 32.0029, Training Accuracy: 0.0969\nEpoch 2247/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0911\nEpoch 2248/8108\nTraining Loss: 32.0093, Training Accuracy: 0.0897\nEpoch 2249/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0921\nEpoch 2250/8108\nTraining Loss: 31.9811, Training Accuracy: 0.1036\nEpoch 2251/8108\nTraining Loss: 31.9911, Training Accuracy: 0.1012\nEpoch 2252/8108\nTraining Loss: 31.9924, Training Accuracy: 0.1012\nEpoch 2253/8108\nTraining Loss: 31.9955, Training Accuracy: 0.0734\nEpoch 2254/8108\nTraining Loss: 31.9945, Training Accuracy: 0.1113\nEpoch 2255/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0748\nEpoch 2256/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0763\nEpoch 2257/8108\nTraining Loss: 31.9822, Training Accuracy: 0.0887\nEpoch 2258/8108\nTraining Loss: 32.0031, Training Accuracy: 0.0825\nEpoch 2259/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0825\nEpoch 2260/8108\nTraining Loss: 31.9923, Training Accuracy: 0.1065\nEpoch 2261/8108\nTraining Loss: 31.9627, Training Accuracy: 0.0763\nEpoch 2262/8108\nTraining Loss: 31.9985, Training Accuracy: 0.0840\nEpoch 2263/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0931\nEpoch 2264/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0835\nEpoch 2265/8108\nTraining Loss: 31.9819, Training Accuracy: 0.0983\nEpoch 2266/8108\nTraining Loss: 31.9929, Training Accuracy: 0.1084\nEpoch 2267/8108\nTraining Loss: 32.0009, Training Accuracy: 0.1060\nEpoch 2268/8108\nTraining Loss: 31.9788, Training Accuracy: 0.0974\nEpoch 2269/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0863\nEpoch 2270/8108\nTraining Loss: 31.9829, Training Accuracy: 0.0835\nEpoch 2271/8108\nTraining Loss: 31.9700, Training Accuracy: 0.0825\nEpoch 2272/8108\nTraining Loss: 32.0169, Training Accuracy: 0.0854\nEpoch 2273/8108\nTraining Loss: 31.9908, Training Accuracy: 0.1310\nEpoch 2274/8108\nTraining Loss: 31.9866, Training Accuracy: 0.0777\nEpoch 2275/8108\nTraining Loss: 31.9939, Training Accuracy: 0.1017\nEpoch 2276/8108\nTraining Loss: 31.9979, Training Accuracy: 0.1070\nEpoch 2277/8108\nTraining Loss: 31.9979, Training Accuracy: 0.0811\nEpoch 2278/8108\nTraining Loss: 31.9898, Training Accuracy: 0.1070\nEpoch 2279/8108\nTraining Loss: 32.0009, Training Accuracy: 0.0892\nEpoch 2280/8108\nTraining Loss: 31.9898, Training Accuracy: 0.1031\nEpoch 2281/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0878\nEpoch 2282/8108\nTraining Loss: 31.9769, Training Accuracy: 0.0777\nEpoch 2283/8108\nTraining Loss: 31.9746, Training Accuracy: 0.0935\nEpoch 2284/8108\nTraining Loss: 31.9853, Training Accuracy: 0.0983\nEpoch 2285/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0849\nEpoch 2286/8108\nTraining Loss: 31.9981, Training Accuracy: 0.1012\nEpoch 2287/8108\nTraining Loss: 31.9902, Training Accuracy: 0.0921\nEpoch 2288/8108\nTraining Loss: 31.9824, Training Accuracy: 0.0748\nEpoch 2289/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0792\nEpoch 2290/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0720\nEpoch 2291/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0950\nEpoch 2292/8108\nTraining Loss: 32.0080, Training Accuracy: 0.0777\nEpoch 2293/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0863\nEpoch 2294/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0705\nEpoch 2295/8108\nTraining Loss: 32.0062, Training Accuracy: 0.0777\nEpoch 2296/8108\nTraining Loss: 31.9922, Training Accuracy: 0.0854\nEpoch 2297/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0835\nEpoch 2298/8108\nTraining Loss: 32.0082, Training Accuracy: 0.0504\nEpoch 2299/8108\nTraining Loss: 32.0088, Training Accuracy: 0.0734\nEpoch 2300/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0777\nEpoch 2301/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0590\nEpoch 2302/8108\nTraining Loss: 31.9852, Training Accuracy: 0.0748\nEpoch 2303/8108\nTraining Loss: 32.0109, Training Accuracy: 0.0691\nEpoch 2304/8108\nTraining Loss: 31.9812, Training Accuracy: 0.1094\nEpoch 2305/8108\nTraining Loss: 31.9899, Training Accuracy: 0.0705\nEpoch 2306/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0878\nEpoch 2307/8108\nTraining Loss: 31.9928, Training Accuracy: 0.0859\nEpoch 2308/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0950\nEpoch 2309/8108\nTraining Loss: 31.9942, Training Accuracy: 0.1007\nEpoch 2310/8108\nTraining Loss: 31.9825, Training Accuracy: 0.0921\nEpoch 2311/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0763\nEpoch 2312/8108\nTraining Loss: 31.9834, Training Accuracy: 0.0897\nEpoch 2313/8108\nTraining Loss: 31.9987, Training Accuracy: 0.0854\nEpoch 2314/8108\nTraining Loss: 31.9798, Training Accuracy: 0.0748\nEpoch 2315/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0806\nEpoch 2316/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0849\nEpoch 2317/8108\nTraining Loss: 31.9978, Training Accuracy: 0.1075\nEpoch 2318/8108\nTraining Loss: 31.9824, Training Accuracy: 0.0969\nEpoch 2319/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0792\nEpoch 2320/8108\nTraining Loss: 31.9845, Training Accuracy: 0.0993\nEpoch 2321/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0777\nEpoch 2322/8108\nTraining Loss: 31.9871, Training Accuracy: 0.0940\nEpoch 2323/8108\nTraining Loss: 32.0035, Training Accuracy: 0.1041\nEpoch 2324/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0777\nEpoch 2325/8108\nTraining Loss: 31.9800, Training Accuracy: 0.1175\nEpoch 2326/8108\nTraining Loss: 31.9924, Training Accuracy: 0.0806\nEpoch 2327/8108\nTraining Loss: 31.9852, Training Accuracy: 0.1046\nEpoch 2328/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0897\nEpoch 2329/8108\nTraining Loss: 32.0058, Training Accuracy: 0.1055\nEpoch 2330/8108\nTraining Loss: 31.9996, Training Accuracy: 0.0863\nEpoch 2331/8108\nTraining Loss: 31.9853, Training Accuracy: 0.1007\nEpoch 2332/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0782\nEpoch 2333/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0753\nEpoch 2334/8108\nTraining Loss: 31.9862, Training Accuracy: 0.0705\nEpoch 2335/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0792\nEpoch 2336/8108\nTraining Loss: 31.9807, Training Accuracy: 0.0897\nEpoch 2337/8108\nTraining Loss: 31.9888, Training Accuracy: 0.1003\nEpoch 2338/8108\nTraining Loss: 32.0225, Training Accuracy: 0.0897\nEpoch 2339/8108\nTraining Loss: 31.9796, Training Accuracy: 0.0720\nEpoch 2340/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0753\nEpoch 2341/8108\nTraining Loss: 32.0052, Training Accuracy: 0.0734\nEpoch 2342/8108\nTraining Loss: 32.0034, Training Accuracy: 0.0849\nEpoch 2343/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0907\nEpoch 2344/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0849\nEpoch 2345/8108\nTraining Loss: 31.9751, Training Accuracy: 0.0696\nEpoch 2346/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0868\nEpoch 2347/8108\nTraining Loss: 32.0098, Training Accuracy: 0.0983\nEpoch 2348/8108\nTraining Loss: 31.9982, Training Accuracy: 0.0705\nEpoch 2349/8108\nTraining Loss: 31.9848, Training Accuracy: 0.0983\nEpoch 2350/8108\nTraining Loss: 31.9648, Training Accuracy: 0.0964\nEpoch 2351/8108\nTraining Loss: 31.9741, Training Accuracy: 0.0883\nEpoch 2352/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0849\nEpoch 2353/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0892\nEpoch 2354/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0820\nEpoch 2355/8108\nTraining Loss: 31.9917, Training Accuracy: 0.1003\nEpoch 2356/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0648\nEpoch 2357/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0676\nEpoch 2358/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0835\nEpoch 2359/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0926\nEpoch 2360/8108\nTraining Loss: 31.9884, Training Accuracy: 0.1003\nEpoch 2361/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0806\nEpoch 2362/8108\nTraining Loss: 31.9841, Training Accuracy: 0.0787\nEpoch 2363/8108\nTraining Loss: 32.0100, Training Accuracy: 0.0897\nEpoch 2364/8108\nTraining Loss: 31.9813, Training Accuracy: 0.0820\nEpoch 2365/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0892\nEpoch 2366/8108\nTraining Loss: 32.0023, Training Accuracy: 0.0854\nEpoch 2367/8108\nTraining Loss: 31.9843, Training Accuracy: 0.1142\nEpoch 2368/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0835\nEpoch 2369/8108\nTraining Loss: 31.9822, Training Accuracy: 0.0883\nEpoch 2370/8108\nTraining Loss: 31.9754, Training Accuracy: 0.0955\nEpoch 2371/8108\nTraining Loss: 32.0076, Training Accuracy: 0.0792\nEpoch 2372/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0768\nEpoch 2373/8108\nTraining Loss: 31.9874, Training Accuracy: 0.0974\nEpoch 2374/8108\nTraining Loss: 32.0171, Training Accuracy: 0.1007\nEpoch 2375/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0935\nEpoch 2376/8108\nTraining Loss: 31.9973, Training Accuracy: 0.1127\nEpoch 2377/8108\nTraining Loss: 32.0086, Training Accuracy: 0.0878\nEpoch 2378/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0590\nEpoch 2379/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0806\nEpoch 2380/8108\nTraining Loss: 31.9795, Training Accuracy: 0.1060\nEpoch 2381/8108\nTraining Loss: 31.9717, Training Accuracy: 0.0935\nEpoch 2382/8108\nTraining Loss: 32.0303, Training Accuracy: 0.0868\nEpoch 2383/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0863\nEpoch 2384/8108\nTraining Loss: 31.9873, Training Accuracy: 0.0921\nEpoch 2385/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0849\nEpoch 2386/8108\nTraining Loss: 31.9845, Training Accuracy: 0.0883\nEpoch 2387/8108\nTraining Loss: 31.9852, Training Accuracy: 0.0911\nEpoch 2388/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0883\nEpoch 2389/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0878\nEpoch 2390/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0739\nEpoch 2391/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0849\nEpoch 2392/8108\nTraining Loss: 31.9799, Training Accuracy: 0.0739\nEpoch 2393/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0863\nEpoch 2394/8108\nTraining Loss: 31.9807, Training Accuracy: 0.1055\nEpoch 2395/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0979\nEpoch 2396/8108\nTraining Loss: 31.9824, Training Accuracy: 0.0902\nEpoch 2397/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0921\nEpoch 2398/8108\nTraining Loss: 32.0079, Training Accuracy: 0.0748\nEpoch 2399/8108\nTraining Loss: 31.9903, Training Accuracy: 0.0878\nEpoch 2400/8108\nTraining Loss: 31.9939, Training Accuracy: 0.1065\nEpoch 2401/8108\nTraining Loss: 31.9837, Training Accuracy: 0.0849\nEpoch 2402/8108\nTraining Loss: 31.9887, Training Accuracy: 0.0983\nEpoch 2403/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0748\nEpoch 2404/8108\nTraining Loss: 31.9801, Training Accuracy: 0.0964\nEpoch 2405/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0806\nEpoch 2406/8108\nTraining Loss: 31.9866, Training Accuracy: 0.1007\nEpoch 2407/8108\nTraining Loss: 32.0030, Training Accuracy: 0.0825\nEpoch 2408/8108\nTraining Loss: 31.9807, Training Accuracy: 0.0863\nEpoch 2409/8108\nTraining Loss: 31.9995, Training Accuracy: 0.0935\nEpoch 2410/8108\nTraining Loss: 32.0158, Training Accuracy: 0.0532\nEpoch 2411/8108\nTraining Loss: 31.9805, Training Accuracy: 0.0983\nEpoch 2412/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0878\nEpoch 2413/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0926\nEpoch 2414/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0993\nEpoch 2415/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0633\nEpoch 2416/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0940\nEpoch 2417/8108\nTraining Loss: 31.9928, Training Accuracy: 0.0696\nEpoch 2418/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0892\nEpoch 2419/8108\nTraining Loss: 31.9718, Training Accuracy: 0.0825\nEpoch 2420/8108\nTraining Loss: 32.0133, Training Accuracy: 0.0777\nEpoch 2421/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0825\nEpoch 2422/8108\nTraining Loss: 31.9817, Training Accuracy: 0.0705\nEpoch 2423/8108\nTraining Loss: 31.9875, Training Accuracy: 0.0806\nEpoch 2424/8108\nTraining Loss: 32.0083, Training Accuracy: 0.0849\nEpoch 2425/8108\nTraining Loss: 31.9990, Training Accuracy: 0.0811\nEpoch 2426/8108\nTraining Loss: 31.9909, Training Accuracy: 0.1079\nEpoch 2427/8108\nTraining Loss: 32.0056, Training Accuracy: 0.0820\nEpoch 2428/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0983\nEpoch 2429/8108\nTraining Loss: 32.0038, Training Accuracy: 0.0763\nEpoch 2430/8108\nTraining Loss: 31.9851, Training Accuracy: 0.1027\nEpoch 2431/8108\nTraining Loss: 32.0055, Training Accuracy: 0.0648\nEpoch 2432/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0734\nEpoch 2433/8108\nTraining Loss: 32.0129, Training Accuracy: 0.0710\nEpoch 2434/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0763\nEpoch 2435/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0734\nEpoch 2436/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0892\nEpoch 2437/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0950\nEpoch 2438/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0878\nEpoch 2439/8108\nTraining Loss: 32.0023, Training Accuracy: 0.0777\nEpoch 2440/8108\nTraining Loss: 32.0047, Training Accuracy: 0.0763\nEpoch 2441/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0921\nEpoch 2442/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0633\nEpoch 2443/8108\nTraining Loss: 32.0086, Training Accuracy: 0.0561\nEpoch 2444/8108\nTraining Loss: 32.0004, Training Accuracy: 0.1027\nEpoch 2445/8108\nTraining Loss: 31.9776, Training Accuracy: 0.0863\nEpoch 2446/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0868\nEpoch 2447/8108\nTraining Loss: 32.0180, Training Accuracy: 0.0863\nEpoch 2448/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0763\nEpoch 2449/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0696\nEpoch 2450/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0878\nEpoch 2451/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0806\nEpoch 2452/8108\nTraining Loss: 31.9933, Training Accuracy: 0.0878\nEpoch 2453/8108\nTraining Loss: 31.9757, Training Accuracy: 0.0969\nEpoch 2454/8108\nTraining Loss: 32.0173, Training Accuracy: 0.0748\nEpoch 2455/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0955\nEpoch 2456/8108\nTraining Loss: 31.9893, Training Accuracy: 0.0844\nEpoch 2457/8108\nTraining Loss: 32.0063, Training Accuracy: 0.0940\nEpoch 2458/8108\nTraining Loss: 31.9815, Training Accuracy: 0.1103\nEpoch 2459/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0696\nEpoch 2460/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0705\nEpoch 2461/8108\nTraining Loss: 31.9835, Training Accuracy: 0.0921\nEpoch 2462/8108\nTraining Loss: 32.0130, Training Accuracy: 0.0854\nEpoch 2463/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0792\nEpoch 2464/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0964\nEpoch 2465/8108\nTraining Loss: 31.9787, Training Accuracy: 0.0979\nEpoch 2466/8108\nTraining Loss: 31.9868, Training Accuracy: 0.1103\nEpoch 2467/8108\nTraining Loss: 32.0065, Training Accuracy: 0.0840\nEpoch 2468/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0945\nEpoch 2469/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0854\nEpoch 2470/8108\nTraining Loss: 32.0063, Training Accuracy: 0.0911\nEpoch 2471/8108\nTraining Loss: 32.0120, Training Accuracy: 0.0561\nEpoch 2472/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0720\nEpoch 2473/8108\nTraining Loss: 31.9870, Training Accuracy: 0.1012\nEpoch 2474/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0907\nEpoch 2475/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0806\nEpoch 2476/8108\nTraining Loss: 32.0071, Training Accuracy: 0.0921\nEpoch 2477/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0873\nEpoch 2478/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0835\nEpoch 2479/8108\nTraining Loss: 32.0114, Training Accuracy: 0.0720\nEpoch 2480/8108\nTraining Loss: 31.9909, Training Accuracy: 0.0897\nEpoch 2481/8108\nTraining Loss: 31.9767, Training Accuracy: 0.0907\nEpoch 2482/8108\nTraining Loss: 31.9956, Training Accuracy: 0.1079\nEpoch 2483/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0691\nEpoch 2484/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0911\nEpoch 2485/8108\nTraining Loss: 31.9900, Training Accuracy: 0.0811\nEpoch 2486/8108\nTraining Loss: 31.9846, Training Accuracy: 0.0897\nEpoch 2487/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0878\nEpoch 2488/8108\nTraining Loss: 31.9894, Training Accuracy: 0.0648\nEpoch 2489/8108\nTraining Loss: 31.9750, Training Accuracy: 0.0840\nEpoch 2490/8108\nTraining Loss: 32.0078, Training Accuracy: 0.0720\nEpoch 2491/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0782\nEpoch 2492/8108\nTraining Loss: 31.9826, Training Accuracy: 0.1075\nEpoch 2493/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0959\nEpoch 2494/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0849\nEpoch 2495/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0998\nEpoch 2496/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0983\nEpoch 2497/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0835\nEpoch 2498/8108\nTraining Loss: 31.9953, Training Accuracy: 0.0806\nEpoch 2499/8108\nTraining Loss: 32.0165, Training Accuracy: 0.0897\nEpoch 2500/8108\nTraining Loss: 32.0127, Training Accuracy: 0.0835\nEpoch 2501/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0892\nEpoch 2502/8108\nTraining Loss: 32.0040, Training Accuracy: 0.0705\nEpoch 2503/8108\nTraining Loss: 31.9955, Training Accuracy: 0.0720\nEpoch 2504/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0532\nEpoch 2505/8108\nTraining Loss: 32.0151, Training Accuracy: 0.0830\nEpoch 2506/8108\nTraining Loss: 31.9952, Training Accuracy: 0.0796\nEpoch 2507/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0892\nEpoch 2508/8108\nTraining Loss: 32.0005, Training Accuracy: 0.1123\nEpoch 2509/8108\nTraining Loss: 31.9847, Training Accuracy: 0.0604\nEpoch 2510/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0619\nEpoch 2511/8108\nTraining Loss: 32.0142, Training Accuracy: 0.0633\nEpoch 2512/8108\nTraining Loss: 32.0028, Training Accuracy: 0.0993\nEpoch 2513/8108\nTraining Loss: 31.9916, Training Accuracy: 0.1175\nEpoch 2514/8108\nTraining Loss: 31.9998, Training Accuracy: 0.0676\nEpoch 2515/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0782\nEpoch 2516/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0892\nEpoch 2517/8108\nTraining Loss: 32.0029, Training Accuracy: 0.0863\nEpoch 2518/8108\nTraining Loss: 32.0153, Training Accuracy: 0.0907\nEpoch 2519/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0983\nEpoch 2520/8108\nTraining Loss: 31.9914, Training Accuracy: 0.1084\nEpoch 2521/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0863\nEpoch 2522/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0964\nEpoch 2523/8108\nTraining Loss: 31.9797, Training Accuracy: 0.0892\nEpoch 2524/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0950\nEpoch 2525/8108\nTraining Loss: 31.9953, Training Accuracy: 0.1007\nEpoch 2526/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0777\nEpoch 2527/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0868\nEpoch 2528/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0820\nEpoch 2529/8108\nTraining Loss: 31.9856, Training Accuracy: 0.0691\nEpoch 2530/8108\nTraining Loss: 31.9757, Training Accuracy: 0.0878\nEpoch 2531/8108\nTraining Loss: 31.9841, Training Accuracy: 0.0849\nEpoch 2532/8108\nTraining Loss: 32.0108, Training Accuracy: 0.1079\nEpoch 2533/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0748\nEpoch 2534/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0878\nEpoch 2535/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0763\nEpoch 2536/8108\nTraining Loss: 32.0061, Training Accuracy: 0.0969\nEpoch 2537/8108\nTraining Loss: 31.9795, Training Accuracy: 0.1012\nEpoch 2538/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0849\nEpoch 2539/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0868\nEpoch 2540/8108\nTraining Loss: 31.9904, Training Accuracy: 0.1166\nEpoch 2541/8108\nTraining Loss: 31.9916, Training Accuracy: 0.0921\nEpoch 2542/8108\nTraining Loss: 31.9753, Training Accuracy: 0.0907\nEpoch 2543/8108\nTraining Loss: 31.9751, Training Accuracy: 0.1036\nEpoch 2544/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0907\nEpoch 2545/8108\nTraining Loss: 32.0125, Training Accuracy: 0.0964\nEpoch 2546/8108\nTraining Loss: 31.9896, Training Accuracy: 0.0835\nEpoch 2547/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0691\nEpoch 2548/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0883\nEpoch 2549/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0840\nEpoch 2550/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0748\nEpoch 2551/8108\nTraining Loss: 32.0051, Training Accuracy: 0.0835\nEpoch 2552/8108\nTraining Loss: 31.9985, Training Accuracy: 0.0955\nEpoch 2553/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0859\nEpoch 2554/8108\nTraining Loss: 31.9786, Training Accuracy: 0.1065\nEpoch 2555/8108\nTraining Loss: 32.0102, Training Accuracy: 0.0676\nEpoch 2556/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0806\nEpoch 2557/8108\nTraining Loss: 31.9797, Training Accuracy: 0.0935\nEpoch 2558/8108\nTraining Loss: 31.9745, Training Accuracy: 0.0859\nEpoch 2559/8108\nTraining Loss: 32.0055, Training Accuracy: 0.0897\nEpoch 2560/8108\nTraining Loss: 32.0131, Training Accuracy: 0.1017\nEpoch 2561/8108\nTraining Loss: 31.9892, Training Accuracy: 0.1113\nEpoch 2562/8108\nTraining Loss: 32.0071, Training Accuracy: 0.0705\nEpoch 2563/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0964\nEpoch 2564/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0911\nEpoch 2565/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0935\nEpoch 2566/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0907\nEpoch 2567/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0964\nEpoch 2568/8108\nTraining Loss: 32.0013, Training Accuracy: 0.0619\nEpoch 2569/8108\nTraining Loss: 31.9934, Training Accuracy: 0.1007\nEpoch 2570/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0911\nEpoch 2571/8108\nTraining Loss: 32.0012, Training Accuracy: 0.0921\nEpoch 2572/8108\nTraining Loss: 31.9850, Training Accuracy: 0.0993\nEpoch 2573/8108\nTraining Loss: 31.9958, Training Accuracy: 0.1079\nEpoch 2574/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0782\nEpoch 2575/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0820\nEpoch 2576/8108\nTraining Loss: 31.9775, Training Accuracy: 0.1118\nEpoch 2577/8108\nTraining Loss: 32.0031, Training Accuracy: 0.0676\nEpoch 2578/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0748\nEpoch 2579/8108\nTraining Loss: 31.9839, Training Accuracy: 0.0979\nEpoch 2580/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0830\nEpoch 2581/8108\nTraining Loss: 31.9863, Training Accuracy: 0.0844\nEpoch 2582/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0863\nEpoch 2583/8108\nTraining Loss: 31.9730, Training Accuracy: 0.1147\nEpoch 2584/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0768\nEpoch 2585/8108\nTraining Loss: 31.9853, Training Accuracy: 0.0940\nEpoch 2586/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0792\nEpoch 2587/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0691\nEpoch 2588/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0863\nEpoch 2589/8108\nTraining Loss: 32.0062, Training Accuracy: 0.0734\nEpoch 2590/8108\nTraining Loss: 31.9972, Training Accuracy: 0.1123\nEpoch 2591/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0734\nEpoch 2592/8108\nTraining Loss: 31.9927, Training Accuracy: 0.0806\nEpoch 2593/8108\nTraining Loss: 31.9828, Training Accuracy: 0.0883\nEpoch 2594/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0763\nEpoch 2595/8108\nTraining Loss: 31.9927, Training Accuracy: 0.0748\nEpoch 2596/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0806\nEpoch 2597/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0796\nEpoch 2598/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0830\nEpoch 2599/8108\nTraining Loss: 32.0099, Training Accuracy: 0.1099\nEpoch 2600/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0926\nEpoch 2601/8108\nTraining Loss: 31.9879, Training Accuracy: 0.1003\nEpoch 2602/8108\nTraining Loss: 31.9793, Training Accuracy: 0.1094\nEpoch 2603/8108\nTraining Loss: 31.9989, Training Accuracy: 0.1151\nEpoch 2604/8108\nTraining Loss: 31.9940, Training Accuracy: 0.0849\nEpoch 2605/8108\nTraining Loss: 32.0164, Training Accuracy: 0.0863\nEpoch 2606/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0792\nEpoch 2607/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0806\nEpoch 2608/8108\nTraining Loss: 32.0029, Training Accuracy: 0.0763\nEpoch 2609/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0777\nEpoch 2610/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0863\nEpoch 2611/8108\nTraining Loss: 32.0211, Training Accuracy: 0.0796\nEpoch 2612/8108\nTraining Loss: 32.0019, Training Accuracy: 0.0897\nEpoch 2613/8108\nTraining Loss: 31.9902, Training Accuracy: 0.0811\nEpoch 2614/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0840\nEpoch 2615/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0945\nEpoch 2616/8108\nTraining Loss: 32.0065, Training Accuracy: 0.0940\nEpoch 2617/8108\nTraining Loss: 31.9901, Training Accuracy: 0.0849\nEpoch 2618/8108\nTraining Loss: 31.9861, Training Accuracy: 0.1099\nEpoch 2619/8108\nTraining Loss: 32.0135, Training Accuracy: 0.0840\nEpoch 2620/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0777\nEpoch 2621/8108\nTraining Loss: 32.0003, Training Accuracy: 0.1055\nEpoch 2622/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0792\nEpoch 2623/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0969\nEpoch 2624/8108\nTraining Loss: 32.0038, Training Accuracy: 0.0983\nEpoch 2625/8108\nTraining Loss: 31.9904, Training Accuracy: 0.0897\nEpoch 2626/8108\nTraining Loss: 32.0106, Training Accuracy: 0.0835\nEpoch 2627/8108\nTraining Loss: 31.9768, Training Accuracy: 0.0916\nEpoch 2628/8108\nTraining Loss: 31.9868, Training Accuracy: 0.0835\nEpoch 2629/8108\nTraining Loss: 32.0250, Training Accuracy: 0.0868\nEpoch 2630/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0863\nEpoch 2631/8108\nTraining Loss: 31.9815, Training Accuracy: 0.1055\nEpoch 2632/8108\nTraining Loss: 32.0131, Training Accuracy: 0.0926\nEpoch 2633/8108\nTraining Loss: 32.0069, Training Accuracy: 0.1036\nEpoch 2634/8108\nTraining Loss: 31.9808, Training Accuracy: 0.0998\nEpoch 2635/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0935\nEpoch 2636/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0792\nEpoch 2637/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0863\nEpoch 2638/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0648\nEpoch 2639/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0883\nEpoch 2640/8108\nTraining Loss: 31.9984, Training Accuracy: 0.0806\nEpoch 2641/8108\nTraining Loss: 31.9851, Training Accuracy: 0.1022\nEpoch 2642/8108\nTraining Loss: 32.0081, Training Accuracy: 0.0739\nEpoch 2643/8108\nTraining Loss: 32.0097, Training Accuracy: 0.0840\nEpoch 2644/8108\nTraining Loss: 31.9991, Training Accuracy: 0.0863\nEpoch 2645/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0792\nEpoch 2646/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0955\nEpoch 2647/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0777\nEpoch 2648/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0825\nEpoch 2649/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0964\nEpoch 2650/8108\nTraining Loss: 31.9804, Training Accuracy: 0.0979\nEpoch 2651/8108\nTraining Loss: 32.0045, Training Accuracy: 0.0959\nEpoch 2652/8108\nTraining Loss: 31.9761, Training Accuracy: 0.0979\nEpoch 2653/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0979\nEpoch 2654/8108\nTraining Loss: 32.0031, Training Accuracy: 0.0820\nEpoch 2655/8108\nTraining Loss: 31.9996, Training Accuracy: 0.0691\nEpoch 2656/8108\nTraining Loss: 31.9849, Training Accuracy: 0.0964\nEpoch 2657/8108\nTraining Loss: 31.9891, Training Accuracy: 0.0705\nEpoch 2658/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0705\nEpoch 2659/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0849\nEpoch 2660/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0806\nEpoch 2661/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0950\nEpoch 2662/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0993\nEpoch 2663/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0777\nEpoch 2664/8108\nTraining Loss: 32.0131, Training Accuracy: 0.0768\nEpoch 2665/8108\nTraining Loss: 31.9870, Training Accuracy: 0.0983\nEpoch 2666/8108\nTraining Loss: 31.9765, Training Accuracy: 0.0748\nEpoch 2667/8108\nTraining Loss: 31.9762, Training Accuracy: 0.0705\nEpoch 2668/8108\nTraining Loss: 32.0124, Training Accuracy: 0.0763\nEpoch 2669/8108\nTraining Loss: 31.9968, Training Accuracy: 0.1113\nEpoch 2670/8108\nTraining Loss: 31.9963, Training Accuracy: 0.1204\nEpoch 2671/8108\nTraining Loss: 31.9980, Training Accuracy: 0.0835\nEpoch 2672/8108\nTraining Loss: 31.9817, Training Accuracy: 0.0868\nEpoch 2673/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0921\nEpoch 2674/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0734\nEpoch 2675/8108\nTraining Loss: 32.0074, Training Accuracy: 0.0792\nEpoch 2676/8108\nTraining Loss: 31.9815, Training Accuracy: 0.1007\nEpoch 2677/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0748\nEpoch 2678/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0854\nEpoch 2679/8108\nTraining Loss: 32.0194, Training Accuracy: 0.0734\nEpoch 2680/8108\nTraining Loss: 31.9927, Training Accuracy: 0.1099\nEpoch 2681/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0840\nEpoch 2682/8108\nTraining Loss: 32.0102, Training Accuracy: 0.0724\nEpoch 2683/8108\nTraining Loss: 31.9976, Training Accuracy: 0.1007\nEpoch 2684/8108\nTraining Loss: 31.9871, Training Accuracy: 0.0806\nEpoch 2685/8108\nTraining Loss: 31.9782, Training Accuracy: 0.0969\nEpoch 2686/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0840\nEpoch 2687/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0633\nEpoch 2688/8108\nTraining Loss: 31.9723, Training Accuracy: 0.0734\nEpoch 2689/8108\nTraining Loss: 31.9688, Training Accuracy: 0.0868\nEpoch 2690/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0926\nEpoch 2691/8108\nTraining Loss: 32.0055, Training Accuracy: 0.0926\nEpoch 2692/8108\nTraining Loss: 31.9899, Training Accuracy: 0.0691\nEpoch 2693/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0835\nEpoch 2694/8108\nTraining Loss: 31.9877, Training Accuracy: 0.0911\nEpoch 2695/8108\nTraining Loss: 31.9633, Training Accuracy: 0.0979\nEpoch 2696/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0892\nEpoch 2697/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0691\nEpoch 2698/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0863\nEpoch 2699/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0849\nEpoch 2700/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0863\nEpoch 2701/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0897\nEpoch 2702/8108\nTraining Loss: 32.0002, Training Accuracy: 0.1242\nEpoch 2703/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0633\nEpoch 2704/8108\nTraining Loss: 32.0058, Training Accuracy: 0.0705\nEpoch 2705/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0748\nEpoch 2706/8108\nTraining Loss: 31.9718, Training Accuracy: 0.1075\nEpoch 2707/8108\nTraining Loss: 32.0095, Training Accuracy: 0.0854\nEpoch 2708/8108\nTraining Loss: 31.9999, Training Accuracy: 0.1027\nEpoch 2709/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0782\nEpoch 2710/8108\nTraining Loss: 31.9885, Training Accuracy: 0.1166\nEpoch 2711/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0777\nEpoch 2712/8108\nTraining Loss: 31.9815, Training Accuracy: 0.0806\nEpoch 2713/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0835\nEpoch 2714/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0926\nEpoch 2715/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0763\nEpoch 2716/8108\nTraining Loss: 32.0079, Training Accuracy: 0.0993\nEpoch 2717/8108\nTraining Loss: 31.9906, Training Accuracy: 0.0907\nEpoch 2718/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0955\nEpoch 2719/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0993\nEpoch 2720/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0734\nEpoch 2721/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0777\nEpoch 2722/8108\nTraining Loss: 32.0076, Training Accuracy: 0.0892\nEpoch 2723/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0806\nEpoch 2724/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0911\nEpoch 2725/8108\nTraining Loss: 31.9793, Training Accuracy: 0.0955\nEpoch 2726/8108\nTraining Loss: 31.9806, Training Accuracy: 0.0777\nEpoch 2727/8108\nTraining Loss: 31.9881, Training Accuracy: 0.0935\nEpoch 2728/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0806\nEpoch 2729/8108\nTraining Loss: 31.9876, Training Accuracy: 0.0907\nEpoch 2730/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0835\nEpoch 2731/8108\nTraining Loss: 31.9789, Training Accuracy: 0.0993\nEpoch 2732/8108\nTraining Loss: 31.9869, Training Accuracy: 0.1233\nEpoch 2733/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0950\nEpoch 2734/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0734\nEpoch 2735/8108\nTraining Loss: 31.9829, Training Accuracy: 0.0892\nEpoch 2736/8108\nTraining Loss: 31.9770, Training Accuracy: 0.0950\nEpoch 2737/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0863\nEpoch 2738/8108\nTraining Loss: 32.0004, Training Accuracy: 0.0849\nEpoch 2739/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0705\nEpoch 2740/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0816\nEpoch 2741/8108\nTraining Loss: 31.9826, Training Accuracy: 0.0940\nEpoch 2742/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0739\nEpoch 2743/8108\nTraining Loss: 31.9922, Training Accuracy: 0.0806\nEpoch 2744/8108\nTraining Loss: 31.9801, Training Accuracy: 0.0863\nEpoch 2745/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0792\nEpoch 2746/8108\nTraining Loss: 32.0128, Training Accuracy: 0.0849\nEpoch 2747/8108\nTraining Loss: 31.9865, Training Accuracy: 0.0806\nEpoch 2748/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0777\nEpoch 2749/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0863\nEpoch 2750/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0955\nEpoch 2751/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0854\nEpoch 2752/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0835\nEpoch 2753/8108\nTraining Loss: 31.9843, Training Accuracy: 0.0983\nEpoch 2754/8108\nTraining Loss: 32.0049, Training Accuracy: 0.0662\nEpoch 2755/8108\nTraining Loss: 32.0031, Training Accuracy: 0.0983\nEpoch 2756/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0840\nEpoch 2757/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0820\nEpoch 2758/8108\nTraining Loss: 31.9873, Training Accuracy: 0.0854\nEpoch 2759/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0696\nEpoch 2760/8108\nTraining Loss: 31.9913, Training Accuracy: 0.1161\nEpoch 2761/8108\nTraining Loss: 32.0019, Training Accuracy: 0.0840\nEpoch 2762/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0907\nEpoch 2763/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0945\nEpoch 2764/8108\nTraining Loss: 31.9903, Training Accuracy: 0.1007\nEpoch 2765/8108\nTraining Loss: 31.9878, Training Accuracy: 0.0840\nEpoch 2766/8108\nTraining Loss: 31.9938, Training Accuracy: 0.0734\nEpoch 2767/8108\nTraining Loss: 32.0129, Training Accuracy: 0.0935\nEpoch 2768/8108\nTraining Loss: 31.9977, Training Accuracy: 0.0720\nEpoch 2769/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0801\nEpoch 2770/8108\nTraining Loss: 31.9882, Training Accuracy: 0.0777\nEpoch 2771/8108\nTraining Loss: 31.9796, Training Accuracy: 0.1036\nEpoch 2772/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0849\nEpoch 2773/8108\nTraining Loss: 32.0041, Training Accuracy: 0.0998\nEpoch 2774/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0964\nEpoch 2775/8108\nTraining Loss: 31.9951, Training Accuracy: 0.0792\nEpoch 2776/8108\nTraining Loss: 31.9821, Training Accuracy: 0.0840\nEpoch 2777/8108\nTraining Loss: 31.9836, Training Accuracy: 0.1132\nEpoch 2778/8108\nTraining Loss: 31.9866, Training Accuracy: 0.1247\nEpoch 2779/8108\nTraining Loss: 32.0009, Training Accuracy: 0.0931\nEpoch 2780/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0777\nEpoch 2781/8108\nTraining Loss: 31.9859, Training Accuracy: 0.0844\nEpoch 2782/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0777\nEpoch 2783/8108\nTraining Loss: 31.9822, Training Accuracy: 0.0921\nEpoch 2784/8108\nTraining Loss: 32.0184, Training Accuracy: 0.0720\nEpoch 2785/8108\nTraining Loss: 32.0023, Training Accuracy: 0.0907\nEpoch 2786/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0676\nEpoch 2787/8108\nTraining Loss: 32.0032, Training Accuracy: 0.0950\nEpoch 2788/8108\nTraining Loss: 31.9968, Training Accuracy: 0.1046\nEpoch 2789/8108\nTraining Loss: 31.9972, Training Accuracy: 0.0720\nEpoch 2790/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0734\nEpoch 2791/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0763\nEpoch 2792/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0787\nEpoch 2793/8108\nTraining Loss: 31.9872, Training Accuracy: 0.0710\nEpoch 2794/8108\nTraining Loss: 32.0070, Training Accuracy: 0.0940\nEpoch 2795/8108\nTraining Loss: 32.0116, Training Accuracy: 0.0820\nEpoch 2796/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0868\nEpoch 2797/8108\nTraining Loss: 32.0024, Training Accuracy: 0.0955\nEpoch 2798/8108\nTraining Loss: 32.0003, Training Accuracy: 0.0955\nEpoch 2799/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0691\nEpoch 2800/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0897\nEpoch 2801/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0763\nEpoch 2802/8108\nTraining Loss: 31.9931, Training Accuracy: 0.0926\nEpoch 2803/8108\nTraining Loss: 31.9732, Training Accuracy: 0.0979\nEpoch 2804/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0777\nEpoch 2805/8108\nTraining Loss: 32.0064, Training Accuracy: 0.0705\nEpoch 2806/8108\nTraining Loss: 31.9812, Training Accuracy: 0.0849\nEpoch 2807/8108\nTraining Loss: 31.9956, Training Accuracy: 0.0720\nEpoch 2808/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0950\nEpoch 2809/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0897\nEpoch 2810/8108\nTraining Loss: 31.9784, Training Accuracy: 0.0734\nEpoch 2811/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0940\nEpoch 2812/8108\nTraining Loss: 31.9844, Training Accuracy: 0.0983\nEpoch 2813/8108\nTraining Loss: 32.0226, Training Accuracy: 0.0705\nEpoch 2814/8108\nTraining Loss: 31.9908, Training Accuracy: 0.0964\nEpoch 2815/8108\nTraining Loss: 32.0003, Training Accuracy: 0.0763\nEpoch 2816/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0921\nEpoch 2817/8108\nTraining Loss: 32.0038, Training Accuracy: 0.0926\nEpoch 2818/8108\nTraining Loss: 31.9823, Training Accuracy: 0.0883\nEpoch 2819/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0931\nEpoch 2820/8108\nTraining Loss: 31.9949, Training Accuracy: 0.0897\nEpoch 2821/8108\nTraining Loss: 31.9875, Training Accuracy: 0.1084\nEpoch 2822/8108\nTraining Loss: 32.0033, Training Accuracy: 0.0887\nEpoch 2823/8108\nTraining Loss: 31.9830, Training Accuracy: 0.1161\nEpoch 2824/8108\nTraining Loss: 31.9723, Training Accuracy: 0.0883\nEpoch 2825/8108\nTraining Loss: 32.0087, Training Accuracy: 0.0897\nEpoch 2826/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0825\nEpoch 2827/8108\nTraining Loss: 31.9885, Training Accuracy: 0.0676\nEpoch 2828/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0887\nEpoch 2829/8108\nTraining Loss: 31.9851, Training Accuracy: 0.0840\nEpoch 2830/8108\nTraining Loss: 31.9976, Training Accuracy: 0.0792\nEpoch 2831/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0878\nEpoch 2832/8108\nTraining Loss: 31.9864, Training Accuracy: 0.0931\nEpoch 2833/8108\nTraining Loss: 31.9968, Training Accuracy: 0.0782\nEpoch 2834/8108\nTraining Loss: 31.9869, Training Accuracy: 0.0897\nEpoch 2835/8108\nTraining Loss: 32.0007, Training Accuracy: 0.0590\nEpoch 2836/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0868\nEpoch 2837/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0979\nEpoch 2838/8108\nTraining Loss: 31.9880, Training Accuracy: 0.0820\nEpoch 2839/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0907\nEpoch 2840/8108\nTraining Loss: 32.0099, Training Accuracy: 0.0940\nEpoch 2841/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0806\nEpoch 2842/8108\nTraining Loss: 31.9845, Training Accuracy: 0.0974\nEpoch 2843/8108\nTraining Loss: 32.0063, Training Accuracy: 0.0792\nEpoch 2844/8108\nTraining Loss: 31.9824, Training Accuracy: 0.1031\nEpoch 2845/8108\nTraining Loss: 32.0049, Training Accuracy: 0.1003\nEpoch 2846/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0849\nEpoch 2847/8108\nTraining Loss: 31.9879, Training Accuracy: 0.0921\nEpoch 2848/8108\nTraining Loss: 31.9828, Training Accuracy: 0.0926\nEpoch 2849/8108\nTraining Loss: 31.9694, Training Accuracy: 0.0993\nEpoch 2850/8108\nTraining Loss: 31.9757, Training Accuracy: 0.0840\nEpoch 2851/8108\nTraining Loss: 32.0086, Training Accuracy: 0.0691\nEpoch 2852/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0763\nEpoch 2853/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0878\nEpoch 2854/8108\nTraining Loss: 32.0101, Training Accuracy: 0.1132\nEpoch 2855/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0691\nEpoch 2856/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0676\nEpoch 2857/8108\nTraining Loss: 31.9937, Training Accuracy: 0.0878\nEpoch 2858/8108\nTraining Loss: 31.9988, Training Accuracy: 0.0576\nEpoch 2859/8108\nTraining Loss: 31.9842, Training Accuracy: 0.0897\nEpoch 2860/8108\nTraining Loss: 32.0052, Training Accuracy: 0.1079\nEpoch 2861/8108\nTraining Loss: 31.9803, Training Accuracy: 0.0868\nEpoch 2862/8108\nTraining Loss: 32.0046, Training Accuracy: 0.0911\nEpoch 2863/8108\nTraining Loss: 32.0113, Training Accuracy: 0.0758\nEpoch 2864/8108\nTraining Loss: 31.9928, Training Accuracy: 0.0983\nEpoch 2865/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0907\nEpoch 2866/8108\nTraining Loss: 32.0076, Training Accuracy: 0.0763\nEpoch 2867/8108\nTraining Loss: 31.9947, Training Accuracy: 0.0926\nEpoch 2868/8108\nTraining Loss: 32.0055, Training Accuracy: 0.0734\nEpoch 2869/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0955\nEpoch 2870/8108\nTraining Loss: 32.0025, Training Accuracy: 0.0983\nEpoch 2871/8108\nTraining Loss: 31.9972, Training Accuracy: 0.1003\nEpoch 2872/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0950\nEpoch 2873/8108\nTraining Loss: 31.9837, Training Accuracy: 0.0983\nEpoch 2874/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0720\nEpoch 2875/8108\nTraining Loss: 31.9919, Training Accuracy: 0.0931\nEpoch 2876/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0840\nEpoch 2877/8108\nTraining Loss: 31.9883, Training Accuracy: 0.0931\nEpoch 2878/8108\nTraining Loss: 31.9918, Training Accuracy: 0.0820\nEpoch 2879/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0907\nEpoch 2880/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0892\nEpoch 2881/8108\nTraining Loss: 31.9962, Training Accuracy: 0.0921\nEpoch 2882/8108\nTraining Loss: 31.9645, Training Accuracy: 0.0863\nEpoch 2883/8108\nTraining Loss: 31.9891, Training Accuracy: 0.0748\nEpoch 2884/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0734\nEpoch 2885/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0935\nEpoch 2886/8108\nTraining Loss: 32.0129, Training Accuracy: 0.0724\nEpoch 2887/8108\nTraining Loss: 31.9972, Training Accuracy: 0.1031\nEpoch 2888/8108\nTraining Loss: 31.9910, Training Accuracy: 0.0883\nEpoch 2889/8108\nTraining Loss: 31.9742, Training Accuracy: 0.1094\nEpoch 2890/8108\nTraining Loss: 31.9949, Training Accuracy: 0.0863\nEpoch 2891/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0974\nEpoch 2892/8108\nTraining Loss: 31.9798, Training Accuracy: 0.0983\nEpoch 2893/8108\nTraining Loss: 31.9949, Training Accuracy: 0.0820\nEpoch 2894/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0796\nEpoch 2895/8108\nTraining Loss: 32.0024, Training Accuracy: 0.1070\nEpoch 2896/8108\nTraining Loss: 32.0034, Training Accuracy: 0.1007\nEpoch 2897/8108\nTraining Loss: 31.9833, Training Accuracy: 0.0907\nEpoch 2898/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0883\nEpoch 2899/8108\nTraining Loss: 31.9825, Training Accuracy: 0.0806\nEpoch 2900/8108\nTraining Loss: 31.9961, Training Accuracy: 0.0667\nEpoch 2901/8108\nTraining Loss: 31.9964, Training Accuracy: 0.0753\nEpoch 2902/8108\nTraining Loss: 31.9964, Training Accuracy: 0.1055\nEpoch 2903/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0955\nEpoch 2904/8108\nTraining Loss: 31.9897, Training Accuracy: 0.0854\nEpoch 2905/8108\nTraining Loss: 32.0048, Training Accuracy: 0.0782\nEpoch 2906/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0892\nEpoch 2907/8108\nTraining Loss: 31.9779, Training Accuracy: 0.1027\nEpoch 2908/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0748\nEpoch 2909/8108\nTraining Loss: 31.9920, Training Accuracy: 0.0883\nEpoch 2910/8108\nTraining Loss: 31.9981, Training Accuracy: 0.0849\nEpoch 2911/8108\nTraining Loss: 32.0039, Training Accuracy: 0.0902\nEpoch 2912/8108\nTraining Loss: 31.9770, Training Accuracy: 0.0748\nEpoch 2913/8108\nTraining Loss: 32.0089, Training Accuracy: 0.0863\nEpoch 2914/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0988\nEpoch 2915/8108\nTraining Loss: 31.9819, Training Accuracy: 0.1031\nEpoch 2916/8108\nTraining Loss: 31.9922, Training Accuracy: 0.0777\nEpoch 2917/8108\nTraining Loss: 32.0235, Training Accuracy: 0.0720\nEpoch 2918/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0720\nEpoch 2919/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0897\nEpoch 2920/8108\nTraining Loss: 31.9955, Training Accuracy: 0.0806\nEpoch 2921/8108\nTraining Loss: 31.9873, Training Accuracy: 0.1204\nEpoch 2922/8108\nTraining Loss: 31.9837, Training Accuracy: 0.0619\nEpoch 2923/8108\nTraining Loss: 31.9888, Training Accuracy: 0.0739\nEpoch 2924/8108\nTraining Loss: 32.0125, Training Accuracy: 0.0648\nEpoch 2925/8108\nTraining Loss: 31.9923, Training Accuracy: 0.1099\nEpoch 2926/8108\nTraining Loss: 31.9920, Training Accuracy: 0.1027\nEpoch 2927/8108\nTraining Loss: 31.9893, Training Accuracy: 0.1070\nEpoch 2928/8108\nTraining Loss: 31.9867, Training Accuracy: 0.0955\nEpoch 2929/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0883\nEpoch 2930/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0907\nEpoch 2931/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0840\nEpoch 2932/8108\nTraining Loss: 31.9959, Training Accuracy: 0.0652\nEpoch 2933/8108\nTraining Loss: 31.9982, Training Accuracy: 0.1012\nEpoch 2934/8108\nTraining Loss: 32.0037, Training Accuracy: 0.0983\nEpoch 2935/8108\nTraining Loss: 31.9955, Training Accuracy: 0.1041\nEpoch 2936/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0835\nEpoch 2937/8108\nTraining Loss: 32.0104, Training Accuracy: 0.1108\nEpoch 2938/8108\nTraining Loss: 32.0077, Training Accuracy: 0.0777\nEpoch 2939/8108\nTraining Loss: 32.0058, Training Accuracy: 0.0648\nEpoch 2940/8108\nTraining Loss: 32.0073, Training Accuracy: 0.0720\nEpoch 2941/8108\nTraining Loss: 31.9819, Training Accuracy: 0.0950\nEpoch 2942/8108\nTraining Loss: 31.9941, Training Accuracy: 0.0926\nEpoch 2943/8108\nTraining Loss: 31.9841, Training Accuracy: 0.0849\nEpoch 2944/8108\nTraining Loss: 31.9803, Training Accuracy: 0.0849\nEpoch 2945/8108\nTraining Loss: 31.9760, Training Accuracy: 0.0840\nEpoch 2946/8108\nTraining Loss: 32.0090, Training Accuracy: 0.0883\nEpoch 2947/8108\nTraining Loss: 31.9907, Training Accuracy: 0.0676\nEpoch 2948/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0897\nEpoch 2949/8108\nTraining Loss: 31.9737, Training Accuracy: 0.1103\nEpoch 2950/8108\nTraining Loss: 31.9772, Training Accuracy: 0.0734\nEpoch 2951/8108\nTraining Loss: 32.0100, Training Accuracy: 0.0878\nEpoch 2952/8108\nTraining Loss: 31.9838, Training Accuracy: 0.0676\nEpoch 2953/8108\nTraining Loss: 31.9932, Training Accuracy: 0.0979\nEpoch 2954/8108\nTraining Loss: 31.9886, Training Accuracy: 0.0892\nEpoch 2955/8108\nTraining Loss: 32.0018, Training Accuracy: 0.0878\nEpoch 2956/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0576\nEpoch 2957/8108\nTraining Loss: 31.9948, Training Accuracy: 0.0892\nEpoch 2958/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0854\nEpoch 2959/8108\nTraining Loss: 32.0021, Training Accuracy: 0.0753\nEpoch 2960/8108\nTraining Loss: 31.9796, Training Accuracy: 0.0950\nEpoch 2961/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0796\nEpoch 2962/8108\nTraining Loss: 31.9976, Training Accuracy: 0.0792\nEpoch 2963/8108\nTraining Loss: 31.9771, Training Accuracy: 0.1022\nEpoch 2964/8108\nTraining Loss: 32.0071, Training Accuracy: 0.0849\nEpoch 2965/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0763\nEpoch 2966/8108\nTraining Loss: 31.9917, Training Accuracy: 0.0945\nEpoch 2967/8108\nTraining Loss: 31.9945, Training Accuracy: 0.0998\nEpoch 2968/8108\nTraining Loss: 31.9881, Training Accuracy: 0.1099\nEpoch 2969/8108\nTraining Loss: 31.9771, Training Accuracy: 0.0753\nEpoch 2970/8108\nTraining Loss: 31.9944, Training Accuracy: 0.1012\nEpoch 2971/8108\nTraining Loss: 32.0059, Training Accuracy: 0.0955\nEpoch 2972/8108\nTraining Loss: 32.0139, Training Accuracy: 0.0878\nEpoch 2973/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0748\nEpoch 2974/8108\nTraining Loss: 31.9905, Training Accuracy: 0.0710\nEpoch 2975/8108\nTraining Loss: 31.9923, Training Accuracy: 0.0969\nEpoch 2976/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0921\nEpoch 2977/8108\nTraining Loss: 31.9911, Training Accuracy: 0.0998\nEpoch 2978/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0849\nEpoch 2979/8108\nTraining Loss: 32.0011, Training Accuracy: 0.0820\nEpoch 2980/8108\nTraining Loss: 31.9942, Training Accuracy: 0.0825\nEpoch 2981/8108\nTraining Loss: 31.9966, Training Accuracy: 0.1007\nEpoch 2982/8108\nTraining Loss: 32.0053, Training Accuracy: 0.0777\nEpoch 2983/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0792\nEpoch 2984/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0782\nEpoch 2985/8108\nTraining Loss: 32.0078, Training Accuracy: 0.0892\nEpoch 2986/8108\nTraining Loss: 31.9803, Training Accuracy: 0.0792\nEpoch 2987/8108\nTraining Loss: 32.0118, Training Accuracy: 0.0840\nEpoch 2988/8108\nTraining Loss: 31.9858, Training Accuracy: 0.0878\nEpoch 2989/8108\nTraining Loss: 31.9982, Training Accuracy: 0.0931\nEpoch 2990/8108\nTraining Loss: 32.0014, Training Accuracy: 0.0863\nEpoch 2991/8108\nTraining Loss: 31.9892, Training Accuracy: 0.0921\nEpoch 2992/8108\nTraining Loss: 31.9915, Training Accuracy: 0.0907\nEpoch 2993/8108\nTraining Loss: 31.9865, Training Accuracy: 0.0840\nEpoch 2994/8108\nTraining Loss: 32.0080, Training Accuracy: 0.0777\nEpoch 2995/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0705\nEpoch 2996/8108\nTraining Loss: 32.0006, Training Accuracy: 0.0561\nEpoch 2997/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0840\nEpoch 2998/8108\nTraining Loss: 32.0056, Training Accuracy: 0.0969\nEpoch 2999/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0897\nEpoch 3000/8108\nTraining Loss: 31.9813, Training Accuracy: 0.0763\nEpoch 3001/8108\nTraining Loss: 32.0031, Training Accuracy: 0.0753\nEpoch 3002/8108\nTraining Loss: 31.9987, Training Accuracy: 0.1123\nEpoch 3003/8108\nTraining Loss: 31.9986, Training Accuracy: 0.0748\nEpoch 3004/8108\nTraining Loss: 31.9948, Training Accuracy: 0.1027\nEpoch 3005/8108\nTraining Loss: 31.9880, Training Accuracy: 0.0691\nEpoch 3006/8108\nTraining Loss: 31.9831, Training Accuracy: 0.0921\nEpoch 3007/8108\nTraining Loss: 31.9943, Training Accuracy: 0.0892\nEpoch 3008/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0820\nEpoch 3009/8108\nTraining Loss: 31.9781, Training Accuracy: 0.0983\nEpoch 3010/8108\nTraining Loss: 31.9994, Training Accuracy: 0.0935\nEpoch 3011/8108\nTraining Loss: 31.9960, Training Accuracy: 0.0792\nEpoch 3012/8108\nTraining Loss: 32.0107, Training Accuracy: 0.1075\nEpoch 3013/8108\nTraining Loss: 31.9809, Training Accuracy: 0.0940\nEpoch 3014/8108\nTraining Loss: 32.0080, Training Accuracy: 0.0720\nEpoch 3015/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0691\nEpoch 3016/8108\nTraining Loss: 31.9985, Training Accuracy: 0.0777\nEpoch 3017/8108\nTraining Loss: 31.9797, Training Accuracy: 0.0926\nEpoch 3018/8108\nTraining Loss: 31.9912, Training Accuracy: 0.0648\nEpoch 3019/8108\nTraining Loss: 31.9946, Training Accuracy: 0.0633\nEpoch 3020/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0907\nEpoch 3021/8108\nTraining Loss: 31.9974, Training Accuracy: 0.0825\nEpoch 3022/8108\nTraining Loss: 32.0131, Training Accuracy: 0.0897\nEpoch 3023/8108\nTraining Loss: 32.0022, Training Accuracy: 0.0945\nEpoch 3024/8108\nTraining Loss: 31.9965, Training Accuracy: 0.0748\nEpoch 3025/8108\nTraining Loss: 32.0090, Training Accuracy: 0.0662\nEpoch 3026/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0796\nEpoch 3027/8108\nTraining Loss: 31.9975, Training Accuracy: 0.0868\nEpoch 3028/8108\nTraining Loss: 31.9984, Training Accuracy: 0.0892\nEpoch 3029/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0777\nEpoch 3030/8108\nTraining Loss: 32.0085, Training Accuracy: 0.0792\nEpoch 3031/8108\nTraining Loss: 31.9954, Training Accuracy: 0.0883\nEpoch 3032/8108\nTraining Loss: 31.9862, Training Accuracy: 0.1027\nEpoch 3033/8108\nTraining Loss: 31.9698, Training Accuracy: 0.1012\nEpoch 3034/8108\nTraining Loss: 31.9814, Training Accuracy: 0.1022\nEpoch 3035/8108\nTraining Loss: 31.9815, Training Accuracy: 0.0840\nEpoch 3036/8108\nTraining Loss: 31.9978, Training Accuracy: 0.0811\nEpoch 3037/8108\nTraining Loss: 31.9999, Training Accuracy: 0.0897\nEpoch 3038/8108\nTraining Loss: 31.9791, Training Accuracy: 0.0691\nEpoch 3039/8108\nTraining Loss: 32.0095, Training Accuracy: 0.0921\nEpoch 3040/8108\nTraining Loss: 31.9780, Training Accuracy: 0.0835\nEpoch 3041/8108\nTraining Loss: 31.9710, Training Accuracy: 0.1012\nEpoch 3042/8108\nTraining Loss: 32.0097, Training Accuracy: 0.0734\nEpoch 3043/8108\nTraining Loss: 31.9679, Training Accuracy: 0.0897\nEpoch 3044/8108\nTraining Loss: 31.9966, Training Accuracy: 0.0825\nEpoch 3045/8108\nTraining Loss: 32.0043, Training Accuracy: 0.0849\nEpoch 3046/8108\nTraining Loss: 31.9969, Training Accuracy: 0.0676\nEpoch 3047/8108\nTraining Loss: 31.9884, Training Accuracy: 0.0964\nEpoch 3048/8108\nTraining Loss: 31.9801, Training Accuracy: 0.0897\nEpoch 3049/8108\nTraining Loss: 31.9930, Training Accuracy: 0.0892\nEpoch 3050/8108\nTraining Loss: 31.9702, Training Accuracy: 0.0820\nEpoch 3051/8108\nTraining Loss: 31.9694, Training Accuracy: 0.1079\nEpoch 3052/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0983\nEpoch 3053/8108\nTraining Loss: 32.0093, Training Accuracy: 0.0863\nEpoch 3054/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0753\nEpoch 3055/8108\nTraining Loss: 31.9958, Training Accuracy: 0.0782\nEpoch 3056/8108\nTraining Loss: 32.0072, Training Accuracy: 0.0811\nEpoch 3057/8108\nTraining Loss: 32.0023, Training Accuracy: 0.0955\nEpoch 3058/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0720\nEpoch 3059/8108\nTraining Loss: 32.0008, Training Accuracy: 0.1175\nEpoch 3060/8108\nTraining Loss: 32.0042, Training Accuracy: 0.0816\nEpoch 3061/8108\nTraining Loss: 31.9889, Training Accuracy: 0.0911\nEpoch 3062/8108\nTraining Loss: 32.0284, Training Accuracy: 0.0705\nEpoch 3063/8108\nTraining Loss: 31.9772, Training Accuracy: 0.0820\nEpoch 3064/8108\nTraining Loss: 31.9865, Training Accuracy: 0.0796\nEpoch 3065/8108\nTraining Loss: 31.9906, Training Accuracy: 0.0768\nEpoch 3066/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0955\nEpoch 3067/8108\nTraining Loss: 32.0001, Training Accuracy: 0.0926\nEpoch 3068/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0590\nEpoch 3069/8108\nTraining Loss: 31.9972, Training Accuracy: 0.1051\nEpoch 3070/8108\nTraining Loss: 32.0027, Training Accuracy: 0.0720\nEpoch 3071/8108\nTraining Loss: 31.9989, Training Accuracy: 0.0820\nEpoch 3072/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0840\nEpoch 3073/8108\nTraining Loss: 31.9837, Training Accuracy: 0.1218\nEpoch 3074/8108\nTraining Loss: 32.0041, Training Accuracy: 0.0619\nEpoch 3075/8108\nTraining Loss: 32.0057, Training Accuracy: 0.0720\nEpoch 3076/8108\nTraining Loss: 32.0079, Training Accuracy: 0.0878\nEpoch 3077/8108\nTraining Loss: 31.9913, Training Accuracy: 0.0964\nEpoch 3078/8108\nTraining Loss: 31.9936, Training Accuracy: 0.0710\nEpoch 3079/8108\nTraining Loss: 31.9778, Training Accuracy: 0.0921\nEpoch 3080/8108\nTraining Loss: 32.0033, Training Accuracy: 0.0998\nEpoch 3081/8108\nTraining Loss: 31.9848, Training Accuracy: 0.0753\nEpoch 3082/8108\nTraining Loss: 31.9921, Training Accuracy: 0.0935\nEpoch 3083/8108\nTraining Loss: 31.9925, Training Accuracy: 0.0911\nEpoch 3084/8108\nTraining Loss: 32.0090, Training Accuracy: 0.0691\nEpoch 3085/8108\nTraining Loss: 31.9992, Training Accuracy: 0.0921\nEpoch 3086/8108\nTraining Loss: 32.0010, Training Accuracy: 0.0863\nEpoch 3087/8108\nTraining Loss: 31.9993, Training Accuracy: 0.0691\nEpoch 3088/8108\nTraining Loss: 31.9983, Training Accuracy: 0.0753\nEpoch 3089/8108\nTraining Loss: 31.9898, Training Accuracy: 0.0710\nEpoch 3090/8108\nTraining Loss: 31.9727, Training Accuracy: 0.1070\nEpoch 3091/8108\nTraining Loss: 32.0099, Training Accuracy: 0.1027\nEpoch 3092/8108\nTraining Loss: 31.9831, Training Accuracy: 0.0911\nEpoch 3093/8108\nTraining Loss: 32.0016, Training Accuracy: 0.0887\nEpoch 3094/8108\nTraining Loss: 31.9967, Training Accuracy: 0.0945\nEpoch 3095/8108\nTraining Loss: 31.9816, Training Accuracy: 0.1007\nEpoch 3096/8108\nTraining Loss: 31.9838, Training Accuracy: 0.0681\nEpoch 3097/8108\nTraining Loss: 32.0075, Training Accuracy: 0.0705\nEpoch 3098/8108\nTraining Loss: 32.0044, Training Accuracy: 0.0676\nEpoch 3099/8108\nTraining Loss: 32.0046, Training Accuracy: 0.0748\nEpoch 3100/8108\nTraining Loss: 32.0110, Training Accuracy: 0.0897\nEpoch 3101/8108\nTraining Loss: 31.9935, Training Accuracy: 0.0681\nEpoch 3102/8108\nTraining Loss: 32.0009, Training Accuracy: 0.0921\nEpoch 3103/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0590\nEpoch 3104/8108\nTraining Loss: 31.9970, Training Accuracy: 0.0763\nEpoch 3105/8108\nTraining Loss: 31.9944, Training Accuracy: 0.0806\nEpoch 3106/8108\nTraining Loss: 31.9957, Training Accuracy: 0.0806\nEpoch 3107/8108\nTraining Loss: 31.9851, Training Accuracy: 0.0792\nEpoch 3108/8108\nTraining Loss: 31.9950, Training Accuracy: 0.0849\nEpoch 3109/8108\nTraining Loss: 32.0000, Training Accuracy: 0.0633\nEpoch 3110/8108\nTraining Loss: 31.9722, Training Accuracy: 0.0777\nEpoch 3111/8108\nTraining Loss: 31.9998, Training Accuracy: 0.1065\nEpoch 3112/8108\nTraining Loss: 32.0008, Training Accuracy: 0.0902\nEpoch 3113/8108\nTraining Loss: 32.0100, Training Accuracy: 0.0979\nEpoch 3114/8108\nTraining Loss: 31.9930, Training Accuracy: 0.1084\nEpoch 3115/8108\nTraining Loss: 31.9929, Training Accuracy: 0.0998\nEpoch 3116/8108\nTraining Loss: 31.9914, Training Accuracy: 0.0935\nEpoch 3117/8108\nTraining Loss: 31.9750, Training Accuracy: 0.0892\nEpoch 3118/8108\nTraining Loss: 31.9973, Training Accuracy: 0.0921\nEpoch 3119/8108\nTraining Loss: 31.9855, Training Accuracy: 0.0940\nEpoch 3120/8108\nTraining Loss: 32.0014, Training Accuracy: 0.1041\nEpoch 3121/8108\nTraining Loss: 32.0017, Training Accuracy: 0.0907\nEpoch 3122/8108\nTraining Loss: 31.9997, Training Accuracy: 0.0854\nEpoch 3123/8108\nTraining Loss: 32.0060, Training Accuracy: 0.0796\nEpoch 3124/8108\nTraining Loss: 31.9892, Training Accuracy: 0.0926\nEpoch 3125/8108\nTraining Loss: 31.9934, Training Accuracy: 0.0907\nEpoch 3126/8108\nTraining Loss: 31.9830, Training Accuracy: 0.0840\nEpoch 3127/8108\nTraining Loss: 31.9840, Training Accuracy: 0.0748\nEpoch 3128/8108\nTraining Loss: 31.9926, Training Accuracy: 0.0676\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-40950ce993f2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Call your training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# End the timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-611dea4fd694>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mtotal_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Converts tuple of tensors to a single tensor (7, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-dc28c82ce73d>\u001b[0m in \u001b[0;36mcustom_collate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom_collate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-dc28c82ce73d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom_collate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"results_filename = \"/kaggle/working/results/algo_online/algo_filternone_sampling_False.pt\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:20:21.965152Z","iopub.status.idle":"2025-02-11T14:20:21.965582Z","shell.execute_reply":"2025-02-11T14:20:21.965378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args.label = \"filternone_sampling_False\"\nanalyze_results(args.label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:20:21.966229Z","iopub.status.idle":"2025-02-11T14:20:21.966541Z","shell.execute_reply":"2025-02-11T14:20:21.966432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Same as used in paper of Grokfast\n    parser = ArgumentParser(description=\"Train a model on MNIST without custom sampling\")\n    \n    parser.add_argument(\"--label\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--p\", type=int, default=97)\n    parser.add_argument(\"--budget\", type=int, default=3e5)\n    parser.add_argument(\"--train_points\", type=int, default=1000)\n    parser.add_argument(\"--optimization_steps\", type=int, default=100000)\n    parser.add_argument(\"--batch_size\", type=int, default=512)\n    parser.add_argument(\"--optimizer\", type=str, default=\"Adam\")\n    parser.add_argument(\"--beta1\", type=float, default=0.9)\n    parser.add_argument(\"--beta2\", type=float, default=0.98)\n    parser.add_argument(\"--weight_decay\", type=float, default=0)\n    parser.add_argument(\"--lr\", type=float, default=1e-3)\n    parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n    parser.add_argument(\"--download_directory\", type=str, default=\".\")\n    parser.add_argument(\"--depth\", type=int, default=3)\n    parser.add_argument(\"--width\", type=int, default=200)\n    parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n\n    # Grokfast\n    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"ema\")\n    parser.add_argument(\"--alpha\", type=float, default=0.8)\n    parser.add_argument(\"--lamb\", type=float, default=0.1)\n\n    # Samples ranking\n    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n\n    # Boolean arguements need this due to bad behavior of parser.parse_args\n    def boolean_string(s):\n        if s not in {\"False\", \"True\"}:\n            raise ValueError(\"Not a valid boolean string\")\n        return s == \"True\"\n\n    # These are the hyperparameters related to our online sampling filtering algorithm\n    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n\n    # -----------------------------------------------------------------\n    # Try different hyperparameter values for your grid search here\n    # -----------------------------------------------------------------\n    args = parser.parse_args(\n        [\n            \"--appl_sampl_filter\", \"True\", # booleans as non strings in order to work\n            \"--sampling_distr_upd_freq\", \"1\", # the rest as strings for some reason\n            \"--top_k\", \"0.1\",\n            \"--top_k_sampling_prob\", \"0.9\",\n            \"--high_freq_better\", \"True\",\n        ]\n    )\n    # -----------------------------------------------------------------\n    # -----------------------------------------------------------------\n\n    # Create arg.label for the filename of the saved results\n    if not args.appl_sampl_filter:\n        args.label = f\"filter{args.filter}_sampling_{args.appl_sampl_filter}\"\n    else:\n        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n\n    # Training with time recording\n\n    # Start the timer\n    start_time = time.time()\n\n    # Call your training function\n    main(args)\n\n    # End the timer\n    end_time = time.time()\n\n    # Calculate elapsed time\n    elapsed_time = end_time - start_time\n\n    # Convert to minutes and seconds (optional)\n    minutes, seconds = divmod(elapsed_time, 60)\n\n    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n    print(f\"label:{args.label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:20:21.967367Z","iopub.status.idle":"2025-02-11T14:20:21.967709Z","shell.execute_reply":"2025-02-11T14:20:21.967544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args.label=\"filterema_sampling_False\"\nanalyze_results(args.label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define possible values for each parameter\nparam_grid = {\n    \"top_k\": [0.2],  # Convert to float\n    \"top_k_sampling_prob\": [0.7],  # Convert to float\n    \"sampling_distr_upd_freq\": [10],  # Convert to int\n    \"high_freq_better\": [True]  # Boolean parameter\n}\n\n# Generate all combinations of parameters\nparam_combinations = list(itertools.product(*param_grid.values()))\n\n# Run main in a loop for each combination\nfor param_values in param_combinations:\n    # Extract parameter values\n    top_k = param_values[0]\n    top_k_sampling_prob = param_values[1]\n    sampling_distr_upd_freq = param_values[2]\n    high_freq_better = param_values[3]  # Boolean value\n    \n    # Ensure boolean values are correctly formatted as strings for argparse\n    high_freq_better_str = \"True\" if high_freq_better else \"False\"\n\n    # Create args dynamically\n    args_list = [\n        \"--appl_sampl_filter\" , \"True\",\n        \"--top_k\", str(top_k),\n        \"--top_k_sampling_prob\", str(top_k_sampling_prob),\n        \"--sampling_distr_upd_freq\", str(sampling_distr_upd_freq),\n        \"--high_freq_better\", high_freq_better_str,\n        \"--label\", f\"{top_k}_{top_k_sampling_prob}_{sampling_distr_upd_freq}_{high_freq_better}\"\n    ]\n\n    # Debug print statement\n    print(f\"\\nRunning with parameters: {args_list}\")\n\n    # Parse the arguments dynamically\n    args = parser.parse_args(args_list)\n\n    # Call main() with the updated args\n    main(args)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#/kaggle/working/results/mnist_online/mnist_high_freq_True_top_k_0.2_top_k_prob_0.9_upd_freq_1.pt\nargs.label=\"high_freq_True_top_k_0.2_top_k_prob_0.9_upd_freq_1\"\nanalyze_results(args.label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    args_list = [\n        \"--appl_sampl_filter\" , \"False\",\n    ]\n\n    # Parse the arguments dynamically\n    args = parser.parse_args(args_list)\n\n    # Call main() with the updated args\n    main(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:43:00.339715Z","iopub.status.idle":"2025-02-11T09:43:00.340111Z","shell.execute_reply":"2025-02-11T09:43:00.339922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"analyze_results(args.label)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}