{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10611406,
          "sourceType": "datasetVersion",
          "datasetId": 6569340
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "name": "sample_complexity_dataset",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickxir12/PatRec_Project_2024-2025/blob/main/MNIST_code/mnist-sample-complexity-online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "konstantinosbarkas_mnist_dataset_processed_from_local_path = kagglehub.dataset_download('konstantinosbarkas/mnist-dataset-processed-from-local')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "QaD9qYj95rqk"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea 4: Samples fitering - online\n",
        "\n",
        "## Περιγραφή του αλγορίθμου\n",
        "- Γίνεται εκπαίδευση με grokfast - EMA. Όταν **appl_sampl_filter** is False έχω μόνο αυτό, ενώ για True εφαρμόζω επιπλέον και την ιδέα 4 για πιο έξυπνη επιλογή δειγμάτων.\n",
        "- Ο Dataloader έχει έναν custom sampler (WeightedRandomSampler) ο οποίος κάθε φορά διαλέγει ένα δείγμα με βάση κάποιο βάρος/πιθανότητα.\n",
        "- Στην αρχή τα βάρη είναι όλα ίδια (ομοιόμορφη κατανομή) οπότε ο Dataloader λειτουργεί όπως συνήθως διαλέγοντας τυχαία ένα sample.\n",
        "- Σε κάθε επανάληψη φτιάχνεται ένα ranking των δειγμάτων (με βάση του πόσο high frequency περιέχει το καθένα) το οποίο χρησιμοποιείται για να αποφασιστεί τι βάρος/πιθανότητα θα δοθεί σε κάθε δείγμα να επιλεγεί για εκπαίδευση. Το διάνυσμα βαρών/πιθανοτήτων ανανεώνεται κάθε **sampling_distr_upd_freq** επαναλήψεις.\n",
        "- Στην κατασκευή του διανύσματος βαρών από την συνολική πιθανότητα 1 δίνουμε στα **top_k** δείγματα συνολικά **top_k_sampling_prob** (και στα υπόλοιπα length(dataset) - **top_k** δείγματα δίνουμε συνολικά το υπόλοιπο 1 - **top_k_sampling_prob**).\n",
        "- Με **high_freq_better** is True ακολουθούμε την αρχική μας υπόθεση ότι τα δείγματα με high frequency είναι αυτά που θα πρέπει να ταΐσουμε το δίκτυο περισσότερο για να μάθει γρηγορότερα, για False γίνεται το αντίθετο.\n",
        "\n",
        "## Οδηγίες χρήσης για τρέξιμο\n",
        "Πήγαινε στον τίτλο **Execute training (by running main funciton)**. Πήγαινε στο parser.parse_args και όρισε τις τιμές που θες να δοκιμάσεις για grid search. Οι υπερπαράμετροι που σχετίζονται με την ιδέα 4 online είναι:\n",
        "\n",
        "- **top_k**\n",
        "- **top_k_sampling_prob**\n",
        "- **high_freq_better**\n",
        "- **sampling_distr_upd_freq**: Μάλλον είναι οκ στο 1 γιατί ακόμα και έτσι η εκπαίδευση δεν είναι αργή οπότε δεν έχω λόγο να το αυξήσω.\n",
        "\n",
        "Αν κάποιος θέλει να τρέξει κάποιες τιμές για το grid search, έχω βάλει στον φάκελο και ένα αρχείο για να σημειώνουμε τις τιμές των υπερπαραμέτρων που δοκίμασε ο καθένας για να μην τρέχουμε όλοι τα ίδια. Βάλτε GPU P100 (νομίζω είναι ελαφρώς καλύτερη), εμένα για τα 100.000 βήματα που έχω βάλει να είναι το default ένα τρέξιμο που κάνω μόνο με grokfast (δηλαδή **appl_sampl_filter** is False) παίρνει περίπου **7 λεπτά** οπότε καλά είμαστε από χρόνο.\n",
        "\n"
      ],
      "metadata": {
        "id": "_F8_0v245rqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Maybe this is needed if you want to import private datasets\n",
        "# kagglehub.login()\n"
      ],
      "metadata": {
        "id": "f4s6HWPGPSBJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:07.656295Z",
          "iopub.execute_input": "2025-01-30T10:24:07.656652Z",
          "iopub.status.idle": "2025-01-30T10:24:07.883452Z",
          "shell.execute_reply.started": "2025-01-30T10:24:07.656584Z",
          "shell.execute_reply": "2025-01-30T10:24:07.882396Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "# hojjatk_mnist_dataset_path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
        "\n",
        "# The dataset was uploaded from me but I made it public so you too can probably load it with this line\n",
        "_ = kagglehub.dataset_download(\"konstantinosbarkas/mnist-dataset-processed-from-local\")\n",
        "\n",
        "print(\"Data source import complete.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:07.884881Z",
          "iopub.execute_input": "2025-01-30T10:24:07.885147Z",
          "iopub.status.idle": "2025-01-30T10:24:07.954371Z",
          "shell.execute_reply.started": "2025-01-30T10:24:07.885123Z",
          "shell.execute_reply": "2025-01-30T10:24:07.953348Z"
        },
        "id": "uDY1WE__5rqq",
        "outputId": "5a61be1c-b302-45fe-e557-58a5e6b9b6fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Data source import complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:07.955511Z",
          "iopub.execute_input": "2025-01-30T10:24:07.955872Z",
          "iopub.status.idle": "2025-01-30T10:24:07.964761Z",
          "shell.execute_reply.started": "2025-01-30T10:24:07.955831Z",
          "shell.execute_reply": "2025-01-30T10:24:07.963752Z"
        },
        "id": "Jd_DXpbm5rqr",
        "outputId": "ba85a2c0-51d3-4b7f-e36c-0c6388d8b6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/MNIST_data_processed_from_local/train.pt\n/kaggle/input/MNIST_data_processed_from_local/test.pt\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r /kaggle/input/enter-data-dn-req/requirements.txt\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:07.96592Z",
          "iopub.execute_input": "2025-01-30T10:24:07.966294Z",
          "iopub.status.idle": "2025-01-30T10:24:07.977153Z",
          "shell.execute_reply.started": "2025-01-30T10:24:07.966222Z",
          "shell.execute_reply": "2025-01-30T10:24:07.976232Z"
        },
        "id": "gdOjsJQK5rqs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Grokfast library\n",
        "!wget https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\n",
        "\n",
        "sys.path.append(\"/kaggle/working\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:07.980035Z",
          "iopub.execute_input": "2025-01-30T10:24:07.980316Z",
          "iopub.status.idle": "2025-01-30T10:24:08.22142Z",
          "shell.execute_reply.started": "2025-01-30T10:24:07.980291Z",
          "shell.execute_reply": "2025-01-30T10:24:08.220134Z"
        },
        "id": "ixtJ3QNx5rqs",
        "outputId": "cf14146f-8e26-43bc-bfa4-2ab0248cd3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "--2025-01-30 10:24:08--  https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1703 (1.7K) [text/plain]\nSaving to: ‘grokfast.py.3’\n\ngrokfast.py.3       100%[===================>]   1.66K  --.-KB/s    in 0s      \n\n2025-01-30 10:24:08 (21.0 MB/s) - ‘grokfast.py.3’ saved [1703/1703]\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import gzip\n",
        "import math\n",
        "import random\n",
        "# import struct\n",
        "import time\n",
        "from argparse import ArgumentParser\n",
        "# from collections import Counter, defaultdict, deque\n",
        "from itertools import islice\n",
        "# from pathlib import Path\n",
        "# from typing import Dict, List, Literal, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "from functorch import grad, vmap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import grad\n",
        "\n",
        "# from torch.nn.utils.stateless import functional_call, # This is deprecated, use the next one instead\n",
        "from torch.func import functional_call\n",
        "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler, Dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from grokfast import gradfilter_ema\n"
      ],
      "metadata": {
        "id": "QLUi9XZMRpId",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:08.223722Z",
          "iopub.execute_input": "2025-01-30T10:24:08.22406Z",
          "iopub.status.idle": "2025-01-30T10:24:11.7782Z",
          "shell.execute_reply.started": "2025-01-30T10:24:08.22403Z",
          "shell.execute_reply": "2025-01-30T10:24:11.777178Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "results_dir = \"/kaggle/working/results/mnist_online\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "dataset_path = \"/kaggle/input/MNIST_data_processed_from_local/\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.779289Z",
          "iopub.execute_input": "2025-01-30T10:24:11.779893Z",
          "iopub.status.idle": "2025-01-30T10:24:11.784493Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.779855Z",
          "shell.execute_reply": "2025-01-30T10:24:11.783404Z"
        },
        "id": "IMz5Dkwb5rqt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_dict = {\"AdamW\": torch.optim.AdamW, \"Adam\": torch.optim.Adam, \"SGD\": torch.optim.SGD}\n",
        "\n",
        "activation_dict = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"Sigmoid\": nn.Sigmoid, \"GELU\": nn.GELU}\n",
        "\n",
        "loss_function_dict = {\"MSE\": nn.MSELoss, \"CrossEntropy\": nn.CrossEntropyLoss}\n"
      ],
      "metadata": {
        "id": "8hhZpMIuSC4q",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.785854Z",
          "iopub.execute_input": "2025-01-30T10:24:11.786094Z",
          "iopub.status.idle": "2025-01-30T10:24:11.805068Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.786072Z",
          "shell.execute_reply": "2025-01-30T10:24:11.804014Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n"
      ],
      "metadata": {
        "id": "RIifrNowR89e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.805997Z",
          "iopub.execute_input": "2025-01-30T10:24:11.806382Z",
          "iopub.status.idle": "2025-01-30T10:24:11.823522Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.806342Z",
          "shell.execute_reply": "2025-01-30T10:24:11.822578Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn_2(batch):\n",
        "    \"\"\"Custom collate function to handle extra fields in the dataset.\"\"\"\n",
        "    images, labels, _, _ = zip(*batch)  # Ignore the indices and extra_fields for loss computation\n",
        "    images = torch.stack(images)  # Stack images into a single tensor\n",
        "    labels = torch.tensor(labels)  # Convert labels to a tensor\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def compute_accuracy(model, dataset, device, N=None):\n",
        "    \"\"\"Utility to compute accuracy on a given dataset.\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=256, shuffle=False, collate_fn=custom_collate_fn_2)\n",
        "\n",
        "    for x, y in loader:  # Unpack index and extra_fields as well\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x)\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "        correct += (predictions == y).sum().item()\n",
        "        total += y.size(0)\n",
        "        if N is not None and total >= N:\n",
        "            break\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def compute_loss(model, dataset, loss_function_name, device, N=None):\n",
        "    \"\"\"Utility to compute the average loss on a given dataset.\"\"\"\n",
        "    loss_fn = loss_function_dict[loss_function_name]()\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=256, shuffle=False, collate_fn=custom_collate_fn_2)\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    one_hots = torch.eye(10, device=device)\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x)\n",
        "            if loss_function_name == \"CrossEntropy\":\n",
        "                loss = loss_fn(outputs, y)\n",
        "            elif loss_function_name == \"MSE\":\n",
        "                loss = loss_fn(outputs, one_hots[y])\n",
        "        batch_size = x.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        count += batch_size\n",
        "        if N is not None and count >= N:\n",
        "            break\n",
        "    return total_loss / count\n"
      ],
      "metadata": {
        "id": "zlciE-2nKkLg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.824482Z",
          "iopub.execute_input": "2025-01-30T10:24:11.824873Z",
          "iopub.status.idle": "2025-01-30T10:24:11.836585Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.824835Z",
          "shell.execute_reply": "2025-01-30T10:24:11.835677Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Added extra fields to keep ema_gra and history\n",
        "# In this implementation I use variance metric --> I don't also store deviation metric for memory efficiency\n",
        "class MyMNIST(torchvision.datasets.MNIST):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset to simulate MyMNIST behavior with extra fields.\n",
        "        Args:\n",
        "            data: Tensor of shape [N, 28, 28].\n",
        "            targets: Tensor of shape [N].\n",
        "            transform: Transformations to apply to the images.\n",
        "        \"\"\"\n",
        "        self.data = data.to(torch.float32)\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "        # Initialize extra fields\n",
        "        self.extra_fields = [\n",
        "            {\n",
        "                \"ema_grad\": 0.0,  # EMA of gradient\n",
        "                \"num_updates\": 0,\n",
        "                # \"deviation_metric\": 0.0, # Deviation metric\n",
        "                \"variance_metric\": 0.0,  # Variance metric\n",
        "            }\n",
        "            for _ in range(len(self.data))\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns a single data sample and its associated extra fields.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        extra_field = self.extra_fields[index]\n",
        "        return img, target, index, extra_field\n",
        "\n",
        "    def update_fields(self, indices, grad_stats, ema_alpha=0.9):\n",
        "        \"\"\"\n",
        "        Update the extra fields for specified dataset indices.\n",
        "        \"\"\"\n",
        "\n",
        "        for idx, grad in zip(indices, grad_stats):\n",
        "            # Update EMA\n",
        "            sample_field = self.extra_fields[idx]\n",
        "\n",
        "            current_ema = sample_field[\"ema_grad\"]\n",
        "            updated_ema = ema_alpha * current_ema + (1 - ema_alpha) * grad\n",
        "            sample_field[\"ema_grad\"] = updated_ema\n",
        "\n",
        "            deviation = abs(grad - updated_ema)\n",
        "\n",
        "            num_updates = sample_field[\"num_updates\"] + 1  # Increment the update count\n",
        "            # current_avg_deviation = sample_field[\"deviation_metric\"]\n",
        "            current_avg_deviation = sample_field[\"variance_metric\"] ** 0.5\n",
        "            new_avg_deviation = ((current_avg_deviation * (num_updates - 1)) + deviation) / num_updates\n",
        "\n",
        "            # Update the deviation metric and number of updates\n",
        "            sample_field[\"deviation_metric\"] = new_avg_deviation\n",
        "            sample_field[\"num_updates\"] = num_updates\n",
        "\n",
        "            # sample_field[\"deviation_metric\"] += deviation\n",
        "\n",
        "            # Optionally update variance-like metric\n",
        "            # Variance estimate (for higher sensitivity to fast changes)\n",
        "            sample_field[\"variance_metric\"] = new_avg_deviation**2\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.837561Z",
          "iopub.execute_input": "2025-01-30T10:24:11.837994Z",
          "iopub.status.idle": "2025-01-30T10:24:11.858766Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.837914Z",
          "shell.execute_reply": "2025-01-30T10:24:11.857755Z"
        },
        "id": "1eQMRjBl5rqv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    images, labels, indices, extra_fields = zip(*batch)\n",
        "    images = torch.stack(images)  # Stack images into a single tensor\n",
        "    labels = torch.tensor(labels)  # Convert labels to a tensor\n",
        "    return images, labels, indices, extra_fields\n"
      ],
      "metadata": {
        "id": "zif6Q-IEjFJ7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:38:11.386695Z",
          "iopub.execute_input": "2025-01-30T10:38:11.387104Z",
          "iopub.status.idle": "2025-01-30T10:38:11.392458Z",
          "shell.execute_reply.started": "2025-01-30T10:38:11.387075Z",
          "shell.execute_reply": "2025-01-30T10:38:11.39144Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed for per sample gradient computations\n",
        "def select_random_subset(tensor, percentage, seed=42):\n",
        "    \"\"\"\n",
        "    Flatten the parameter dimensions for each batch sample, select a percentage of elements,\n",
        "    and return a tensor with shape [batch_size, selected_elements].\n",
        "\n",
        "    Args:\n",
        "        tensor (torch.Tensor): The gradient tensor of shape [batch_size, *parameter_dims].\n",
        "        percentage (float): The percentage of elements to select.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of shape [batch_size, selected_elements].\n",
        "    \"\"\"\n",
        "    batch_size, *param_dims = tensor.shape  # Extract batch and parameter dimensions\n",
        "    total_params = torch.prod(torch.tensor(param_dims))  # Total parameters per sample\n",
        "    subset_size = int(total_params * percentage)  # 20% of parameters\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    random.seed(seed)\n",
        "    indices = random.sample(range(total_params), subset_size)  # Random indices for selection\n",
        "\n",
        "    # Flatten parameter dimensions and select elements for each batch\n",
        "    flat_tensor = tensor.view(batch_size, -1)  # Flatten parameter dimensions for each sample\n",
        "    selected_subset = flat_tensor[:, indices]  # Select the same random indices across the batch\n",
        "\n",
        "    return selected_subset\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.881864Z",
          "iopub.execute_input": "2025-01-30T10:24:11.882204Z",
          "iopub.status.idle": "2025-01-30T10:24:11.892537Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.882178Z",
          "shell.execute_reply": "2025-01-30T10:24:11.891601Z"
        },
        "id": "mVipEiHn5rqv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed for online sample filtering\n",
        "def rank_to_sampling_weights(my_dataset, top_k, top_k_sampling_prob, high_freq_better):\n",
        "    \"\"\"\n",
        "    Rank samples by variance_metric and assign sampling weights.\n",
        "\n",
        "    Parameters:\n",
        "    - my_dataset: MyMNIST object.\n",
        "    - top_k: Fraction of top samples to assign higher sampling probability.\n",
        "    - top_k_sampling_prob: Probability assigned to the top_k fraction of samples.\n",
        "\n",
        "    Returns:\n",
        "    - new_weights: List of sampling weights for each sample.\n",
        "    \"\"\"\n",
        "    # Calculate the number of top_k samples\n",
        "    num_samples = len(my_dataset)\n",
        "    top_k_count = int(top_k * num_samples)\n",
        "\n",
        "    # Sort indices by variance_metric in descending order\n",
        "    sorted_indices = sorted(\n",
        "        range(num_samples),\n",
        "        key=lambda idx: my_dataset.dataset.extra_fields[idx][\"variance_metric\"],\n",
        "        reverse=high_freq_better,\n",
        "    )\n",
        "\n",
        "    # Initialize new_weights with zeros\n",
        "    new_weights = [0.0] * num_samples\n",
        "\n",
        "    # Assign weights to the top_k samples\n",
        "    for idx in sorted_indices[:top_k_count]:\n",
        "        new_weights[idx] = top_k_sampling_prob / top_k_count\n",
        "\n",
        "    # Assign weights to the rest of the samples\n",
        "    for idx in sorted_indices[top_k_count:]:\n",
        "        new_weights[idx] = (1 - top_k_sampling_prob) / (num_samples - top_k_count)\n",
        "\n",
        "    return new_weights\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:25:20.482578Z",
          "iopub.execute_input": "2025-01-30T10:25:20.483016Z",
          "iopub.status.idle": "2025-01-30T10:25:20.489296Z",
          "shell.execute_reply.started": "2025-01-30T10:25:20.482988Z",
          "shell.execute_reply": "2025-01-30T10:25:20.488133Z"
        },
        "id": "P2QMkHyk5rqv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def main"
      ],
      "metadata": {
        "id": "HYoiLJ_u5rqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from functorch import vmap, grad\n",
        "from torch.nn.utils.stateless import functional_call\n",
        "\n",
        "def main(args):\n",
        "    log_freq = math.ceil(args.optimization_steps / 150)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dtype = torch.float32\n",
        "    one_hots = torch.eye(10, 10).to(device)\n",
        "\n",
        "    torch.set_default_dtype(dtype)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    #                                 Load dataset\n",
        "    #   -------------------------------------------------------------------------------   #\n",
        "    train_data = torch.load(f\"{dataset_path}/train.pt\", weights_only=True)\n",
        "    test_data = torch.load(f\"{dataset_path}/test.pt\", weights_only=True)\n",
        "\n",
        "    transform = None\n",
        "\n",
        "    train_images, train_labels = train_data\n",
        "    test_images, test_labels = test_data\n",
        "\n",
        "    # Create MyMNIST datasets\n",
        "    train_dataset = MyMNIST(train_images, train_labels, transform=transform)\n",
        "    test_dataset = MyMNIST(test_images, test_labels, transform=transform)\n",
        "    test = test_dataset  # For compatibility with our older code\n",
        "\n",
        "    # Create indices & stratify\n",
        "    train_indices = list(range(len(train_dataset)))\n",
        "    train_labels = [train_dataset.targets[i].item() for i in train_indices]\n",
        "\n",
        "    # Use train_test_split with stratification to randomly select a specified number of samples (args.train_points)\n",
        "    stratified_indices, _ = train_test_split(\n",
        "        train_indices,\n",
        "        train_size=args.train_points,\n",
        "        stratify=train_labels,\n",
        "        random_state=args.seed,\n",
        "    )\n",
        "\n",
        "    train_subset = Subset(train_dataset, stratified_indices)\n",
        "\n",
        "    # Create initial weights for uniform sampling\n",
        "    weights = [1.0] * len(train_subset)\n",
        "    sampler = WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=args.batch_size, sampler=sampler, collate_fn=custom_collate_fn)\n",
        "\n",
        "    activation_fn = activation_dict[args.activation]\n",
        "\n",
        "    #                                   Create model\n",
        "    #   -------------------------------------------------------------------------------   #\n",
        "\n",
        "    layers = [nn.Flatten()]\n",
        "    for i in range(args.depth):\n",
        "        if i == 0:\n",
        "            layers.append(nn.Linear(784, args.width))\n",
        "            layers.append(activation_fn())\n",
        "        elif i == args.depth - 1:\n",
        "            layers.append(nn.Linear(args.width, 10))\n",
        "        else:\n",
        "            layers.append(nn.Linear(args.width, args.width))\n",
        "            layers.append(activation_fn())\n",
        "    mlp = nn.Sequential(*layers).to(device)\n",
        "    with torch.no_grad():\n",
        "        for p in mlp.parameters():\n",
        "            p.data = args.initialization_scale * p.data\n",
        "    nparams = sum([p.numel() for p in mlp.parameters() if p.requires_grad])\n",
        "    print(f\"Number of parameters: {nparams}\")\n",
        "\n",
        "    # create optimizer\n",
        "    assert args.optimizer in optimizer_dict, f\"Unsupported optimizer choice: {args.optimizer}\"\n",
        "    optimizer = optimizer_dict[args.optimizer](mlp.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "    # define loss function\n",
        "    assert args.loss_function in loss_function_dict\n",
        "    loss_fn = loss_function_dict[args.loss_function]()\n",
        "\n",
        "    # Needed for per sample gradient computations\n",
        "    if args.appl_sampl_filter:\n",
        "        # Define a function for forward + loss computation\n",
        "        def compute_loss_vmap(params, buffers, model, x, y):\n",
        "            # Use functional_call to pass parameters and buffers explicitly\n",
        "            logits = functional_call(model, {**params, **buffers}, x.unsqueeze(0))  # Single input\n",
        "            loss = loss_fn(logits, y.unsqueeze(0))  # Single output\n",
        "            return loss.mean()\n",
        "\n",
        "        # Prepare model parameters and buffers\n",
        "        params_and_buffers = {**dict(mlp.named_parameters()), **dict(mlp.named_buffers())}\n",
        "\n",
        "        params = {k: v for k, v in params_and_buffers.items() if v.requires_grad}\n",
        "        buffers = {k: v for k, v in params_and_buffers.items() if not v.requires_grad}\n",
        "\n",
        "\n",
        "        # Create the gradient function\n",
        "        gradient_fn = grad(compute_loss_vmap)\n",
        "\n",
        "        # Initialize EMA and metric history for each sample\n",
        "        gradient_ema = [0.0 for _ in range(len(train_subset))]\n",
        "        # gradient_metric_history = [[] for _ in range(len(train_subset))] # Probably unused\n",
        "\n",
        "    #                           Start Training below\n",
        "    #   -------------------------------------------------------------------------------   #\n",
        "    log_steps, train_losses, train_accuracies, test_losses, test_accuracies = [], [], [], [], []\n",
        "    one_hots = torch.eye(10, 10).to(device)\n",
        "\n",
        "    grads = None\n",
        "\n",
        "    with tqdm(total=args.optimization_steps, dynamic_ncols=True) as pbar:\n",
        "\n",
        "        reached_early_stop = False  # Flag to indicate early stopping\n",
        "        steps_to_reach_val_acc = None  # Variable to store steps for 0.95 validation accuracy\n",
        "\n",
        "        stable_threshold = 100  # Number of steps the validation accuracy must remain > 0.9\n",
        "        stable_steps = 0  # Counter for steps validation accuracy remains above 0.9\n",
        "\n",
        "        for step in range(args.optimization_steps):\n",
        "            if reached_early_stop: break\n",
        "            # Update the sampling distribution (according to the latest ranking of the samples)\n",
        "            if args.appl_sampl_filter:\n",
        "                if step % args.sampling_distr_upd_freq == 0 and step != 0:\n",
        "                    # Update the weights of the sampling based on the latest gradient metrics\n",
        "                    weights = rank_to_sampling_weights(train_subset, args.top_k, args.top_k_sampling_prob, args.high_freq_better)\n",
        "                    sampler = WeightedRandomSampler(weights, num_samples=len(weights))\n",
        "                    train_loader = DataLoader(\n",
        "                        train_subset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        sampler=sampler,\n",
        "                        collate_fn=custom_collate_fn,\n",
        "                    )\n",
        "\n",
        "            for batch in islice(cycle(train_loader),1):\n",
        "                x, labels, indices, _ = batch\n",
        "                do_log = (step < 30) or (step < 150 and step % 10 == 0) or step % log_freq == 0\n",
        "                if do_log:\n",
        "                    train_losses.append(compute_loss(mlp, train_subset, args.loss_function, device, N=len(train_subset)))\n",
        "                    train_accuracies.append(compute_accuracy(mlp, train_subset, device, N=len(train_subset)))\n",
        "                    test_losses.append(compute_loss(mlp, test, args.loss_function, device, N=len(test)))\n",
        "                    test_accuracies.append(compute_accuracy(mlp, test, device, N=len(test)))\n",
        "                    log_steps.append(step)\n",
        "                    pbar.set_description(\n",
        "                        \"Loss: {0:1.1e}|{1:1.1e}. Acc: {2:2.1f}%|{3:2.1f}%\".format(\n",
        "                            train_losses[-1],\n",
        "                            test_losses[-1],\n",
        "                            train_accuracies[-1] * 100,\n",
        "                            test_accuracies[-1] * 100,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                x, labels = x.to(device), labels.to(device)\n",
        "                y = mlp(x.to(device))\n",
        "                if args.loss_function == \"CrossEntropy\":\n",
        "                    # Use integer labels for CrossEntropyLoss\n",
        "                    loss = loss_fn(y, labels)\n",
        "                elif args.loss_function == \"MSE\":\n",
        "                    loss = loss_fn(y, one_hots[labels])\n",
        "\n",
        "                loss.backward(create_graph=True)\n",
        "\n",
        "                if args.appl_sampl_filter:  # Unnecessary if we are not applying sample filtering\n",
        "                    # -----------------------------------------------------------------\n",
        "                    #   Gradient Stats: Capture grads for each sample\n",
        "                    # -----------------------------------------------------------------\n",
        "                    # Identify the last two Linear layers dynamically\n",
        "                    batch_gradients = []\n",
        "\n",
        "                    # Use vmap for batch gradient computation\n",
        "                    per_sample_grads = vmap(gradient_fn, in_dims=(None, None, None, 0, 0))(params, buffers, mlp, x, labels)\n",
        "\n",
        "                    last_layer_grad = per_sample_grads[\"3.weight\"]  # Replace with your actual parameter name\n",
        "                    second_last_layer_grad = per_sample_grads[\"5.weight\"]\n",
        "\n",
        "                    percentage_s_l = 0.2  # Adjust percentage as needed\n",
        "                    percentage_l = 0.5\n",
        "                    selected_last = select_random_subset(last_layer_grad, percentage_l, seed=42)\n",
        "                    selected_second_last = select_random_subset(second_last_layer_grad, percentage_s_l, seed=42)\n",
        "\n",
        "                    selected_last_avg = selected_last.mean(dim=-1)\n",
        "                    selected_second_last_avg = selected_second_last.mean(dim=-1)\n",
        "                    total_avg = (selected_last_avg + selected_second_last_avg) / 2\n",
        "\n",
        "                    train_subset.dataset.update_fields(indices, total_avg, args.ema_alpha_sampl_rank)\n",
        "\n",
        "                    # -----------------------------------------------------------------\n",
        "                    # -----------------------------------------------------------------\n",
        "\n",
        "                # Grokfast (EMA)\n",
        "                grads = gradfilter_ema(mlp, grads=grads, alpha=args.alpha, lamb=args.lamb)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                # Check for early stopping conditions\n",
        "                test_acc = test_accuracies[-1] if len(test_accuracies) > 0 else 0\n",
        "\n",
        "                # Normally it is for 0.95 - 0.9\n",
        "                if test_acc >= 0.9 and steps_to_reach_val_acc is None:\n",
        "                    steps_to_reach_val_acc = step  # Record the first step reaching 0.9 validation accuracy\n",
        "\n",
        "                if test_acc > 0.85:\n",
        "                    stable_steps += 1\n",
        "                else:\n",
        "                    stable_steps = 0  # Reset counter if accuracy drops below 0.85\n",
        "\n",
        "                if stable_steps >= stable_threshold and test_acc >= 0.9:\n",
        "                    reached_early_stop = True\n",
        "                    print(f\"Validation accuracy of 0.9 reached and remained > 0.85 for {stable_threshold} step at step {step}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "\n",
        "    # Save results\n",
        "    specific_result_dir = f\"mnist_{args.label}.pt\"\n",
        "    results_filename = os.path.join(results_dir, specific_result_dir)\n",
        "    torch.save(\n",
        "        {\n",
        "            \"its\": log_steps,\n",
        "            \"train_acc\": train_accuracies,\n",
        "            \"train_loss\": train_losses,\n",
        "            \"val_acc\": test_accuracies,\n",
        "            \"val_loss\": test_losses,\n",
        "            \"steps_to_reach\": steps_to_reach_val_acc,\n",
        "            \"model_state_dict\": mlp.state_dict(),  # Save the model's state dictionary, maybe unnecessary\n",
        "        },\n",
        "        results_filename,\n",
        "    )\n",
        "\n",
        "    print(f\"\\nSteps needed to reach 0.9 validation accuracy: {steps_to_reach_val_acc}\")\n"
      ],
      "metadata": {
        "id": "LWPwaBWpSF52",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:56:12.195389Z",
          "iopub.execute_input": "2025-01-30T10:56:12.195763Z",
          "iopub.status.idle": "2025-01-30T10:56:12.219811Z",
          "shell.execute_reply.started": "2025-01-30T10:56:12.195725Z",
          "shell.execute_reply": "2025-01-30T10:56:12.218881Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the extra arguments passed by the Jupyter Notebook kernel\n",
        "sys.argv = [\"\"]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.944837Z",
          "iopub.execute_input": "2025-01-30T10:24:11.945128Z",
          "iopub.status.idle": "2025-01-30T10:24:11.96445Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.945086Z",
          "shell.execute_reply": "2025-01-30T10:24:11.963529Z"
        },
        "id": "kqlS1k3I5rqw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute training (by running main function)"
      ],
      "metadata": {
        "id": "khmxiBv55rqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom parameters (not the same with Grokfast's implementation)\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Same as used in paper of Grokfast\n",
        "#     parser = ArgumentParser(description=\"Train a model on MNIST with custom sampling\")\n",
        "\n",
        "#     parser.add_argument(\"--label\", type=str, default=\"\")\n",
        "#     parser.add_argument(\"--seed\", type=int, default=42)\n",
        "\n",
        "#     parser.add_argument(\"--train_points\", type=int, default=1000)\n",
        "#     parser.add_argument(\"--optimization_steps\", type=int, default=100000)\n",
        "#     parser.add_argument(\"--batch_size\", type=int, default=200)\n",
        "#     parser.add_argument(\"--loss_function\", type=str, default=\"CrossEntropy\")  # changed\n",
        "#     parser.add_argument(\"--optimizer\", type=str, default=\"AdamW\")\n",
        "#     parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
        "#     parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "#     parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n",
        "#     parser.add_argument(\"--download_directory\", type=str, default=\".\")\n",
        "#     parser.add_argument(\"--depth\", type=int, default=5)  # changed\n",
        "#     parser.add_argument(\"--width\", type=int, default=512)  # changed\n",
        "#     parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n",
        "\n",
        "#     # Grokfast\n",
        "#     parser.add_argument(\"--alpha\", type=float, default=0.99)\n",
        "#     parser.add_argument(\"--lamb\", type=float, default=5.0)\n",
        "\n",
        "#     # Samples ranking\n",
        "#     parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n",
        "\n",
        "#     # Boolean arguements need this due to bad behavior of parser.parse_args\n",
        "#     def boolean_string(s):\n",
        "#         if s not in {\"False\", \"True\"}:\n",
        "#             raise ValueError(\"Not a valid boolean string\")\n",
        "#         return s == \"True\"\n",
        "\n",
        "#     # These are the hyperparameters related to our online sampling filtering algorithm\n",
        "#     parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n",
        "#     parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n",
        "#     parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n",
        "#     parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n",
        "#     parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n",
        "\n",
        "#     # -----------------------------------------------------------------\n",
        "#     # Try different hyperparameter values for your grid search here\n",
        "#     # -----------------------------------------------------------------\n",
        "#     args = parser.parse_args(\n",
        "#         [\n",
        "#             \"--appl_sampl_filter\", \"False\", # booleans as non strings in order to work\n",
        "#             \"--sampling_distr_upd_freq\", \"1\", # the rest as strings for some reason\n",
        "#             \"--top_k\", \"0.1\",\n",
        "#             \"--top_k_sampling_prob\", \"0.9\",\n",
        "#             \"--high_freq_better\", \"True\",\n",
        "#         ]\n",
        "#     )\n",
        "#     # -----------------------------------------------------------------\n",
        "#     # -----------------------------------------------------------------\n",
        "\n",
        "#     # Create arg.label for the filename of the saved results\n",
        "#     if not args.appl_sampl_filter:\n",
        "#         args.label = \"only_grokfast\"\n",
        "#     else:\n",
        "#         args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n",
        "\n",
        "#     # Training with time recording\n",
        "\n",
        "#     # Start the timer\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     # Call your training function\n",
        "#     main(args)\n",
        "\n",
        "#     # End the timer\n",
        "#     end_time = time.time()\n",
        "\n",
        "#     # Calculate elapsed time\n",
        "#     elapsed_time = end_time - start_time\n",
        "\n",
        "#     # Convert to minutes and seconds (optional)\n",
        "#     minutes, seconds = divmod(elapsed_time, 60)\n",
        "\n",
        "#     print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n"
      ],
      "metadata": {
        "id": "MawvRmI9yt3T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:24:11.967296Z",
          "iopub.execute_input": "2025-01-30T10:24:11.967585Z",
          "iopub.status.idle": "2025-01-30T10:24:11.982261Z",
          "shell.execute_reply.started": "2025-01-30T10:24:11.967559Z",
          "shell.execute_reply": "2025-01-30T10:24:11.981225Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Same as used in paper of Grokfast\n",
        "    parser = ArgumentParser(description=\"Train a model on MNIST with custom sampling\")\n",
        "\n",
        "    parser.add_argument(\"--label\", type=str, default=\"\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "\n",
        "    parser.add_argument(\"--train_points\", type=int, default=1000)\n",
        "    parser.add_argument(\"--optimization_steps\", type=int, default=100000)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=200)\n",
        "    parser.add_argument(\"--loss_function\", type=str, default=\"MSE\")\n",
        "    parser.add_argument(\"--optimizer\", type=str, default=\"AdamW\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n",
        "    parser.add_argument(\"--download_directory\", type=str, default=\".\")\n",
        "    parser.add_argument(\"--depth\", type=int, default=3)\n",
        "    parser.add_argument(\"--width\", type=int, default=200)\n",
        "    parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n",
        "\n",
        "    # Grokfast\n",
        "    parser.add_argument(\"--alpha\", type=float, default=0.8)\n",
        "    parser.add_argument(\"--lamb\", type=float, default=0.1)\n",
        "\n",
        "    # Samples ranking\n",
        "    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n",
        "\n",
        "    # Boolean arguements need this due to bad behavior of parser.parse_args\n",
        "    def boolean_string(s):\n",
        "        if s not in {\"False\", \"True\"}:\n",
        "            raise ValueError(\"Not a valid boolean string\")\n",
        "        return s == \"True\"\n",
        "\n",
        "    # These are the hyperparameters related to our online sampling filtering algorithm\n",
        "    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n",
        "    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n",
        "    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n",
        "    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n",
        "    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # Try different hyperparameter values for your grid search here\n",
        "    # -----------------------------------------------------------------\n",
        "    args = parser.parse_args(\n",
        "        [\n",
        "            \"--appl_sampl_filter\", \"True\", # booleans as non strings in order to work\n",
        "            \"--sampling_distr_upd_freq\", \"1\", # the rest as strings for some reason\n",
        "            \"--top_k\", \"0.1\",\n",
        "            \"--top_k_sampling_prob\", \"0.9\",\n",
        "            \"--high_freq_better\", \"True\",\n",
        "        ]\n",
        "    )\n",
        "    # -----------------------------------------------------------------\n",
        "    # -----------------------------------------------------------------\n",
        "\n",
        "    # Create arg.label for the filename of the saved results\n",
        "    if not args.appl_sampl_filter:\n",
        "        args.label = \"only_grokfast\"\n",
        "    else:\n",
        "        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n",
        "\n",
        "    # Training with time recording\n",
        "\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Call your training function\n",
        "    main(args)\n",
        "\n",
        "    # End the timer\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Convert to minutes and seconds (optional)\n",
        "    minutes, seconds = divmod(elapsed_time, 60)\n",
        "\n",
        "    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:56:15.860877Z",
          "iopub.execute_input": "2025-01-30T10:56:15.861223Z",
          "iopub.status.idle": "2025-01-30T11:08:43.781174Z",
          "shell.execute_reply.started": "2025-01-30T10:56:15.861193Z",
          "shell.execute_reply": "2025-01-30T11:08:43.779784Z"
        },
        "id": "ic8iXmLw5rqx",
        "outputId": "d18d2d28-ac0a-4672-c867-7f17bce0f2ee",
        "colab": {
          "referenced_widgets": [
            "e8adf849bedc4a62bea1e2a0e1269304"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of parameters: 199210\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "<ipython-input-55-e74ed5448420>:100: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.grad` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.grad` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n  gradient_fn = grad(compute_loss_vmap)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/100000 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8adf849bedc4a62bea1e2a0e1269304"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "<ipython-input-55-e74ed5448420>:174: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n  per_sample_grads = vmap(gradient_fn, in_dims=(None, None, None, 0, 0))(params, buffers, mlp, x, labels)\n<ipython-input-55-e74ed5448420>:88: FutureWarning: `torch.nn.utils.stateless.functional_call` is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use `torch.func.functional_call` instead which is a drop-in replacement.\n  logits = functional_call(model, {**params, **buffers}, x.unsqueeze(0))  # Single input\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-840070828ba9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Call your training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# End the timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-e74ed5448420>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0;31m# Use vmap for batch gradient computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                     \u001b[0mper_sample_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mlast_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_sample_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"3.weight\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Replace with your actual parameter name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         return vmap_impl(\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# If chunk_size is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     return _flat_vmap(\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         )\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mbatched_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unwrap_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meager_transforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margnums_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_and_value_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_saved_tensors_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0;31m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             flat_grad_input = _autograd_grad(\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0mflat_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_diff_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     grad_inputs = torch.autograd.grad(\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mdiff_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 139.12 MiB is free. Process 17156 has 15.75 GiB memory in use. Of the allocated memory 15.41 GiB is allocated by PyTorch, and 45.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ],
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 120.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 139.12 MiB is free. Process 17156 has 15.75 GiB memory in use. Of the allocated memory 15.41 GiB is allocated by PyTorch, and 45.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the saved results"
      ],
      "metadata": {
        "id": "Y5rDBd4r5rqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved results for plotting/printing\n",
        "\n",
        "results_dir = \"/kaggle/working/results/mnist_online\"\n",
        "filename = f\"mnist_{args.label}.pt\"\n",
        "results_filename = os.path.join(results_dir, filename)\n",
        "\n",
        "filename_plot_acc = f\"mnist_{args.label}_acc.png\"\n",
        "results_filename_plot_acc = os.path.join(results_dir, filename_plot_acc)\n",
        "\n",
        "filename_plot_loss = f\"mnist_{args.label}_loss.png\"\n",
        "results_filename_plot_loss = os.path.join(results_dir, filename_plot_loss)\n",
        "\n",
        "\n",
        "\n",
        "results = torch.load(results_filename, weights_only=True)\n",
        "\n",
        "\n",
        "# Extract data from results\n",
        "its = results[\"its\"]  # Optimization steps\n",
        "train_acc = results[\"train_acc\"]  # Training accuracy\n",
        "val_acc = results[\"val_acc\"]  # Validation accuracy\n",
        "train_loss = results[\"train_loss\"]  # Training loss\n",
        "val_loss = results[\"val_loss\"]  # Validation loss\n",
        "steps_to_reach = results[\"steps_to_reach\"]  # Steps to reach 90% validation accuracy\n",
        "\n",
        "print(f\"Steps needed to reach 0.9 validation accuracy: {steps_to_reach}\")\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure()\n",
        "plt.plot(its, train_acc, label=\"train\")\n",
        "plt.plot(its, val_acc, label=\"val\")\n",
        "plt.legend()\n",
        "plt.title(f\"Accuracy\")\n",
        "plt.xlabel(\"Optimization Steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xscale(\"log\", base=10)\n",
        "plt.grid()\n",
        "plt.savefig(results_filename_plot_acc, dpi=150)\n",
        "# plt.close()\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure()\n",
        "plt.plot(its, train_loss, label=\"train\")\n",
        "plt.plot(its, val_loss, label=\"val\")\n",
        "plt.legend()\n",
        "plt.title(f\"Loss\")\n",
        "plt.xlabel(\"Optimization Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xscale(\"log\", base=10)\n",
        "plt.grid()\n",
        "plt.savefig(results_filename_plot_loss, dpi=150)\n",
        "# plt.close()\n",
        "\n",
        "print(\"Plots loaded successfully.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T10:15:38.295777Z",
          "iopub.execute_input": "2025-01-30T10:15:38.296085Z",
          "iopub.status.idle": "2025-01-30T10:15:38.31624Z",
          "shell.execute_reply.started": "2025-01-30T10:15:38.29606Z",
          "shell.execute_reply": "2025-01-30T10:15:38.314859Z"
        },
        "id": "C8oMdf7M5rqx",
        "outputId": "8d9f42e4-4fbb-4777-ab29-a5b0d5536379"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-6c723b1834e5>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/results/mnist_online/mnist_high_freq_True_top_k_0.1_top_k_prob_0.9_upd_freq_1.pt'"
          ],
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/working/results/mnist_online/mnist_high_freq_True_top_k_0.1_top_k_prob_0.9_upd_freq_1.pt'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    }
  ]
}