{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10611406,"sourceType":"datasetVersion","datasetId":6569340}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"authorship_tag":"ABX9TyMSJa3XxLl7rvVV5eF0x78J","gpuType":"T4","provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"064bec9d457349bb9fa6a559339e9263":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9bf43e30b5a499a8de4604dc3f10ee2","IPY_MODEL_17166645b0b14019a9f5fe3d70b6caa5","IPY_MODEL_e451f4ad11bc44f8ab881eed658a8ee3"],"layout":"IPY_MODEL_ceb7ec9cff0049c098ac4b5fc5b6bd3c"}},"07c3736f38804f78bea83bb64c695cac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ff27ff9889744e6af586debee44669c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"17166645b0b14019a9f5fe3d70b6caa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1776d91874314cd0ab3df42e1b77bd5d","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_915f46f7e6694bf0b14a91ced2256596","value":100}},"1776d91874314cd0ab3df42e1b77bd5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c00be62ed5f48868daac8e39944ba63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_740bfc3868e947e68c87f28690b96945","placeholder":"​","style":"IPY_MODEL_73b20e21e4394b6c9326c75594d6f062","value":"Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"}},"30e1fcaaa18c40fcbb2182e33df28ea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c0b6a5234ea48e3bf8fa9fb04e7442d","placeholder":"​","style":"IPY_MODEL_ccfc9ef4719e4aef9379f11160003ce1","value":" 100/100 [04:53&lt;00:00,  3.31s/it]"}},"328beb9c29ed4dc094b6e476734a725f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36d9e94b822a4a5bb069cd474608bdb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37733b27dcd64ffaafd07959bc847f26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a3e587d4127443eb3bbae84621d4d06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eb47dc8d09747718044ca73cfadc307":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_936aa9172b2d4e0aa2f0cb84c0dabfad","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07c3736f38804f78bea83bb64c695cac","value":100}},"5c0b6a5234ea48e3bf8fa9fb04e7442d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b20e21e4394b6c9326c75594d6f062":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"740bfc3868e947e68c87f28690b96945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"915f46f7e6694bf0b14a91ced2256596":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"936aa9172b2d4e0aa2f0cb84c0dabfad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9bf43e30b5a499a8de4604dc3f10ee2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a3e587d4127443eb3bbae84621d4d06","placeholder":"​","style":"IPY_MODEL_37733b27dcd64ffaafd07959bc847f26","value":"Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"}},"ccfc9ef4719e4aef9379f11160003ce1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceb7ec9cff0049c098ac4b5fc5b6bd3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e451f4ad11bc44f8ab881eed658a8ee3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_328beb9c29ed4dc094b6e476734a725f","placeholder":"​","style":"IPY_MODEL_36d9e94b822a4a5bb069cd474608bdb6","value":" 100/100 [04:51&lt;00:00,  3.11s/it]"}},"fe873b542e0243ebbf591f5bc21ccd2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c00be62ed5f48868daac8e39944ba63","IPY_MODEL_4eb47dc8d09747718044ca73cfadc307","IPY_MODEL_30e1fcaaa18c40fcbb2182e33df28ea1"],"layout":"IPY_MODEL_0ff27ff9889744e6af586debee44669c"}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Idea 4: Samples fitering - online\n\n## Περιγραφή του αλγορίθμου\n- Γίνεται εκπαίδευση με grokfast - EMA. Όταν **appl_sampl_filter** is False έχω μόνο αυτό, ενώ για True εφαρμόζω επιπλέον και την ιδέα 4 για πιο έξυπνη επιλογή δειγμάτων.\n- Ο Dataloader έχει έναν custom sampler (WeightedRandomSampler) ο οποίος κάθε φορά διαλέγει ένα δείγμα με βάση κάποιο βάρος/πιθανότητα.\n- Στην αρχή τα βάρη είναι όλα ίδια (ομοιόμορφη κατανομή) οπότε ο Dataloader λειτουργεί όπως συνήθως διαλέγοντας τυχαία ένα sample.\n- Σε κάθε επανάληψη φτιάχνεται ένα ranking των δειγμάτων (με βάση του πόσο high frequency περιέχει το καθένα) το οποίο χρησιμοποιείται για να αποφασιστεί τι βάρος/πιθανότητα θα δοθεί σε κάθε δείγμα να επιλεγεί για εκπαίδευση. Το διάνυσμα βαρών/πιθανοτήτων ανανεώνεται κάθε **sampling_distr_upd_freq** επαναλήψεις.\n- Στην κατασκευή του διανύσματος βαρών από την συνολική πιθανότητα 1 δίνουμε στα **top_k** δείγματα συνολικά **top_k_sampling_prob** (και στα υπόλοιπα length(dataset) - **top_k** δείγματα δίνουμε συνολικά το υπόλοιπο 1 - **top_k_sampling_prob**).\n- Με **high_freq_better** is True ακολουθούμε την αρχική μας υπόθεση ότι τα δείγματα με high frequency είναι αυτά που θα πρέπει να ταΐσουμε το δίκτυο περισσότερο για να μάθει γρηγορότερα, για False γίνεται το αντίθετο.\n\n## Οδηγίες χρήσης για τρέξιμο\nΠήγαινε στον τίτλο **Execute training (by running main funciton)**. Πήγαινε στο parser.parse_args και όρισε τις τιμές που θες να δοκιμάσεις για grid search. Οι υπερπαράμετροι που σχετίζονται με την ιδέα 4 online είναι:\n\n- **top_k**\n- **top_k_sampling_prob**\n- **high_freq_better**\n- **sampling_distr_upd_freq**: Μάλλον είναι οκ στο 1 γιατί ακόμα και έτσι η εκπαίδευση δεν είναι αργή οπότε δεν έχω λόγο να το αυξήσω.\n\nΑν κάποιος θέλει να τρέξει κάποιες τιμές για το grid search, έχω βάλει στον φάκελο και ένα αρχείο για να σημειώνουμε τις τιμές των υπερπαραμέτρων που δοκίμασε ο καθένας για να μην τρέχουμε όλοι τα ίδια. Βάλτε GPU P100 (νομίζω είναι ελαφρώς καλύτερη), εμένα για τα 100.000 βήματα που έχω βάλει να είναι το default ένα τρέξιμο που κάνω μόνο με grokfast (δηλαδή **appl_sampl_filter** is False) παίρνει περίπου **7 λεπτά** οπότε καλά είμαστε από χρόνο.\n\n","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Maybe this is needed if you want to import private datasets \n# kagglehub.login()\n","metadata":{"executionInfo":{"elapsed":24569,"status":"ok","timestamp":1737367419894,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"f4s6HWPGPSBJ","outputId":"be8dc80d-eeb5-492c-f36e-15ba319dec89","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:37.578262Z","iopub.execute_input":"2025-02-13T17:37:37.578639Z","iopub.status.idle":"2025-02-13T17:37:39.641572Z","shell.execute_reply.started":"2025-02-13T17:37:37.578593Z","shell.execute_reply":"2025-02-13T17:37:39.640540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\n# hojjatk_mnist_dataset_path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n\n# The dataset was uploaded from me but I made it public so you too can probably load it with this line\n# _ = kagglehub.dataset_download(\"konstantinosbarkas/mnist-dataset-processed-from-local\")\n\n# print(\"Data source import complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:39.642641Z","iopub.execute_input":"2025-02-13T17:37:39.643182Z","iopub.status.idle":"2025-02-13T17:37:39.647658Z","shell.execute_reply.started":"2025-02-13T17:37:39.643147Z","shell.execute_reply":"2025-02-13T17:37:39.646523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport sys\n\nfor dirname, _, filenames in os.walk(\"/kaggle/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:39.648680Z","iopub.execute_input":"2025-02-13T17:37:39.648955Z","iopub.status.idle":"2025-02-13T17:37:39.687541Z","shell.execute_reply.started":"2025-02-13T17:37:39.648921Z","shell.execute_reply":"2025-02-13T17:37:39.686522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install -r /kaggle/input/enter-data-dn-req/requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:39.688519Z","iopub.execute_input":"2025-02-13T17:37:39.688879Z","iopub.status.idle":"2025-02-13T17:37:39.692969Z","shell.execute_reply.started":"2025-02-13T17:37:39.688845Z","shell.execute_reply":"2025-02-13T17:37:39.691891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install the Grokfast library\n!wget https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\n\nsys.path.append(\"/kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:39.694432Z","iopub.execute_input":"2025-02-13T17:37:39.694777Z","iopub.status.idle":"2025-02-13T17:37:40.014374Z","shell.execute_reply.started":"2025-02-13T17:37:39.694744Z","shell.execute_reply":"2025-02-13T17:37:40.013238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import argparse\n# import gzip\nimport math\nimport random\n# import struct\nimport time\nfrom argparse import ArgumentParser\n# from collections import Counter, defaultdict, deque\nfrom itertools import islice\n# from pathlib import Path\n# from typing import Dict, List, Literal, Optional\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\n# import torchvision.transforms as transforms\nfrom functorch import grad, vmap\nfrom sklearn.model_selection import train_test_split\nfrom torch.autograd import grad\n\n# from torch.nn.utils.stateless import functional_call, # This is deprecated, use the next one instead\nfrom torch.func import functional_call\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler, Dataset\nfrom tqdm.auto import tqdm\n\nfrom grokfast import gradfilter_ema,gradfilter_ma\nimport torchvision.transforms as transforms\n","metadata":{"executionInfo":{"elapsed":36273,"status":"ok","timestamp":1737367466050,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"QLUi9XZMRpId","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:40.017335Z","iopub.execute_input":"2025-02-13T17:37:40.017610Z","iopub.status.idle":"2025-02-13T17:37:47.200354Z","shell.execute_reply.started":"2025-02-13T17:37:40.017578Z","shell.execute_reply":"2025-02-13T17:37:47.199496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import vmap  # ✅ Use the new built-in vmap\nfrom torch.func import functional_call  # ✅ Use the new built-in functional_call\nfrom torch.func import grad ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.201699Z","iopub.execute_input":"2025-02-13T17:37:47.202161Z","iopub.status.idle":"2025-02-13T17:37:47.206709Z","shell.execute_reply.started":"2025-02-13T17:37:47.202136Z","shell.execute_reply":"2025-02-13T17:37:47.205437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nresults_dir = \"/kaggle/working/results/mnist_online\"\nos.makedirs(results_dir, exist_ok=True)\ndataset_path = \"/kaggle/input/MNIST_data_processed_from_local/\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.208038Z","iopub.execute_input":"2025-02-13T17:37:47.208419Z","iopub.status.idle":"2025-02-13T17:37:47.228145Z","shell.execute_reply.started":"2025-02-13T17:37:47.208396Z","shell.execute_reply":"2025-02-13T17:37:47.227117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer_dict = {\"AdamW\": torch.optim.AdamW, \"Adam\": torch.optim.Adam, \"SGD\": torch.optim.SGD}\n\nactivation_dict = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"Sigmoid\": nn.Sigmoid, \"GELU\": nn.GELU}\n\nloss_function_dict = {\"MSE\": nn.MSELoss, \"CrossEntropy\": nn.CrossEntropyLoss}\n","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1737367466051,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"8hhZpMIuSC4q","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.229313Z","iopub.execute_input":"2025-02-13T17:37:47.229688Z","iopub.status.idle":"2025-02-13T17:37:47.245715Z","shell.execute_reply.started":"2025-02-13T17:37:47.229649Z","shell.execute_reply":"2025-02-13T17:37:47.244849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cycle(iterable):\n    while True:\n        for x in iterable:\n            yield x\n","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1737367466051,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"RIifrNowR89e","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.246674Z","iopub.execute_input":"2025-02-13T17:37:47.246942Z","iopub.status.idle":"2025-02-13T17:37:47.261170Z","shell.execute_reply.started":"2025-02-13T17:37:47.246906Z","shell.execute_reply":"2025-02-13T17:37:47.260334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_collate_fn_2(batch):\n    \"\"\"Custom collate function to handle extra fields in the dataset.\"\"\"\n    images, labels, _, _ = zip(*batch)  # Ignore the indices and extra_fields for loss computation\n    images = torch.stack(images)  # Stack images into a single tensor\n    labels = torch.tensor(labels)  # Convert labels to a tensor\n    return images, labels\n\n\n\ndef compute_loss(network, dataset, loss_function, device, N=2000, batch_size=50):\n    \"\"\"Computes mean loss of `network` on `dataset`.\n    \"\"\"\n    with torch.no_grad():\n        N = min(len(dataset), N)\n        batch_size = min(batch_size, N)\n        #dataset_loader =DataLoader(dataset, batch_size=256, shuffle=False)\n        dataset_loader =DataLoader(dataset, batch_size=256, shuffle=False, collate_fn=custom_collate_fn_2)\n        \n        loss_fn = loss_function_dict[loss_function](reduction='sum')\n        one_hots = torch.eye(10, 10).to(device)\n        total = 0\n        points = 0\n        \n        for x, labels in islice(dataset_loader, N // batch_size):\n            y = network(x.to(device))\n            if loss_function == 'CrossEntropy':\n                total += loss_fn(y, labels.to(device)).item()\n            elif loss_function == 'MSE':\n                total += loss_fn(y, one_hots[labels]).item()\n            points += len(labels)\n        return total / points\n\n\n\ndef compute_accuracy(model, dataset, device, N=2000,batch_size=50):\n    \"\"\"Utility to compute accuracy on a given dataset.\"\"\"\n    correct = 0\n    total = 0\n\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn_2)\n    #loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    N = min(len(dataset), N)\n    batch_size = min(batch_size, N)\n    \n    for x, y in loader:  # Unpack index and extra_fields as well\n        x, y = x.to(device), y.to(device)\n        with torch.no_grad():\n            outputs = model(x)\n            predictions = outputs.argmax(dim=1)\n        correct += (predictions == y).sum().item()\n        total += y.size(0)\n        if N is not None and total >= N:\n            break\n    return correct / total\n\n    ","metadata":{"executionInfo":{"elapsed":212,"status":"ok","timestamp":1737367551771,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"zlciE-2nKkLg","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.262220Z","iopub.execute_input":"2025-02-13T17:37:47.262569Z","iopub.status.idle":"2025-02-13T17:37:47.277436Z","shell.execute_reply.started":"2025-02-13T17:37:47.262545Z","shell.execute_reply":"2025-02-13T17:37:47.276635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Added extra fields to keep ema_gra and history\n# In this implementation I use variance metric --> I don't also store deviation metric for memory efficiency\nclass MyMNIST(torch.utils.data.Dataset):\n    def __init__(self, dataset,targets, transform=None):\n        \"\"\"\n        Custom dataset to extend MNIST with extra fields.\n        \"\"\"\n        \n        self.data = dataset.data.float()\n        self.targets = targets\n        self.transform = transform\n\n        # Initialize extra fields\n        self.extra_fields = [\n            {\n                \"ema_grad\": 0.0,\n                \"num_updates\": 0,\n                \"variance_metric\": 0.0,\n            }\n            for _ in range(len(self.data))\n        ]\n\n    def __getitem__(self, index):\n        \"\"\"Returns a single data sample with extra fields.\"\"\"\n        img, target = self.data[index], self.targets[index]\n\n        # Ensure image is in [0,1] range\n        img = img / 255.0  # Normalize manually\n\n        # Apply any other transformations\n        if self.transform:\n            img = self.transform(img)\n\n        extra_field = self.extra_fields[index]\n        return img, target, int(index), extra_field\n        \n    def __len__(self):\n        return len(self.data)\n    def update_fields(self, indices, grad_stats, ema_alpha=0.9):\n        \"\"\"\n        Update the extra fields for specified dataset indices.\n        \"\"\"\n\n        for idx, grad in zip(indices, grad_stats):\n            # Update EMA\n            sample_field = self.extra_fields[idx]\n\n            current_ema = sample_field[\"ema_grad\"]\n            updated_ema = ema_alpha * current_ema + (1 - ema_alpha) * grad\n            sample_field[\"ema_grad\"] = updated_ema\n\n            deviation = abs(grad - current_ema)\n\n            num_updates = sample_field[\"num_updates\"] + 1  # Increment the update count\n            \n            current_avg_deviation = sample_field[\"variance_metric\"] ** 0.5\n            new_avg_deviation = ((current_avg_deviation * (num_updates - 1)) + deviation) / num_updates\n\n            sample_field[\"num_updates\"] = num_updates\n\n            # Variance estimate (for higher sensitivity to fast changes)\n            sample_field[\"variance_metric\"] = new_avg_deviation**2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.278382Z","iopub.execute_input":"2025-02-13T17:37:47.278734Z","iopub.status.idle":"2025-02-13T17:37:47.300821Z","shell.execute_reply.started":"2025-02-13T17:37:47.278709Z","shell.execute_reply":"2025-02-13T17:37:47.299918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef custom_collate_fn(batch):\n    images, labels, indices, extra_fields = zip(*batch)\n    images = torch.stack(images)  # Stack images into a single tensor\n    labels = torch.tensor(labels)  # Convert labels to a tensor\n    return images, labels, indices, extra_fields","metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1737367559166,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"zif6Q-IEjFJ7","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.301807Z","iopub.execute_input":"2025-02-13T17:37:47.302070Z","iopub.status.idle":"2025-02-13T17:37:47.319042Z","shell.execute_reply.started":"2025-02-13T17:37:47.302049Z","shell.execute_reply":"2025-02-13T17:37:47.318104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Needed for per sample gradient computations\ndef select_random_subset(tensor, percentage, seed=42):\n    \"\"\"\n    Flatten the parameter dimensions for each batch sample, select a percentage of elements,\n    and return a tensor with shape [batch_size, selected_elements].\n\n    Args:\n        tensor (torch.Tensor): The gradient tensor of shape [batch_size, *parameter_dims].\n        percentage (float): The percentage of elements to select.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        torch.Tensor: A tensor of shape [batch_size, selected_elements].\n    \"\"\"\n    batch_size, *param_dims = tensor.shape  # Extract batch and parameter dimensions\n    total_params = torch.prod(torch.tensor(param_dims))  # Total parameters per sample\n    subset_size = int(total_params * percentage)  # 20% of parameters\n\n    # Set seed for reproducibility\n    random.seed(seed)\n    indices = random.sample(range(total_params), subset_size)  # Random indices for selection\n\n    # Flatten parameter dimensions and select elements for each batch\n    flat_tensor = tensor.view(batch_size, -1)  # Flatten parameter dimensions for each sample\n    selected_subset = flat_tensor[:, indices]  # Select the same random indices across the batch\n\n    return selected_subset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.319904Z","iopub.execute_input":"2025-02-13T17:37:47.320185Z","iopub.status.idle":"2025-02-13T17:37:47.335413Z","shell.execute_reply.started":"2025-02-13T17:37:47.320162Z","shell.execute_reply":"2025-02-13T17:37:47.334420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Needed for online sample filtering\ndef rank_to_sampling_weights(my_dataset, top_k, top_k_sampling_prob, high_freq_better):\n    \"\"\"\n    Rank samples by variance_metric and assign sampling weights.\n\n    Parameters:\n    - my_dataset: MyMNIST object.\n    - top_k: Fraction of top samples to assign higher sampling probability.\n    - top_k_sampling_prob: Probability assigned to the top_k fraction of samples.\n\n    Returns:\n    - new_weights: List of sampling weights for each sample.\n    \"\"\"\n    # Calculate the number of top_k samples\n    num_samples = len(my_dataset)\n    top_k_count = int(top_k * num_samples)\n\n    # Sort indices by variance_metric in descending order\n    sorted_indices = sorted(\n        range(num_samples),\n        key=lambda idx: my_dataset.dataset.extra_fields[idx][\"variance_metric\"],\n        reverse=high_freq_better,\n    )\n\n    # Initialize new_weights with zeros\n    new_weights = [0.0] * num_samples\n\n    # Assign weights to the top_k samples\n    for idx in sorted_indices[:top_k_count]:\n        new_weights[idx] = top_k_sampling_prob / top_k_count\n\n    # Assign weights to the rest of the samples\n    for idx in sorted_indices[top_k_count:]:\n        new_weights[idx] = (1 - top_k_sampling_prob) / (num_samples - top_k_count)\n\n    return new_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.336234Z","iopub.execute_input":"2025-02-13T17:37:47.336575Z","iopub.status.idle":"2025-02-13T17:37:47.345151Z","shell.execute_reply.started":"2025-02-13T17:37:47.336542Z","shell.execute_reply":"2025-02-13T17:37:47.344260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## def main","metadata":{}},{"cell_type":"code","source":"import random\nfrom torch.func import vmap, grad, functional_call\n\ndef main(args):\n    log_freq = math.ceil(args.optimization_steps / 150)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dtype = torch.float32\n\n    torch.set_default_dtype(dtype)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n     # load dataset\n    transform = transforms.ToTensor()\n\n    train_dataset = torchvision.datasets.MNIST(root=args.download_directory, train=True,\n        transform=torchvision.transforms.ToTensor(), download=True)\n    \n\n    test = torchvision.datasets.MNIST(root=args.download_directory, train=False,\n        transform=torchvision.transforms.ToTensor(), download=True)\n\n    # Create indices stratified by digit labels\n    # train_indices = list(range(len(train_dataset)))\n    # train_labels = [train_dataset.targets[i].item() for i in train_indices]\n\n    transform = None\n\n    train_dataset = MyMNIST(train_dataset,train_dataset.targets,transform=transform)\n    test = MyMNIST(test,test.targets, transform=transform)\n\n\n    # # Use train_test_split with stratification to randomly select a specified number of samples (args.train_points)\n    # stratified_indices, _ = train_test_split(\n    #     train_indices,\n    #     train_size=args.train_points,\n    #     stratify=train_labels,\n    #     random_state=args.seed\n    # )\n\n    # Create a subset with the stratified indices\n    train_subset = torch.utils.data.Subset(train_dataset, range(args.train_points))\n\n    \n    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=args.batch_size, shuffle=True,collate_fn=custom_collate_fn_2)\n\n    # Create initial weights for uniform sampling\n    weights = [1.0] * len(train_subset)\n    sampler = WeightedRandomSampler(weights, len(weights))\n\n    train_loader = DataLoader(train_subset, batch_size=args.batch_size, sampler=sampler, collate_fn=custom_collate_fn)\n\n    data_iter = cycle(train_loader)\n    \n    activation_fn = activation_dict[args.activation]\n\n    #                                   Create model\n    #   -------------------------------------------------------------------------------   #\n\n    layers = [nn.Flatten()]\n    for i in range(args.depth):\n        if i == 0:\n            layers.append(nn.Linear(784, args.width))\n            layers.append(activation_fn())\n        elif i == args.depth - 1:\n            layers.append(nn.Linear(args.width, 10))\n        else:\n            layers.append(nn.Linear(args.width, args.width))\n            layers.append(activation_fn())\n    mlp = nn.Sequential(*layers).to(device)\n    with torch.no_grad():\n        for p in mlp.parameters():\n            p.data = args.initialization_scale * p.data\n    nparams = sum([p.numel() for p in mlp.parameters() if p.requires_grad])\n    print(f\"Number of parameters: {nparams}\")\n\n    # create optimizer\n    assert args.optimizer in optimizer_dict, f\"Unsupported optimizer choice: {args.optimizer}\"\n    optimizer = optimizer_dict[args.optimizer](mlp.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n\n    # define loss function\n    assert args.loss_function in loss_function_dict\n    loss_fn = loss_function_dict[args.loss_function]()\n\n    # Needed for per sample gradient computations\n    if args.appl_sampl_filter:\n        # Define a function for forward + loss computation\n        def compute_loss_vmap(params, buffers, model, x, y):\n            logits = functional_call(model, {**params, **buffers}, x.unsqueeze(0))\n            loss = loss_fn(logits, y.unsqueeze(0))  # Single output\n            return loss.mean()\n        \n        # Prepare model parameters and buffers\n        params_and_buffers = {**dict(mlp.named_parameters()), **dict(mlp.named_buffers())}\n        \n        params = {k: v for k, v in params_and_buffers.items() if v.requires_grad}\n        buffers = {k: v for k, v in params_and_buffers.items() if not v.requires_grad}\n        \n        # Create the gradient function\n        gradient_fn = grad(compute_loss_vmap)\n    \n        # Initialize EMA and metric history for each sample\n        gradient_ema = [0.0 for _ in range(len(train_subset))]\n        # gradient_metric_history = [[] for _ in range(len(train_subset))] # Probably unused\n\n    #                           Start Training below\n    #   -------------------------------------------------------------------------------   #\n    log_steps, train_losses, train_accuracies, test_losses, test_accuracies = [], [], [], [], []\n    one_hots = torch.eye(10, 10).to(device)\n\n    grads = None\n    \n    \n    with tqdm(total=args.optimization_steps, dynamic_ncols=True) as pbar:\n\n        reached_early_stop = False  # Flag to indicate early stopping\n        steps_to_reach_val_acc = None  # Variable to store steps for 0.95 validation accuracy\n\n        stable_threshold = 1000  # Number of steps the validation accuracy must remain > 0.9\n        stable_steps = 0  # Counter for steps validation accuracy remains above 0.9\n     \n        for step in range(args.optimization_steps):\n            if reached_early_stop: break\n            # Update the sampling distribution (according to the latest ranking of the samples)\n            if args.appl_sampl_filter and step % args.sampling_distr_upd_freq == 0 and step >= args.start:\n                # Rebuild the train_loader with new sampling weights, for example\n\n                weights = rank_to_sampling_weights(train_subset, args.top_k, args.top_k_sampling_prob, args.high_freq_better)\n                new_sampler = WeightedRandomSampler(weights, num_samples=len(train_subset), replacement=True)\n                del train_loader  \n                train_loader = DataLoader(\n                    train_subset,\n                    batch_size=args.batch_size,\n                    sampler=new_sampler,  # WeightedRandomSampler or anything you want\n                    collate_fn=custom_collate_fn\n                )\n                # Re-initialize the iterator to the new train_loader\n                del data_iter\n                data_iter = cycle(train_loader)\n                #data_iter = iter(train_loader)\n\n            # try:\n            x, labels, indices, _ = next(data_iter)\n            # except StopIteration:\n            #     #print(f\"Step {step}: Data iterator exhausted, resetting...\")\n            #     data_iter = iter(train_loader)  # Restart the iterator\n            #     x, labels, indices, _ = next(data_iter)\n\n            do_log = (step < 150 and step % 10 == 0) or step % log_freq == 0\n            \n            #do_log = (step < 30) or (step < 150 and step % 10 == 0) or step % log_freq == 0\n            if do_log:\n                with torch.no_grad():\n                    train_losses.append(compute_loss(mlp, train_subset, args.loss_function, device, N=len(train_subset)))\n                    train_accuracies.append(compute_accuracy(mlp, train_subset, device, N=len(train_subset)))\n                    test_losses.append(compute_loss(mlp, test, args.loss_function, device, N=len(test)))\n                    test_accuracies.append(compute_accuracy(mlp, test, device, N=len(test)))\n                    log_steps.append(step)\n                    pbar.set_description(\n                        \"Loss: {0:1.1e}|{1:1.1e}. Acc: {2:2.1f}%|{3:2.1f}%\".format(\n                            train_losses[-1],\n                            test_losses[-1],\n                            train_accuracies[-1] * 100,\n                            test_accuracies[-1] * 100,\n                        )\n                    )\n\n                                # Save results\n                    specific_result_dir = f\"mnist_{args.label}.pt\"\n                    results_filename = os.path.join(results_dir, specific_result_dir)\n                    torch.save(\n                        {\n                            \"its\": log_steps,\n                            \"train_acc\": train_accuracies,\n                            \"train_loss\": train_losses,\n                            \"val_acc\": test_accuracies,\n                            \"val_loss\": test_losses,\n                            \"steps_to_reach\": steps_to_reach_val_acc,\n                            \"model_state_dict\": mlp.state_dict(),  # Save the model's state dictionary, maybe unnecessary\n                        },\n                        results_filename,\n                    )\n                    \n\n\n            x, labels = x.to(device), labels.to(device)\n            y = mlp(x.to(device))\n            if args.loss_function == \"CrossEntropy\":\n                # Use integer labels for CrossEntropyLoss\n                loss = loss_fn(y, labels.to(device))\n            elif args.loss_function == \"MSE\":\n                loss = loss_fn(y, one_hots[labels])\n\n            optimizer.zero_grad()\n            \n            loss.backward() # Do I need create_graph?\n            #loss.backward(create_graph=True)\n\n            if args.appl_sampl_filter:  # Unnecessary if we are not applying sample filtering\n                # -----------------------------------------------------------------\n                #   Gradient Stats: Capture grads for each sample\n                # -----------------------------------------------------------------\n                # Identify the last two Linear layers dynamically\n                batch_gradients = []\n\n                with torch.no_grad():\n                    per_sample_grads = vmap(gradient_fn, in_dims=(None, None, None, 0, 0))(\n                        params, buffers, mlp, x, labels\n                    )  # ✅ Use torch.vmap\n\n                \n                    # Extract gradients for the target layers\n                    last_layer_grad = per_sample_grads[\"5.weight\"]  # Adjust key as needed\n                    second_last_layer_grad = per_sample_grads[\"3.weight\"]\n                \n                    # Select a subset of gradients\n                    percentage_s_l = 0.5\n                    percentage_l = 0.1\n                    selected_last = select_random_subset(last_layer_grad, percentage_l, seed=42)\n                    selected_second_last = select_random_subset(second_last_layer_grad, percentage_s_l, seed=42)\n                \n                    # Compute the average and detach\n                    selected_last_avg = selected_last.mean(dim=-1).detach().cpu()\n                    selected_second_last_avg = selected_second_last.mean(dim=-1).detach().cpu()\n                    total_avg = (selected_last_avg + selected_second_last_avg) / 2\n\n                train_subset.dataset.update_fields(indices, total_avg, args.ema_alpha_sampl_rank)\n\n            # -----------------------------------------------------------------\n            # -----------------------------------------------------------------\n\n            # Grokfast (EMA)\n            # Add required code for applying grokfast\n\n            if args.filter == \"none\":\n                pass\n            elif args.filter == \"ma\":\n                grads = gradfilter_ma(mlp, grads=grads, window_size=args.window_size, lamb=args.lamb)\n            elif args.filter == \"ema\":\n                grads = gradfilter_ema(mlp, grads=grads, alpha=args.alpha, lamb=args.lamb)\n            else:\n                raise ValueError(f\"Invalid gradient filter type `{args.filter}`\")\n        \n            optimizer.step()\n\n            test_acc = test_accuracies[-1] if len(test_accuracies) > 0 else 0\n\n            if test_acc >= 0.92 and steps_to_reach_val_acc is None:\n                steps_to_reach_val_acc = step\n\n            #Check for early stopping conditions\n            if test_acc > 0.9:\n                stable_steps += 1\n            else:\n                stable_steps = 0  # Reset counter if accuracy drops below 0.85\n\n            if stable_steps >= stable_threshold and test_acc >= 0.9 and steps_to_reach_val_acc is not None:\n                reached_early_stop = True\n                print(f\"Validation accuracy of 0.92 reached and remained > 0.9 for {stable_threshold} step at step {step}\")\n\n            \n            pbar.update(1)\n                \n\n    # Save results\n    specific_result_dir = f\"mnist_{args.label}.pt\"\n    results_filename = os.path.join(results_dir, specific_result_dir)\n    torch.save(\n        {\n            \"its\": log_steps,\n            \"train_acc\": train_accuracies,\n            \"train_loss\": train_losses,\n            \"val_acc\": test_accuracies,\n            \"val_loss\": test_losses,\n            \"steps_to_reach\": steps_to_reach_val_acc,\n            \"model_state_dict\": mlp.state_dict(),  # Save the model's state dictionary, maybe unnecessary\n        },\n        results_filename,\n    )\n\n    print(f\"\\nSteps needed to reach 0.9 validation accuracy: {steps_to_reach_val_acc}\")\n","metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1737370352760,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"},"user_tz":-120},"id":"LWPwaBWpSF52","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T19:55:47.754512Z","iopub.execute_input":"2025-02-13T19:55:47.754844Z","iopub.status.idle":"2025-02-13T19:55:50.874779Z","shell.execute_reply.started":"2025-02-13T19:55:47.754820Z","shell.execute_reply":"2025-02-13T19:55:50.874126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove the extra arguments passed by the Jupyter Notebook kernel\nsys.argv = [\"\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:37:47.374490Z","iopub.execute_input":"2025-02-13T17:37:47.374782Z","iopub.status.idle":"2025-02-13T17:37:47.393619Z","shell.execute_reply.started":"2025-02-13T17:37:47.374758Z","shell.execute_reply":"2025-02-13T17:37:47.392808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Functions for creating plot\nimport os\nimport torch\nimport matplotlib.pyplot as plt\n\ndef analyze_results(label, results_dir=\"/kaggle/working/results/mnist_online\"):\n    \"\"\"\n    Loads model results, extracts accuracy/loss data, and generates plots.\n    \n    Args:\n        label (str): Label identifier for the results file.\n        results_dir (str): Directory where results are stored.\n    \n    Returns:\n        None\n    \"\"\"\n\n    # Define file paths\n    filename = f\"mnist_{label}.pt\"\n    results_filename = os.path.join(results_dir, filename)\n\n    filename_plot_acc = f\"mnist_{label}_acc.png\"\n    results_filename_plot_acc = os.path.join(results_dir, filename_plot_acc)\n\n    filename_plot_loss = f\"mnist_{label}_loss.png\"\n    results_filename_plot_loss = os.path.join(results_dir, filename_plot_loss)\n\n    try:\n        # Load results\n        results = torch.load(results_filename, weights_only=True)\n\n        # Extract data\n        its = results[\"its\"]  # Optimization steps\n        train_acc = results[\"train_acc\"]  # Training accuracy\n        val_acc = results[\"val_acc\"]  # Validation accuracy\n        train_loss = results[\"train_loss\"]  # Training loss\n        val_loss = results[\"val_loss\"]  # Validation loss\n        steps_to_reach = results[\"steps_to_reach\"]  # Steps to reach 90% validation accuracy\n\n        print(f\"Steps needed to reach 0.9 validation accuracy: {steps_to_reach}\")\n\n        # Plot Accuracy\n        plt.figure()\n        plt.plot(its, train_acc, label=\"Train Accuracy\")\n        plt.plot(its, val_acc, label=\"Validation Accuracy\")\n\n        # Find and annotate the maximum validation accuracy\n        max_val_acc = max(val_acc)\n        max_val_idx = val_acc.index(max_val_acc)\n        plt.annotate(f\"Max Val Acc: {max_val_acc:.4f}\", \n                     (its[max_val_idx], max_val_acc), \n                     textcoords=\"offset points\", \n                     xytext=(0, 10), \n                     ha='center', \n                     fontsize=10, \n                     color='red')\n\n        plt.legend()\n        plt.title(f\"Accuracy - Grokfast:{args.filter} Sample Filtering:{args.appl_sampl_filter}\")\n        plt.xlabel(\"Optimization Steps\")\n        plt.ylabel(\"Accuracy\")\n        plt.xscale(\"log\", base=10)\n        plt.grid()\n        plt.figtext(0.5, -0.1, f\"Steps to reach val=0.9 = {steps_to_reach}\", \n            ha=\"center\", fontsize=10, style=\"italic\")\n       # plt.savefig(results_filename_plot_acc, dpi=150)\n        plt.show()\n        plt.close()\n\n        print(\"Plots saved successfully.\")\n\n    except FileNotFoundError:\n        print(f\"Error: Results file {results_filename} not found.\")\n    except Exception as e:\n        print(f\"Error while processing {label}: {e}\")\n\ndef plot_all_experiments_together(labels, results_dir=\"/kaggle/working/results/mnist_online\", show_only_val=False):\n    \"\"\"\n    Plots train and validation accuracy for multiple experiments in a single graph.\n    Allows showing only validation accuracy if `show_only_val=True`.\n\n    Args:\n        labels (list of str): List of labels corresponding to result files.\n        results_dir (str): Directory where results are stored.\n        show_only_val (bool): If True, only plots validation accuracy.\n\n    Returns:\n        None\n    \"\"\"\n    plt.figure(figsize=(10, 6))  # Set figure size\n\n    # Generate distinct colors for each experiment\n    base_colors = plt.cm.viridis(np.linspace(0, 1, len(labels)))  \n\n    for i, label in enumerate(labels):\n        results_filename = os.path.join(results_dir, f\"mnist_{label}.pt\")\n\n        try:\n            # Load results\n            results = torch.load(results_filename, weights_only=True)\n            its = results[\"its\"]\n            train_acc = results[\"train_acc\"]\n            val_acc = results[\"val_acc\"]\n\n            # Assign colors for train and validation curves\n            val_color = base_colors[i]  # Primary color for validation\n            train_color = tuple(c * 0.7 for c in base_colors[i])  # Slightly darker shade for train\n            steps_to_reach = results[\"steps_to_reach\"]\n            \n            # Plot validation accuracy (always shown)\n            plt.plot(its, val_acc, label=f\"Validation ({label})\", color=val_color, linestyle=\"solid\")\n            plt.figtext(0.5, -0.1, f\"Steps to reach val=0.9 = {steps_to_reach}\", \n                ha=\"center\", fontsize=10, style=\"italic\")\n        \n            # Plot train accuracy if `show_only_val` is False\n            if not show_only_val:\n                plt.plot(its, train_acc, label=f\"Train ({label})\", color=train_color, linestyle=\"dashed\")\n\n        except FileNotFoundError:\n            print(f\"Warning: Results file {results_filename} not found.\")\n        except Exception as e:\n            print(f\"Error while processing {label}: {e}\")\n\n    plt.legend()\n    plt.title(\"Train & Validation Accuracy for Multiple Experiments\" if not show_only_val else \"Validation Accuracy for Multiple Experiments\")\n    plt.xlabel(\"Optimization Steps\")\n    plt.ylabel(\"Accuracy\")\n    plt.xscale(\"log\", base=10)\n    plt.grid()\n    \n    # Save and show the plot\n    filename = \"combined_train_val_plot.png\" if not show_only_val else \"combined_val_plot.png\"\n    plot_path = os.path.join(results_dir, filename)\n    #plt.savefig(plot_path, dpi=150)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T16:39:13.368381Z","iopub.execute_input":"2025-02-08T16:39:13.368687Z","iopub.status.idle":"2025-02-08T16:39:13.381978Z","shell.execute_reply.started":"2025-02-08T16:39:13.368655Z","shell.execute_reply":"2025-02-08T16:39:13.381333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sampling Filtering - our best (top_k-=0.1, top_k_sampling_prob=0.9, high_freq_better=True, sampling_distr_upd_freq=50)","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Same as used in paper of Grokfast\n    parser = ArgumentParser(description=\"Train a model on MNIST without custom sampling\")\n    \n    parser.add_argument(\"--label\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    \n    parser.add_argument(\"--train_points\", type=int, default=1000)\n    parser.add_argument(\"--optimization_steps\", type=int, default=150000)\n    parser.add_argument(\"--batch_size\", type=int, default=200)\n    parser.add_argument(\"--loss_function\", type=str, default=\"MSE\") #MSE or CrossEntropy\n    parser.add_argument(\"--optimizer\", type=str, default=\"AdamW\")\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--lr\", type=float, default=1e-3)\n    parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n    parser.add_argument(\"--download_directory\", type=str, default=\".\")\n    parser.add_argument(\"--depth\", type=int, default=3)\n    parser.add_argument(\"--width\", type=int, default=200)\n    parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n    parser.add_argument(\"--start\", type=int, default=100)\n    \n\n    # Grokfast\n    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"ema\")\n    parser.add_argument(\"--alpha\", type=float, default=0.8)\n    parser.add_argument(\"--lamb\", type=float, default=0.1)\n\n    # Samples ranking\n    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n\n    # Boolean arguements need this due to bad behavior of parser.parse_args\n    def boolean_string(s):\n        if s not in {\"False\", \"True\"}:\n            raise ValueError(\"Not a valid boolean string\")\n        return s == \"True\"\n\n    # These are the hyperparameters related to our online sampling filtering algorithm\n    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n\n    # -----------------------------------------------------------------\n    # Try different hyperparameter values for your grid search here\n    # -----------------------------------------------------------------\n    args = parser.parse_args(\n        [\n            \"--appl_sampl_filter\", \"True\", # booleans as non strings in order to work\n            \"--sampling_distr_upd_freq\", \"50\", # the rest as strings for some reason\n            \"--top_k\", \"0.1\",\n            \"--top_k_sampling_prob\", \"0.9\",\n            \"--high_freq_better\", \"True\",\n        ]\n    )\n    # -----------------------------------------------------------------\n    # -----------------------------------------------------------------\n\n    # Create arg.label for the filename of the saved results\n    if not args.appl_sampl_filter:\n        args.label = f\"filter{args.filter}_sampling_{args.appl_sampl_filter}\"\n    else:\n        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n\n    # Training with time recording\n\n    # Start the timer\n    start_time = time.time()\n\n    # Call your training function\n    main(args)\n\n    # End the timer\n    end_time = time.time()\n\n    # Calculate elapsed time\n    elapsed_time = end_time - start_time\n\n    # Convert to minutes and seconds (optional)\n    minutes, seconds = divmod(elapsed_time, 60)\n\n    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n    print(f\"label:{args.label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:44:38.924852Z","iopub.execute_input":"2025-02-13T17:44:38.925176Z","execution_failed":"2025-02-13T18:05:51.836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Only Grokfast","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Same as used in paper of Grokfast\n    parser = ArgumentParser(description=\"Train a model on MNIST without custom sampling\")\n    \n    parser.add_argument(\"--label\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    \n    parser.add_argument(\"--train_points\", type=int, default=1000)\n    parser.add_argument(\"--optimization_steps\", type=int, default=110000)\n    parser.add_argument(\"--batch_size\", type=int, default=200)\n    parser.add_argument(\"--loss_function\", type=str, default=\"MSE\") #MSE or CrossEntropy\n    parser.add_argument(\"--optimizer\", type=str, default=\"AdamW\")\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--lr\", type=float, default=1e-3)\n    parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n    parser.add_argument(\"--download_directory\", type=str, default=\".\")\n    parser.add_argument(\"--depth\", type=int, default=3)\n    parser.add_argument(\"--width\", type=int, default=200)\n    parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n    parser.add_argument(\"--start\", type=int, default=100)\n    \n\n    # Grokfast\n    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"ema\")\n    parser.add_argument(\"--alpha\", type=float, default=0.8)\n    parser.add_argument(\"--lamb\", type=float, default=0.5)\n\n    # Samples ranking\n    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n\n    # Boolean arguements need this due to bad behavior of parser.parse_args\n    def boolean_string(s):\n        if s not in {\"False\", \"True\"}:\n            raise ValueError(\"Not a valid boolean string\")\n        return s == \"True\"\n\n    # These are the hyperparameters related to our online sampling filtering algorithm\n    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n\n    # -----------------------------------------------------------------\n    # Try different hyperparameter values for your grid search here\n    # -----------------------------------------------------------------\n    args = parser.parse_args(\n        [\n            \"--appl_sampl_filter\", \"False\", # booleans as non strings in order to work\n            \"--sampling_distr_upd_freq\", \"50\", # the rest as strings for some reason\n            \"--top_k\", \"0.1\",\n            \"--top_k_sampling_prob\", \"0.9\",\n            \"--high_freq_better\", \"True\",\n        ]\n    )\n    # -----------------------------------------------------------------\n    # -----------------------------------------------------------------\n\n    # Create arg.label for the filename of the saved results\n    if not args.appl_sampl_filter:\n        args.label = f\"filter{args.filter}_sampling_{args.appl_sampl_filter}\"\n    else:\n        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n\n    # Training with time recording\n\n    # Start the timer\n    start_time = time.time()\n\n    # Call your training function\n    main(args)\n\n    # End the timer\n    end_time = time.time()\n\n    # Calculate elapsed time\n    elapsed_time = end_time - start_time\n\n    # Convert to minutes and seconds (optional)\n    minutes, seconds = divmod(elapsed_time, 60)\n\n    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n    print(f\"label:{args.label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T16:12:50.804141Z","iopub.execute_input":"2025-02-07T16:12:50.804460Z","iopub.status.idle":"2025-02-07T16:15:06.562769Z","shell.execute_reply.started":"2025-02-07T16:12:50.804435Z","shell.execute_reply":"2025-02-07T16:15:06.561857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Grid Search - Sample complexity","metadata":{}},{"cell_type":"markdown","source":"Put your grid search values in param_grid","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom argparse import ArgumentParser\n\nif __name__ == \"__main__\":\n    # Same as used in the paper of Grokfast\n    parser = ArgumentParser(description=\"Train a model on MNIST without custom sampling\")\n    \n    parser.add_argument(\"--label\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    \n    parser.add_argument(\"--train_points\", type=int, default=1000)\n    parser.add_argument(\"--optimization_steps\", type=int, default=100000)\n    parser.add_argument(\"--batch_size\", type=int, default=200)\n    parser.add_argument(\"--loss_function\", type=str, default=\"MSE\") # MSE or CrossEntropy\n    parser.add_argument(\"--optimizer\", type=str, default=\"AdamW\")\n    parser.add_argument(\"--weight_decay\", type=float, default=2.0)\n    parser.add_argument(\"--lr\", type=float, default=1e-3)\n    parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n    parser.add_argument(\"--download_directory\", type=str, default=\".\")\n    parser.add_argument(\"--depth\", type=int, default=3)\n    parser.add_argument(\"--width\", type=int, default=200)\n    parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n    parser.add_argument(\"--start\", type=int, default=1000)\n    \n\n    # Grokfast\n    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"ema\")\n    parser.add_argument(\"--alpha\", type=float, default=0.8)\n    parser.add_argument(\"--lamb\", type=float, default=0.1)\n\n    # Samples ranking\n    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n\n    # Boolean arguements need this due to bad behavior of parser.parse_args\n    def boolean_string(s):\n        if s not in {\"False\", \"True\"}:\n            raise ValueError(\"Not a valid boolean string\")\n        return s == \"True\"\n\n    # These are the hyperparameters related to our online sampling filtering algorithm\n    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n\n\n    # Define possible values for each parameter\n    param_grid = {\n        \"top_k\": [0.3],  # Convert to float\n        \"top_k_sampling_prob\": [0.6 , 0.8],  # Convert to float\n        \"sampling_distr_upd_freq\": [50, 100],  # Convert to int\n        \"high_freq_better\": [True],  # Boolean parameter\n        \"filter\": [\"ema\", \"none\"],\n        \"appl_sampl_filter\": [True]  # Boolean parameter\n    }\n\n    # Generate all combinations of parameters\n    param_combinations = list(itertools.product(*param_grid.values()))\n    \n    # Run main in a loop for each combination\n    for param_values in param_combinations:\n        # Extract parameter values\n        top_k = param_values[0]\n        top_k_sampling_prob = param_values[1]\n        sampling_distr_upd_freq = param_values[2]\n        high_freq_better = param_values[3]  # Boolean value\n        filter = param_values[4]\n        appl_sampl_filter = param_values[5]  # Boolean value\n        \n        # Convert boolean values to strings\n        high_freq_better_str = str(high_freq_better)\n        appl_sampl_filter_str = str(appl_sampl_filter)\n        \n        # Create args dynamically\n        args_list = [\n            \"--appl_sampl_filter\", appl_sampl_filter_str,\n            \"--filter\", filter,\n            \"--top_k\", str(top_k),\n            \"--top_k_sampling_prob\", str(top_k_sampling_prob),\n            \"--sampling_distr_upd_freq\", str(sampling_distr_upd_freq),\n            \"--high_freq_better\", high_freq_better_str,\n            \"--label\", f\"top_{top_k}_prob_{top_k_sampling_prob}_update_{sampling_distr_upd_freq}_high_feq_{high_freq_better}_filter_{filter}_sampling_{appl_sampl_filter_str}\",\n            \"--ema_alpha_sampl_rank\", \"0.9\",\n            \"--start\", \"2000\"\n        ]\n\n        # ✅ Parse arguments correctly\n        args = parser.parse_args(args_list) \n        \n        print(f\"Label={args}\")\n        \n        print(f\"Label={args.label}\")\n        main(args)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}