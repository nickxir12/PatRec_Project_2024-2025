{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea 4: Samples fitering - online\n",
    "\n",
    "## Περιγραφή του αλγορίθμου\n",
    "- Γίνεται εκπαίδευση με grokfast - EMA. Όταν **appl_sampl_filter** is False έχω μόνο αυτό, ενώ για True εφαρμόζω επιπλέον και την ιδέα 4 για πιο έξυπνη επιλογή δειγμάτων.\n",
    "- Ο Dataloader έχει έναν custom sampler (WeightedRandomSampler) ο οποίος κάθε φορά διαλέγει ένα δείγμα με βάση κάποιο βάρος/πιθανότητα.\n",
    "- Στην αρχή τα βάρη είναι όλα ίδια (ομοιόμορφη κατανομή) οπότε ο Dataloader λειτουργεί όπως συνήθως διαλέγοντας τυχαία ένα sample.\n",
    "- Σε κάθε επανάληψη φτιάχνεται ένα ranking των δειγμάτων (με βάση του πόσο high frequency περιέχει το καθένα) το οποίο χρησιμοποιείται για να αποφασιστεί τι βάρος/πιθανότητα θα δοθεί σε κάθε δείγμα να επιλεγεί για εκπαίδευση. Το διάνυσμα βαρών/πιθανοτήτων ανανεώνεται κάθε **sampling_distr_upd_freq** επαναλήψεις.\n",
    "- Στην κατασκευή του διανύσματος βαρών από την συνολική πιθανότητα 1 δίνουμε στα **top_k** δείγματα συνολικά **top_k_sampling_prob** (και στα υπόλοιπα length(dataset) - **top_k** δείγματα δίνουμε συνολικά το υπόλοιπο 1 - **top_k_sampling_prob**).\n",
    "- Με **high_freq_better** is True ακολουθούμε την αρχική μας υπόθεση ότι τα δείγματα με high frequency είναι αυτά που θα πρέπει να ταΐσουμε το δίκτυο περισσότερο για να μάθει γρηγορότερα, για False γίνεται το αντίθετο.\n",
    "\n",
    "## Οδηγίες χρήσης για τρέξιμο\n",
    "Πήγαινε στον τίτλο **Execute training (by running main funciton)**. Πήγαινε στο parser.parse_args και όρισε τις τιμές που θες να δοκιμάσεις για grid search. Οι υπερπαράμετροι που σχετίζονται με την ιδέα 4 online είναι:\n",
    "\n",
    "- **top_k**\n",
    "- **top_k_sampling_prob**\n",
    "- **high_freq_better**\n",
    "- **sampling_distr_upd_freq**: Μάλλον είναι οκ στο 1 γιατί ακόμα και έτσι η εκπαίδευση δεν είναι αργή οπότε δεν έχω λόγο να το αυξήσω.\n",
    "\n",
    "Αν κάποιος θέλει να τρέξει κάποιες τιμές για το grid search, έχω βάλει στον φάκελο και ένα αρχείο για να σημειώνουμε τις τιμές των υπερπαραμέτρων που δοκίμασε ο καθένας για να μην τρέχουμε όλοι τα ίδια. Βάλτε GPU P100 (νομίζω είναι ελαφρώς καλύτερη), εμένα για τα 100.000 βήματα που έχω βάλει να είναι το default ένα τρέξιμο που κάνω μόνο με grokfast (δηλαδή **appl_sampl_filter** is False) παίρνει περίπου **7 λεπτά** οπότε καλά είμαστε από χρόνο.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-05T18:47:53.545586Z",
     "iopub.status.busy": "2025-02-05T18:47:53.545220Z",
     "iopub.status.idle": "2025-02-05T18:47:53.890966Z",
     "shell.execute_reply": "2025-02-05T18:47:53.890033Z",
     "shell.execute_reply.started": "2025-02-05T18:47:53.545553Z"
    },
    "executionInfo": {
     "elapsed": 24569,
     "status": "ok",
     "timestamp": 1737367419894,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "f4s6HWPGPSBJ",
    "outputId": "be8dc80d-eeb5-492c-f36e-15ba319dec89",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Maybe this is needed if you want to import private datasets\n",
    "# kagglehub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:47:53.892561Z",
     "iopub.status.busy": "2025-02-05T18:47:53.892229Z",
     "iopub.status.idle": "2025-02-05T18:47:54.300107Z",
     "shell.execute_reply": "2025-02-05T18:47:54.299189Z",
     "shell.execute_reply.started": "2025-02-05T18:47:53.892529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "# hojjatk_mnist_dataset_path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
    "\n",
    "# The dataset was uploaded from me but I made it public so you too can probably load it with this line\n",
    "_ = kagglehub.dataset_download(\"konstantinosbarkas/mnist-dataset-processed-from-local\")\n",
    "\n",
    "print(\"Data source import complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:47:54.302089Z",
     "iopub.status.busy": "2025-02-05T18:47:54.301877Z",
     "iopub.status.idle": "2025-02-05T18:47:54.314396Z",
     "shell.execute_reply": "2025-02-05T18:47:54.313412Z",
     "shell.execute_reply.started": "2025-02-05T18:47:54.302071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:47:54.315974Z",
     "iopub.status.busy": "2025-02-05T18:47:54.315687Z",
     "iopub.status.idle": "2025-02-05T18:47:54.322860Z",
     "shell.execute_reply": "2025-02-05T18:47:54.321996Z",
     "shell.execute_reply.started": "2025-02-05T18:47:54.315944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r /kaggle/input/enter-data-dn-req/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:47:54.323950Z",
     "iopub.status.busy": "2025-02-05T18:47:54.323625Z",
     "iopub.status.idle": "2025-02-05T18:47:54.662478Z",
     "shell.execute_reply": "2025-02-05T18:47:54.661303Z",
     "shell.execute_reply.started": "2025-02-05T18:47:54.323919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install the Grokfast library\n",
    "!wget https://raw.githubusercontent.com/ironjr/grokfast/main/grokfast.py\n",
    "\n",
    "sys.path.append(\"/kaggle/working\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:47:54.663977Z",
     "iopub.status.busy": "2025-02-05T18:47:54.663647Z",
     "iopub.status.idle": "2025-02-05T18:48:00.715357Z",
     "shell.execute_reply": "2025-02-05T18:48:00.714567Z",
     "shell.execute_reply.started": "2025-02-05T18:47:54.663943Z"
    },
    "executionInfo": {
     "elapsed": 36273,
     "status": "ok",
     "timestamp": 1737367466050,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "QLUi9XZMRpId",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import gzip\n",
    "import math\n",
    "import random\n",
    "# import struct\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "# from collections import Counter, defaultdict, deque\n",
    "from itertools import islice\n",
    "# from pathlib import Path\n",
    "# from typing import Dict, List, Literal, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "from functorch import grad, vmap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import grad\n",
    "\n",
    "# from torch.nn.utils.stateless import functional_call, # This is deprecated, use the next one instead\n",
    "from torch.func import functional_call\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from grokfast import gradfilter_ema,gradfilter_ma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.716521Z",
     "iopub.status.busy": "2025-02-05T18:48:00.716160Z",
     "iopub.status.idle": "2025-02-05T18:48:00.720230Z",
     "shell.execute_reply": "2025-02-05T18:48:00.719618Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.716499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_dir = \"/kaggle/working/results/mnist_online\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "dataset_path = \"/kaggle/input/MNIST_data_processed_from_local/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.722496Z",
     "iopub.status.busy": "2025-02-05T18:48:00.722258Z",
     "iopub.status.idle": "2025-02-05T18:48:00.736203Z",
     "shell.execute_reply": "2025-02-05T18:48:00.735500Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.722476Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1737367466051,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "8hhZpMIuSC4q",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer_dict = {\"AdamW\": torch.optim.AdamW, \"Adam\": torch.optim.Adam, \"SGD\": torch.optim.SGD}\n",
    "\n",
    "activation_dict = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"Sigmoid\": nn.Sigmoid, \"GELU\": nn.GELU}\n",
    "\n",
    "loss_function_dict = {\"MSE\": nn.MSELoss, \"CrossEntropy\": nn.CrossEntropyLoss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.737695Z",
     "iopub.status.busy": "2025-02-05T18:48:00.737441Z",
     "iopub.status.idle": "2025-02-05T18:48:00.749823Z",
     "shell.execute_reply": "2025-02-05T18:48:00.749173Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.737676Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1737367466051,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "RIifrNowR89e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.750688Z",
     "iopub.status.busy": "2025-02-05T18:48:00.750481Z",
     "iopub.status.idle": "2025-02-05T18:48:00.761204Z",
     "shell.execute_reply": "2025-02-05T18:48:00.760541Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.750669Z"
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1737367551771,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "zlciE-2nKkLg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn_2(batch):\n",
    "    \"\"\"Custom collate function to handle extra fields in the dataset.\"\"\"\n",
    "    images, labels, _, _ = zip(*batch)  # Ignore the indices and extra_fields for loss computation\n",
    "    images = torch.stack(images)  # Stack images into a single tensor\n",
    "    labels = torch.tensor(labels)  # Convert labels to a tensor\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(network, dataset, loss_function, device, N=2000, batch_size=50):\n",
    "    \"\"\"Computes mean loss of `network` on `dataset`.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        N = min(len(dataset), N)\n",
    "        batch_size = min(batch_size, N)\n",
    "        dataset_loader =DataLoader(dataset, batch_size=256, shuffle=False, collate_fn=custom_collate_fn_2)\n",
    "        loss_fn = loss_function_dict[loss_function](reduction='sum')\n",
    "        one_hots = torch.eye(10, 10).to(device)\n",
    "        total = 0\n",
    "        points = 0\n",
    "\n",
    "        for x, labels in islice(dataset_loader, N // batch_size):\n",
    "            y = network(x.to(device))\n",
    "            if loss_function == 'CrossEntropy':\n",
    "                total += loss_fn(y, labels.to(device)).item()\n",
    "            elif loss_function == 'MSE':\n",
    "                total += loss_fn(y, one_hots[labels]).item()\n",
    "            points += len(labels)\n",
    "        return total / points\n",
    "\n",
    "\n",
    "\n",
    "def compute_accuracy(model, dataset, device, N=None):\n",
    "    \"\"\"Utility to compute accuracy on a given dataset.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False, collate_fn=custom_collate_fn_2)\n",
    "\n",
    "    for x, y in loader:  # Unpack index and extra_fields as well\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "        correct += (predictions == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        if N is not None and total >= N:\n",
    "            break\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# def compute_loss(model, dataset, loss_function_name, device, N=None):\n",
    "#     \"\"\"Utility to compute the average loss on a given dataset.\"\"\"\n",
    "#     loss_fn = loss_function_dict[loss_function_name]()\n",
    "\n",
    "#     loader = DataLoader(dataset, batch_size=256, shuffle=False, collate_fn=custom_collate_fn_2)\n",
    "#     total_loss = 0.0\n",
    "#     count = 0\n",
    "#     one_hots = torch.eye(10, device=device)\n",
    "#     for x, y in loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(x)\n",
    "#             if loss_function_name == \"CrossEntropy\":\n",
    "#                 loss = loss_fn(outputs, y)\n",
    "#             elif loss_function_name == \"MSE\":\n",
    "#                 loss = loss_fn(outputs, one_hots[y])\n",
    "#         batch_size = x.size(0)\n",
    "#         total_loss += loss.item() * batch_size\n",
    "#         count += batch_size\n",
    "#         if N is not None and count >= N:\n",
    "#             break\n",
    "#     return total_loss / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.762191Z",
     "iopub.status.busy": "2025-02-05T18:48:00.761944Z",
     "iopub.status.idle": "2025-02-05T18:48:00.780361Z",
     "shell.execute_reply": "2025-02-05T18:48:00.779576Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.762160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Added extra fields to keep ema_gra and history\n",
    "# In this implementation I use variance metric --> I don't also store deviation metric for memory efficiency\n",
    "class MyMNIST(torchvision.datasets.MNIST):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset to simulate MyMNIST behavior with extra fields.\n",
    "        Args:\n",
    "            data: Tensor of shape [N, 28, 28].\n",
    "            targets: Tensor of shape [N].\n",
    "            transform: Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.data = data.to(torch.float32)\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "        # Initialize extra fields\n",
    "        self.extra_fields = [\n",
    "            {\n",
    "                \"ema_grad\": 0.0,  # EMA of gradient\n",
    "                \"num_updates\": 0,\n",
    "                # \"deviation_metric\": 0.0, # Deviation metric\n",
    "                \"variance_metric\": 0.0,  # Variance metric\n",
    "            }\n",
    "            for _ in range(len(self.data))\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a single data sample and its associated extra fields.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        extra_field = self.extra_fields[index]\n",
    "        return img, target, index, extra_field\n",
    "\n",
    "    def update_fields(self, indices, grad_stats, ema_alpha=0.9):\n",
    "        \"\"\"\n",
    "        Update the extra fields for specified dataset indices.\n",
    "        \"\"\"\n",
    "\n",
    "        for idx, grad in zip(indices, grad_stats):\n",
    "            # Update EMA\n",
    "            sample_field = self.extra_fields[idx]\n",
    "\n",
    "            current_ema = sample_field[\"ema_grad\"]\n",
    "            updated_ema = ema_alpha * current_ema + (1 - ema_alpha) * grad\n",
    "            sample_field[\"ema_grad\"] = updated_ema\n",
    "\n",
    "            deviation = abs(grad - updated_ema)\n",
    "\n",
    "            num_updates = sample_field[\"num_updates\"] + 1  # Increment the update count\n",
    "            # current_avg_deviation = sample_field[\"deviation_metric\"]\n",
    "            current_avg_deviation = sample_field[\"variance_metric\"] ** 0.5\n",
    "            new_avg_deviation = ((current_avg_deviation * (num_updates - 1)) + deviation) / num_updates\n",
    "\n",
    "            # Update the deviation metric and number of updates\n",
    "            #sample_field[\"deviation_metric\"] = new_avg_deviation\n",
    "            sample_field[\"num_updates\"] = num_updates\n",
    "\n",
    "            # sample_field[\"deviation_metric\"] += deviation\n",
    "\n",
    "            # Optionally update variance-like metric\n",
    "            # Variance estimate (for higher sensitivity to fast changes)\n",
    "            sample_field[\"variance_metric\"] = new_avg_deviation**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.781344Z",
     "iopub.status.busy": "2025-02-05T18:48:00.781110Z",
     "iopub.status.idle": "2025-02-05T18:48:00.798068Z",
     "shell.execute_reply": "2025-02-05T18:48:00.797433Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.781324Z"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1737367559166,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "zif6Q-IEjFJ7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    images, labels, indices, extra_fields = zip(*batch)\n",
    "    images = torch.stack(images)  # Stack images into a single tensor\n",
    "    labels = torch.tensor(labels)  # Convert labels to a tensor\n",
    "    return images, labels, indices, extra_fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.799186Z",
     "iopub.status.busy": "2025-02-05T18:48:00.798972Z",
     "iopub.status.idle": "2025-02-05T18:48:00.812794Z",
     "shell.execute_reply": "2025-02-05T18:48:00.812175Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.799168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Needed for per sample gradient computations\n",
    "def select_random_subset(tensor, percentage):\n",
    "    \"\"\"\n",
    "    Flatten the parameter dimensions for each batch sample, select a percentage of elements,\n",
    "    and return a tensor with shape [batch_size, selected_elements].\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The gradient tensor of shape [batch_size, *parameter_dims].\n",
    "        percentage (float): The percentage of elements to select.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape [batch_size, selected_elements].\n",
    "    \"\"\"\n",
    "    batch_size, *param_dims = tensor.shape  # Extract batch and parameter dimensions\n",
    "    total_params = torch.prod(torch.tensor(param_dims))  # Total parameters per sample\n",
    "    subset_size = int(total_params * percentage)  # 20% of parameters\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(args.seed)\n",
    "    indices = random.sample(range(total_params), subset_size)  # Random indices for selection\n",
    "\n",
    "    # Flatten parameter dimensions and select elements for each batch\n",
    "    flat_tensor = tensor.view(batch_size, -1)  # Flatten parameter dimensions for each sample\n",
    "    selected_subset = flat_tensor[:, indices]  # Select the same random indices across the batch\n",
    "\n",
    "    return selected_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.813953Z",
     "iopub.status.busy": "2025-02-05T18:48:00.813717Z",
     "iopub.status.idle": "2025-02-05T18:48:00.830145Z",
     "shell.execute_reply": "2025-02-05T18:48:00.829313Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.813933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Needed for online sample filtering\n",
    "def rank_to_sampling_weights(my_dataset, top_k, top_k_sampling_prob, high_freq_better):\n",
    "    \"\"\"\n",
    "    Rank samples by variance_metric and assign sampling weights.\n",
    "\n",
    "    Parameters:\n",
    "    - my_dataset: MyMNIST object.\n",
    "    - top_k: Fraction of top samples to assign higher sampling probability.\n",
    "    - top_k_sampling_prob: Probability assigned to the top_k fraction of samples.\n",
    "\n",
    "    Returns:\n",
    "    - new_weights: List of sampling weights for each sample.\n",
    "    \"\"\"\n",
    "    # Calculate the number of top_k samples\n",
    "    num_samples = len(my_dataset)\n",
    "    top_k_count = int(top_k * num_samples)\n",
    "\n",
    "    # Sort indices by variance_metric in descending order\n",
    "    sorted_indices = sorted(\n",
    "        range(num_samples),\n",
    "        key=lambda idx: my_dataset.dataset.extra_fields[idx][\"variance_metric\"],\n",
    "        reverse=high_freq_better,\n",
    "    )\n",
    "\n",
    "    # Initialize new_weights with zeros\n",
    "    new_weights = [0.0] * num_samples\n",
    "\n",
    "    # Assign weights to the top_k samples\n",
    "    for idx in sorted_indices[:top_k_count]:\n",
    "        new_weights[idx] = top_k_sampling_prob / top_k_count\n",
    "\n",
    "    # Assign weights to the rest of the samples\n",
    "    for idx in sorted_indices[top_k_count:]:\n",
    "        new_weights[idx] = (1 - top_k_sampling_prob) / (num_samples - top_k_count)\n",
    "\n",
    "    return new_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T21:37:34.423968Z",
     "iopub.status.idle": "2025-02-05T21:37:34.424216Z",
     "shell.execute_reply": "2025-02-05T21:37:34.424118Z"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1737370352760,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "LWPwaBWpSF52",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from functorch import vmap, grad\n",
    "from torch.nn.utils.stateless import functional_call\n",
    "\n",
    "def main(args):\n",
    "    log_freq = math.ceil(args.optimization_steps / 150)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "    one_hots = torch.eye(10, 10).to(device)\n",
    "\n",
    "    torch.set_default_dtype(dtype)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    #                                 Load dataset\n",
    "    #   -------------------------------------------------------------------------------   #\n",
    "    train_data = torch.load(f\"{dataset_path}/train.pt\", weights_only=True)\n",
    "    test_data = torch.load(f\"{dataset_path}/test.pt\", weights_only=True)\n",
    "\n",
    "    transform = None\n",
    "\n",
    "    train_images, train_labels = train_data\n",
    "    test_images, test_labels = test_data\n",
    "\n",
    "    # Create MyMNIST datasets\n",
    "    train_dataset = MyMNIST(train_images, train_labels, transform=transform)\n",
    "    test_dataset = MyMNIST(test_images, test_labels, transform=transform)\n",
    "    test = test_dataset  # For compatibility with our older code\n",
    "\n",
    "    # # Create indices & stratify\n",
    "    # train_indices = list(range(len(train_dataset)))\n",
    "    # train_labels = [train_dataset.targets[i].item() for i in train_indices]\n",
    "\n",
    "    # # Use train_test_split with stratification to randomly select a specified number of samples (args.train_points)\n",
    "    # stratified_indices, _ = train_test_split(\n",
    "    #     train_indices,\n",
    "    #     train_size=args.train_points,\n",
    "    #     stratify=train_labels,\n",
    "    #     random_state=args.seed,\n",
    "    # )\n",
    "\n",
    "    # train_subset = Subset(train_dataset, stratified_indices)\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, range(args.train_points))\n",
    "    # Create initial weights for uniform sampling\n",
    "    weights = [1.0] * len(train_subset)\n",
    "    sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=args.batch_size, sampler=sampler, collate_fn=custom_collate_fn)\n",
    "\n",
    "    activation_fn = activation_dict[args.activation]\n",
    "\n",
    "    #                                   Create model\n",
    "    #   -------------------------------------------------------------------------------   #\n",
    "\n",
    "    layers = [nn.Flatten()]\n",
    "    for i in range(args.depth):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(784, args.width))\n",
    "            layers.append(activation_fn())\n",
    "        elif i == args.depth - 1:\n",
    "            layers.append(nn.Linear(args.width, 10))\n",
    "        else:\n",
    "            layers.append(nn.Linear(args.width, args.width))\n",
    "            layers.append(activation_fn())\n",
    "    mlp = nn.Sequential(*layers).to(device)\n",
    "    with torch.no_grad():\n",
    "        for p in mlp.parameters():\n",
    "            p.data = args.initialization_scale * p.data\n",
    "    nparams = sum([p.numel() for p in mlp.parameters() if p.requires_grad])\n",
    "    print(f\"Number of parameters: {nparams}\")\n",
    "\n",
    "    # create optimizer\n",
    "    assert args.optimizer in optimizer_dict, f\"Unsupported optimizer choice: {args.optimizer}\"\n",
    "    optimizer = optimizer_dict[args.optimizer](mlp.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    # define loss function\n",
    "    assert args.loss_function in loss_function_dict\n",
    "    loss_fn = loss_function_dict[args.loss_function]()\n",
    "\n",
    "    # Needed for per sample gradient computations\n",
    "    if args.appl_sampl_filter:\n",
    "        # Define a function for forward + loss computation\n",
    "        def compute_loss_vmap(params, buffers, model, x, y):\n",
    "            # Use functional_call to pass parameters and buffers explicitly\n",
    "            logits = functional_call(model, {**params, **buffers}, x.unsqueeze(0))  # Single input\n",
    "            loss = loss_fn(logits, y.unsqueeze(0))  # Single output\n",
    "            return loss.mean()\n",
    "\n",
    "        # Prepare model parameters and buffers\n",
    "        params_and_buffers = {**dict(mlp.named_parameters()), **dict(mlp.named_buffers())}\n",
    "\n",
    "        params = {k: v for k, v in params_and_buffers.items() if v.requires_grad}\n",
    "        buffers = {k: v for k, v in params_and_buffers.items() if not v.requires_grad}\n",
    "\n",
    "\n",
    "        # Create the gradient function\n",
    "        gradient_fn = grad(compute_loss_vmap)\n",
    "\n",
    "        # Initialize EMA and metric history for each sample\n",
    "        gradient_ema = [0.0 for _ in range(len(train_subset))]\n",
    "        # gradient_metric_history = [[] for _ in range(len(train_subset))] # Probably unused\n",
    "\n",
    "    #                           Start Training below\n",
    "    #   -------------------------------------------------------------------------------   #\n",
    "    log_steps, train_losses, train_accuracies, test_losses, test_accuracies = [], [], [], [], []\n",
    "    one_hots = torch.eye(10, 10).to(device)\n",
    "\n",
    "    grads = None\n",
    "\n",
    "    with tqdm(total=args.optimization_steps, dynamic_ncols=True) as pbar:\n",
    "\n",
    "        reached_early_stop = False  # Flag to indicate early stopping\n",
    "        steps_to_reach_val_acc = None  # Variable to store steps for 0.95 validation accuracy\n",
    "\n",
    "        stable_threshold = 100  # Number of steps the validation accuracy must remain > 0.9\n",
    "        stable_steps = 0  # Counter for steps validation accuracy remains above 0.9\n",
    "\n",
    "        for step in range(args.optimization_steps):\n",
    "            if reached_early_stop: break\n",
    "            # Update the sampling distribution (according to the latest ranking of the samples)\n",
    "            if args.appl_sampl_filter:\n",
    "                if step % args.sampling_distr_upd_freq == 0 and step != 0:\n",
    "                    # Update the weights of the sampling based on the latest gradient metrics\n",
    "                    weights = rank_to_sampling_weights(train_subset, args.top_k, args.top_k_sampling_prob, args.high_freq_better)\n",
    "                    sampler = WeightedRandomSampler(weights, num_samples=len(weights))\n",
    "                    train_loader = DataLoader(\n",
    "                        train_subset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        sampler=sampler,\n",
    "                        collate_fn=custom_collate_fn,\n",
    "                    )\n",
    "\n",
    "            for batch in islice(cycle(train_loader),1):\n",
    "                x, labels, indices, _ = batch\n",
    "                do_log = (step < 30) or (step < 150 and step % 10 == 0) or step % log_freq == 0\n",
    "                if do_log:\n",
    "                    with torch.no_grad():\n",
    "                        train_losses.append(compute_loss(mlp, train_subset, args.loss_function, device, N=len(train_subset)))\n",
    "                        train_accuracies.append(compute_accuracy(mlp, train_subset, device, N=len(train_subset)))\n",
    "                        test_losses.append(compute_loss(mlp, test, args.loss_function, device, N=len(test)))\n",
    "                        test_accuracies.append(compute_accuracy(mlp, test, device, N=len(test)))\n",
    "                        log_steps.append(step)\n",
    "                        pbar.set_description(\n",
    "                            \"Loss: {0:1.1e}|{1:1.1e}. Acc: {2:2.1f}%|{3:2.1f}%\".format(\n",
    "                                train_losses[-1],\n",
    "                                test_losses[-1],\n",
    "                                train_accuracies[-1] * 100,\n",
    "                                test_accuracies[-1] * 100,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x, labels = x.to(device), labels.to(device)\n",
    "                y = mlp(x.to(device))\n",
    "                if args.loss_function == \"CrossEntropy\":\n",
    "                    # Use integer labels for CrossEntropyLoss\n",
    "                    loss = loss_fn(y, labels)\n",
    "                elif args.loss_function == \"MSE\":\n",
    "                    loss = loss_fn(y, one_hots[labels])\n",
    "\n",
    "                #loss.backward() # Do I need create_graph?\n",
    "                loss.backward(create_graph=True)\n",
    "\n",
    "                if args.appl_sampl_filter:  # Unnecessary if we are not applying sample filtering\n",
    "                    # -----------------------------------------------------------------\n",
    "                    #   Gradient Stats: Capture grads for each sample\n",
    "                    # -----------------------------------------------------------------\n",
    "                    # Identify the last two Linear layers dynamically\n",
    "                    batch_gradients = []\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        per_sample_grads = vmap(gradient_fn, in_dims=(None, None, None, 0, 0))(\n",
    "                            params, buffers, mlp, x, labels\n",
    "                        )\n",
    "\n",
    "                        # Extract gradients for the target layers\n",
    "                        last_layer_grad = per_sample_grads[\"3.weight\"]  # Adjust key as needed\n",
    "                        second_last_layer_grad = per_sample_grads[\"5.weight\"]\n",
    "\n",
    "                        # Select a subset of gradients\n",
    "                        percentage_s_l = 0.2\n",
    "                        percentage_l = 0.2\n",
    "                        selected_last = select_random_subset(last_layer_grad, percentage_l)\n",
    "                        selected_second_last = select_random_subset(second_last_layer_grad, percentage_s_l)\n",
    "\n",
    "                        # Compute the average and detach\n",
    "                        selected_last_avg = selected_last.mean(dim=-1).detach().cpu()\n",
    "                        selected_second_last_avg = selected_second_last.mean(dim=-1).detach().cpu()\n",
    "                        total_avg = (selected_last_avg + selected_second_last_avg) / 2\n",
    "\n",
    "                    train_subset.dataset.update_fields(indices, total_avg, args.ema_alpha_sampl_rank)\n",
    "\n",
    "                    # -----------------------------------------------------------------\n",
    "                    # -----------------------------------------------------------------\n",
    "\n",
    "                # Grokfast (EMA)\n",
    "                # Add required code for applying grokfast\n",
    "\n",
    "                if args.filter == \"none\":\n",
    "                    pass\n",
    "                elif args.filter == \"ma\":\n",
    "                    grads = gradfilter_ma(mlp, grads=grads, window_size=args.window_size, lamb=args.lamb)\n",
    "                elif args.filter == \"ema\":\n",
    "                    grads = gradfilter_ema(mlp, grads=grads, alpha=args.alpha, lamb=args.lamb)\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid gradient filter type `{args.filter}`\")\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                # Record the first step reaching 0.9 validation accuracy\n",
    "                test_acc = test_accuracies[-1] if len(test_accuracies) > 0 else 0\n",
    "\n",
    "                if test_acc >= 0.9 and steps_to_reach_val_acc is None:\n",
    "                    steps_to_reach_val_acc = step\n",
    "\n",
    "                # Check for early stopping conditions\n",
    "                # if test_acc > 0.85:\n",
    "                #     stable_steps += 1\n",
    "                # else:\n",
    "                #     stable_steps = 0  # Reset counter if accuracy drops below 0.85\n",
    "\n",
    "                # if stable_steps >= stable_threshold and test_acc >= 0.9:\n",
    "                #     reached_early_stop = True\n",
    "                #     print(f\"Validation accuracy of 0.9 reached and remained > 0.85 for {stable_threshold} step at step {step}\")\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    specific_result_dir = f\"mnist_{args.label}.pt\"\n",
    "    results_filename = os.path.join(results_dir, specific_result_dir)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"its\": log_steps,\n",
    "            \"train_acc\": train_accuracies,\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_acc\": test_accuracies,\n",
    "            \"val_loss\": test_losses,\n",
    "            \"steps_to_reach\": steps_to_reach_val_acc,\n",
    "            \"model_state_dict\": mlp.state_dict(),  # Save the model's state dictionary, maybe unnecessary\n",
    "        },\n",
    "        results_filename,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSteps needed to reach 0.9 validation accuracy: {steps_to_reach_val_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:48:00.871663Z",
     "iopub.status.busy": "2025-02-05T18:48:00.871358Z",
     "iopub.status.idle": "2025-02-05T18:48:00.884507Z",
     "shell.execute_reply": "2025-02-05T18:48:00.883841Z",
     "shell.execute_reply.started": "2025-02-05T18:48:00.871635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove the extra arguments passed by the Jupyter Notebook kernel\n",
    "sys.argv = [\"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training (by running main function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Όταν τρέχετε κάτι σώστε ίσως και τοπικά το .pt αρχείο να το έχουμε (επίσης προσοχή στα ονόματα των .pt κλπ είναι λγ μπάχαλο τρ το πως τα ονομάζω να γίνει πιο οργανωμένα κάποια στιγμή)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T15:26:18.302011Z",
     "iopub.status.busy": "2025-02-05T15:26:18.301539Z",
     "iopub.status.idle": "2025-02-05T15:33:18.348171Z",
     "shell.execute_reply": "2025-02-05T15:33:18.347238Z",
     "shell.execute_reply.started": "2025-02-05T15:26:18.301963Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737367489050,
     "user": {
      "displayName": "Nikolas Xiros",
      "userId": "00495869844002127525"
     },
     "user_tz": -120
    },
    "id": "MawvRmI9yt3T",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Same as used in paper of Grokfast\n",
    "    parser = ArgumentParser(description=\"Train a model on MNIST without custom sampling\")\n",
    "\n",
    "    parser.add_argument(\"--label\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "    parser.add_argument(\"--train_points\", type=int, default=1000)\n",
    "    parser.add_argument(\"--optimization_steps\", type=int, default=100000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=200)\n",
    "    parser.add_argument(\"--loss_function\", type=str, default=\"MSE\") #MSE or CrossEntropy\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"AdamW\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=2.00)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--initialization_scale\", type=float, default=8.0)\n",
    "    parser.add_argument(\"--download_directory\", type=str, default=\".\")\n",
    "    parser.add_argument(\"--depth\", type=int, default=3)\n",
    "    parser.add_argument(\"--width\", type=int, default=200)\n",
    "    parser.add_argument(\"--activation\", type=str, default=\"ReLU\")\n",
    "\n",
    "    # Grokfast\n",
    "    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"ema\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--lamb\", type=float, default=0.3) # grokfast's was 0.1 but for us 0.3 works better\n",
    "\n",
    "    # Samples ranking\n",
    "    parser.add_argument(\"--ema_alpha_sampl_rank\", type=float, default=0.9)\n",
    "\n",
    "    # Boolean arguements need this due to bad behavior of parser.parse_args\n",
    "    def boolean_string(s):\n",
    "        if s not in {\"False\", \"True\"}:\n",
    "            raise ValueError(\"Not a valid boolean string\")\n",
    "        return s == \"True\"\n",
    "\n",
    "    # These are the hyperparameters related to our online sampling filtering algorithm\n",
    "    parser.add_argument(\"--appl_sampl_filter\", type=boolean_string, default=True)  # If False, perform regular training\n",
    "    parser.add_argument(\"--sampling_distr_upd_freq\", type=int, default=1)  # How often to update the sampling distribution\n",
    "    parser.add_argument(\"--top_k\", type=float, default=0.1)  # Fraction of samples to select more frequently\n",
    "    parser.add_argument(\"--top_k_sampling_prob\", type=float, default=0.9)  # Probability of selecting a sample from the top-k\n",
    "    parser.add_argument(\"--high_freq_better\", type=boolean_string, default=True)  # If True, samples with higher frequency gradient content are considered better for training\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Try different hyperparameter values for your grid search here\n",
    "    # -----------------------------------------------------------------\n",
    "    args = parser.parse_args(\n",
    "        [\n",
    "            \"--appl_sampl_filter\", \"False\", # booleans as non strings in order to work\n",
    "            \"--sampling_distr_upd_freq\", \"1\", # the rest as strings for some reason\n",
    "            \"--top_k\", \"0.1\",\n",
    "            \"--top_k_sampling_prob\", \"0.9\",\n",
    "            \"--high_freq_better\", \"True\",\n",
    "        ]\n",
    "    )\n",
    "    # -----------------------------------------------------------------\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # Create arg.label for the filename of the saved results\n",
    "    if not args.appl_sampl_filter:\n",
    "        args.label = f\"grokfast_{args.filter}_sampling_{args.appl_sampl_filter}\"\n",
    "    else:\n",
    "        args.label = f\"high_freq_{args.high_freq_better}_top_k_{args.top_k}_top_k_prob_{args.top_k_sampling_prob}_upd_freq_{args.sampling_distr_upd_freq}\"\n",
    "\n",
    "    # Training with time recording\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call your training function\n",
    "    main(args)\n",
    "\n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert to minutes and seconds (optional)\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "    print(f\"Training completed in {int(minutes)} minutes and {int(seconds)} seconds.\")\n",
    "    print(f\"label:{args.label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T21:45:15.469278Z",
     "iopub.status.busy": "2025-02-05T21:45:15.468928Z",
     "iopub.status.idle": "2025-02-05T21:45:15.473270Z",
     "shell.execute_reply": "2025-02-05T21:45:15.472487Z",
     "shell.execute_reply.started": "2025-02-05T21:45:15.469255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the saved results for plotting/printing\n",
    "\n",
    "\n",
    "results_dir = \"/kaggle/working/results/mnist_online\"\n",
    "filename = f\"mnist_{args.label}.pt\"\n",
    "results_filename = os.path.join(results_dir, filename)\n",
    "\n",
    "filename_plot_acc = f\"mnist_{args.label}_acc.png\"\n",
    "results_filename_plot_acc = os.path.join(results_dir, filename_plot_acc)\n",
    "\n",
    "filename_plot_loss = f\"mnist_{args.label}_loss.png\"\n",
    "results_filename_plot_loss = os.path.join(results_dir, filename_plot_loss)\n",
    "\n",
    "\n",
    "\n",
    "results = torch.load(results_filename, weights_only=True)\n",
    "\n",
    "\n",
    "# Extract data from results\n",
    "its = results[\"its\"]  # Optimization steps\n",
    "train_acc = results[\"train_acc\"]  # Training accuracy\n",
    "val_acc = results[\"val_acc\"]  # Validation accuracy\n",
    "train_loss = results[\"train_loss\"]  # Training loss\n",
    "val_loss = results[\"val_loss\"]  # Validation loss\n",
    "steps_to_reach = results[\"steps_to_reach\"]  # Steps to reach 90% validation accuracy\n",
    "\n",
    "print(f\"Steps needed to reach 0.9 validation accuracy: {steps_to_reach}\")\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure()\n",
    "plt.plot(its, train_acc, label=\"train\")\n",
    "plt.plot(its, val_acc, label=\"val\")\n",
    "\n",
    "# Find and annotate the maximum validation accuracy\n",
    "max_val_acc = max(val_acc)\n",
    "max_val_idx = val_acc.index(max_val_acc)\n",
    "plt.annotate(f\"Max Val Acc: {max_val_acc:.4f}\",\n",
    "             (its[max_val_idx], max_val_acc),\n",
    "             textcoords=\"offset points\",\n",
    "             xytext=(0,10),\n",
    "             ha='center',\n",
    "             fontsize=10,\n",
    "             color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy - Grokfast:no Sample Filtering:no\")\n",
    "plt.xlabel(\"Optimization Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xscale(\"log\", base=10)\n",
    "plt.grid()\n",
    "plt.savefig(results_filename_plot_acc, dpi=150)\n",
    "\n",
    "print(\"Plots loaded successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMSJa3XxLl7rvVV5eF0x78J",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6569340,
     "sourceId": 10611406,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "064bec9d457349bb9fa6a559339e9263": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c9bf43e30b5a499a8de4604dc3f10ee2",
       "IPY_MODEL_17166645b0b14019a9f5fe3d70b6caa5",
       "IPY_MODEL_e451f4ad11bc44f8ab881eed658a8ee3"
      ],
      "layout": "IPY_MODEL_ceb7ec9cff0049c098ac4b5fc5b6bd3c"
     }
    },
    "07c3736f38804f78bea83bb64c695cac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ff27ff9889744e6af586debee44669c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "17166645b0b14019a9f5fe3d70b6caa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1776d91874314cd0ab3df42e1b77bd5d",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_915f46f7e6694bf0b14a91ced2256596",
      "value": 100
     }
    },
    "1776d91874314cd0ab3df42e1b77bd5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c00be62ed5f48868daac8e39944ba63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_740bfc3868e947e68c87f28690b96945",
      "placeholder": "​",
      "style": "IPY_MODEL_73b20e21e4394b6c9326c75594d6f062",
      "value": "Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"
     }
    },
    "30e1fcaaa18c40fcbb2182e33df28ea1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c0b6a5234ea48e3bf8fa9fb04e7442d",
      "placeholder": "​",
      "style": "IPY_MODEL_ccfc9ef4719e4aef9379f11160003ce1",
      "value": " 100/100 [04:53&lt;00:00,  3.31s/it]"
     }
    },
    "328beb9c29ed4dc094b6e476734a725f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36d9e94b822a4a5bb069cd474608bdb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37733b27dcd64ffaafd07959bc847f26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a3e587d4127443eb3bbae84621d4d06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4eb47dc8d09747718044ca73cfadc307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_936aa9172b2d4e0aa2f0cb84c0dabfad",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07c3736f38804f78bea83bb64c695cac",
      "value": 100
     }
    },
    "5c0b6a5234ea48e3bf8fa9fb04e7442d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73b20e21e4394b6c9326c75594d6f062": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "740bfc3868e947e68c87f28690b96945": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "915f46f7e6694bf0b14a91ced2256596": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "936aa9172b2d4e0aa2f0cb84c0dabfad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9bf43e30b5a499a8de4604dc3f10ee2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a3e587d4127443eb3bbae84621d4d06",
      "placeholder": "​",
      "style": "IPY_MODEL_37733b27dcd64ffaafd07959bc847f26",
      "value": "Loss: 7.6e+00|1.2e+01. Acc: 11.7%|12.5%: 100%"
     }
    },
    "ccfc9ef4719e4aef9379f11160003ce1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ceb7ec9cff0049c098ac4b5fc5b6bd3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "e451f4ad11bc44f8ab881eed658a8ee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_328beb9c29ed4dc094b6e476734a725f",
      "placeholder": "​",
      "style": "IPY_MODEL_36d9e94b822a4a5bb069cd474608bdb6",
      "value": " 100/100 [04:51&lt;00:00,  3.11s/it]"
     }
    },
    "fe873b542e0243ebbf591f5bc21ccd2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c00be62ed5f48868daac8e39944ba63",
       "IPY_MODEL_4eb47dc8d09747718044ca73cfadc307",
       "IPY_MODEL_30e1fcaaa18c40fcbb2182e33df28ea1"
      ],
      "layout": "IPY_MODEL_0ff27ff9889744e6af586debee44669c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
