{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RLNMqt_y8y0J"},"outputs":[],"source":["#Starting code for PatRec project\n","#   Omada 2 -- Grokfast experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4s6HWPGPSBJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736284264860,"user_tz":-120,"elapsed":1727,"user":{"displayName":"Chat Shared","userId":"05183133997536361572"}},"outputId":"44433b16-e775-4068-ab9c-fbaebc0d810e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gADYjdBHN1cg"},"outputs":[],"source":["# from google.colab import files\n","# files.upload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1736284264862,"user":{"displayName":"Chat Shared","userId":"05183133997536361572"},"user_tz":-120},"id":"pUw0ejsKP3sp","outputId":"4e42740f-b6c4-476e-df13-0cdebe28d8d8"},"outputs":[{"output_type":"stream","name":"stdout","text":[" grokfast.py\t\t\t\t Grokking_mnist_v1.ipynb   requirements.txt\n"," Groking_algo_v1.ipynb\t\t\t Grokking_qm9_v1.ipynb\t   results\n","'Grokking and how to avoid it.gslides'\t __pycache__\n"]}],"source":["!ls /content/drive/MyDrive/PatRec_Project_Shared_Folder/"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHJ2F3QuKau3","executionInfo":{"status":"ok","timestamp":1736284266508,"user_tz":-120,"elapsed":1657,"user":{"displayName":"Chat Shared","userId":"05183133997536361572"}},"outputId":"9ae4cf1b-6479-4216-801d-c1cb82100f18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdXeIGW_QC3w"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/PatRec_Project_Shared_Folder')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2295,"status":"ok","timestamp":1736284268801,"user":{"displayName":"Chat Shared","userId":"05183133997536361572"},"user_tz":-120},"id":"C13vv8V-KHUI","outputId":"1a8b8794-e0d2-49cc-bca4-043e6abdfcd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (3.8.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 3)) (0.20.1+cu121)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (2.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 5)) (2.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 6)) (0.13.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 7)) (4.67.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 8)) (1.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (1.4.7)\n","Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (3.11.10)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (5.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (2.32.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 5)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 5)) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 1)) (1.17.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 2)) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->-r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt (line 4)) (2024.12.14)\n"]}],"source":["!pip install -r /content/drive/MyDrive/PatRec_Project_Shared_Folder/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hrb9IxqrJeok"},"outputs":[],"source":["import math\n","from argparse import ArgumentParser\n","from itertools import permutations\n","import copy\n","\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtT0IRac8_19"},"outputs":[],"source":["from grokfast import gradfilter_ma, gradfilter_ema"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw5pFYxWJbGL"},"outputs":[],"source":["\n","class Block(nn.Module):\n","    \"\"\"Causal transformer block\n","    \"\"\"\n","\n","    def __init__(self, dim, num_heads):\n","        super().__init__()\n","        self.ln_1 = nn.LayerNorm(dim)\n","        self.ln_2 = nn.LayerNorm(dim)\n","        self.attn = nn.MultiheadAttention(dim, num_heads)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, dim * 4),\n","            nn.GELU(),\n","            nn.Linear(dim * 4, dim),\n","        )\n","\n","    def forward(self, x):\n","        attn_mask = torch.full(\n","            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n","        )\n","        attn_mask = torch.triu(attn_mask, diagonal=1)\n","        attn_mask[torch.isnan(attn_mask)] = 0.0 # fixes all 'nan' on 'mps' device\n","\n","        x = self.ln_1(x)\n","        a, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n","        x = x + a\n","        m = self.mlp(self.ln_2(x))\n","        x = x + m\n","        return x\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\"Causal Transformer decoder\n","    \"\"\"\n","\n","    def __init__(self, dim=128, num_layers=2, num_heads=4, num_tokens=97, seq_len=5):\n","        super().__init__()\n","        self.token_embeddings = nn.Embedding(num_tokens, dim)\n","        self.position_embeddings = nn.Embedding(seq_len, dim)\n","        self.layers = nn.ModuleList()\n","        for _ in range(num_layers):\n","            self.layers.append(Block(dim, num_heads))\n","\n","        self.ln_f = nn.LayerNorm(dim)\n","        self.head = nn.Linear(dim, num_tokens, bias=False)\n","\n","    def forward(self, x):\n","        h = self.token_embeddings(x)\n","        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n","        h = h + self.position_embeddings(positions).expand_as(h)\n","        for layer in self.layers:\n","            h = layer(h)\n","\n","        h = self.ln_f(h)\n","        logits = self.head(h)\n","        return logits\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjzP8njaJkJU"},"outputs":[],"source":["def multiplication_mod_p_data(p, eq_token, op_token):\n","    \"\"\"x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n","    \"\"\"\n","    x = torch.arange(p)\n","    y = torch.arange(1, p)\n","    x, y = torch.cartesian_prod(x, y).T\n","\n","    eq = torch.ones_like(x) * eq_token\n","    op = torch.ones_like(x) * op_token\n","    result = x * y % p\n","\n","    # \"All of our experiments used a small transformer trained on datasets of\n","    # equations of the form a◦b = c, where each of “a”, “◦”, “b”, “=”, and “c”\n","    # is a seperate token\"\n","    return torch.stack([x, op, y, eq, result])\n","\n"]},{"cell_type":"code","source":["import os\n","\n","# Specify the path to save in Google Drive\n","results_dir = \"/content/drive/MyDrive/PatRec_Project_Shared_Folder/results\"\n","os.makedirs(results_dir, exist_ok=True)"],"metadata":{"id":"fPwoxB0RIxv0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt:     data = multiplication_mod_p_data(args.p, eq_token, op_token)\n","# wabt to change dataatype for effeiceny\n","\n","def multiplication_mod_p_data(p, eq_token, op_token):\n","    \"\"\"x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n","    \"\"\"\n","    x = torch.arange(p)\n","    y = torch.arange(1, p)\n","    x, y = torch.cartesian_prod(x, y).T\n","\n","    eq = torch.ones_like(x) * eq_token\n","    op = torch.ones_like(x) * op_token\n","    result = x * y % p\n","\n","    # \"All of our experiments used a small transformer trained on datasets of\n","    # equations of the form a◦b = c, where each of “a”, “◦”, “b”, “=”, and “c”\n","    # is a seperate token\"\n","    return torch.stack([x, op, y, eq, result])\n"],"metadata":{"id":"MkrzdbdUZRq1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWtMguE3JmpK"},"outputs":[],"source":["def main(args):\n","    torch.manual_seed(args.seed)\n","\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # tokens for <op> and <=>. It's not clear why <=> is needed at all since it\n","    # has no effect on the output, but we'll leave it in to best follow the\n","    # paper.\n","    eq_token = args.p\n","    op_token = args.p + 1\n","\n","    # \"We trained a standard decoder-only transformer (Vaswani et al., 2017)\n","    # with causal attention masking, and calculated loss and accuracy only on\n","    # the answer part of the equation. For all experiments we used a\n","    # transformer with 2 layers, width 128, and 4 attention heads\"\n","    model = Decoder(\n","        dim=128, num_layers=2, num_heads=4, num_tokens=args.p + 2, seq_len=5\n","    ).to(device)\n","    nparams = sum([p.numel() for p in model.parameters() if p.requires_grad])\n","    print(model)\n","    print(f'Total number of parameters: {nparams}')\n","\n","    data = multiplication_mod_p_data(args.p, eq_token, op_token)\n","\n","    # Split the subset into training and validation sets\n","    split_idx = data.shape[1] // 2\n","    perm = torch.randperm(data.shape[1])\n","    train_idx = perm[:split_idx]\n","    valid_idx = perm[split_idx:]\n","    train_data = data[:, train_idx]\n","    valid_data = data[:, valid_idx]\n","\n","    # For most experiments we used AdamW optimizer with learning rate 10−3,\n","    # weight decay 1, β1 = 0.9, β2 = 0.98\n","    optimizer = getattr(torch.optim, args.optimizer)(\n","        model.parameters(),\n","        lr=args.lr,\n","        weight_decay=args.weight_decay,\n","        betas=(args.beta1, args.beta2),\n","    )\n","\n","    #  linear learning rate warmup over the first 10 updates\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(\n","        optimizer, lambda update: 1 if update > 10 else update / 10\n","    )\n","\n","    steps_per_epoch = math.ceil(train_data.shape[1] / args.batch_size)\n","\n","    its, train_acc, val_acc, train_loss, val_loss = [], [], [], [], []\n","    grads = None\n","    i = 0\n","\n","    # For logging network weights.\n","    net_its, nets = [], []\n","\n","    for e in tqdm(range(int(args.budget) // steps_per_epoch)):\n","\n","        # randomly shuffle train data\n","        train_data = train_data[:, torch.randperm(train_data.shape[1])]\n","\n","        for data, is_train in [(train_data, True), (valid_data, False)]:\n","\n","            model.train(is_train)\n","            total_loss = 0\n","            total_acc = 0\n","\n","            # torch.split faster than dataloader with tensor\n","            dl = torch.split(data, args.batch_size, dim=1)\n","            for input in dl:\n","                input = input.to(device)\n","\n","\n","                with torch.set_grad_enabled(is_train):\n","                    logits = model(input[:-1])\n","                    # calculate loss only on the answer part of the equation (last element\n","                    loss = F.cross_entropy(logits[-1], input[-1])\n","                    total_loss += loss.item() * input.shape[-1]\n","\n","                if is_train:\n","                    model.zero_grad()\n","                    loss.backward()\n","\n","                    #######\n","\n","                    trigger = i < 500 if args.two_stage else False\n","\n","                    if args.filter == \"none\":\n","                        pass\n","                    elif args.filter == \"ma\":\n","                        grads = gradfilter_ma(model, grads=grads, window_size=args.window_size, lamb=args.lamb, trigger=trigger)\n","                    elif args.filter == \"ema\":\n","                        grads = gradfilter_ema(model, grads=grads, alpha=args.alpha, lamb=args.lamb)\n","                    else:\n","                        raise ValueError(f\"Invalid gradient filter type `{args.filter}`\")\n","\n","                    #######\n","\n","                    optimizer.step()\n","                    scheduler.step()\n","                    i += 1\n","\n","                acc = (logits[-1].argmax(-1) == input[-1]).float().mean()\n","                total_acc += acc.item() * input.shape[-1]\n","\n","            if is_train:\n","                train_acc.append(total_acc / train_data.shape[-1])\n","                train_loss.append(total_loss / train_data.shape[-1])\n","                its.append(i)\n","            else:\n","                val_acc.append(total_acc / valid_data.shape[-1])\n","                val_loss.append(total_loss / valid_data.shape[-1])\n","\n","        if args.save_weights:\n","            do_save = e <= 500 or (e > 500 and (e + 1) % 100 == 0) or e == int(args.budget) // steps_per_epoch - 1\n","        else:\n","            do_save = (e + 1) % 100 == 0\n","        if do_save:\n","            steps = torch.arange(len(train_acc)).numpy() * steps_per_epoch\n","            plt.plot(steps, train_acc, label=\"train\")\n","            plt.plot(steps, val_acc, label=\"val\")\n","            plt.legend()\n","            plt.title(\"Modular Multiplication (training on 50% of data)\")\n","            plt.xlabel(\"Optimization Steps\")\n","            plt.ylabel(\"Accuracy\")\n","            plt.xscale(\"log\", base=10)\n","            plt.grid()\n","            plt.savefig(f\"{results_dir}/acc_{args.label}.png\", dpi=150)\n","            plt.close()\n","\n","            plt.plot(steps, train_loss, label=\"train\")\n","            plt.plot(steps, val_loss, label=\"val\")\n","            plt.legend()\n","            plt.title(\"Modular Multiplication (training on 50% of data)\")\n","            plt.xlabel(\"Optimization Steps\")\n","            plt.ylabel(\"Loss\")\n","            plt.xscale(\"log\", base=10)\n","            plt.grid()\n","            plt.savefig(f\"{results_dir}/loss_{args.label}.png\", dpi=150)\n","            plt.close()\n","\n","            results = {\n","                'its': its,\n","                'train_acc': train_acc,\n","                'train_loss': train_loss,\n","                'val_acc': val_acc,\n","                'val_loss': val_loss,\n","            }\n","\n","            if args.save_weights:\n","                net_its.append(e)\n","                nets.append(copy.deepcopy(model.state_dict()))\n","                results['net_its'] = net_its\n","                results['net'] = nets\n","\n","            torch.save(results, f\"{results_dir}/res_{args.label}.pt\")"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dne-ziHDdwNG","executionInfo":{"status":"ok","timestamp":1736284826651,"user_tz":-120,"elapsed":284,"user":{"displayName":"Chat Shared","userId":"05183133997536361572"}},"outputId":"a7c72a7b-2c3c-4639-af0f-e450e483191e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chBCi5e6M0L9","outputId":"0a6e42f7-1044-424e-c9fe-8a4cd709029b","executionInfo":{"status":"ok","timestamp":1736268367577,"user_tz":-120,"elapsed":2595769,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment results saved under name: none\n","Dataset size: 1.0\n","Decoder(\n","  (token_embeddings): Embedding(99, 128)\n","  (position_embeddings): Embedding(5, 128)\n","  (layers): ModuleList(\n","    (0-1): 2 x Block(\n","      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=128, out_features=512, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=512, out_features=128, bias=True)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  (head): Linear(in_features=128, out_features=99, bias=False)\n",")\n","Total number of parameters: 422784\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15789/15789 [43:15<00:00,  6.08it/s]\n"]}],"source":["if __name__ == \"__main__\":\n","    parser = ArgumentParser()\n","    parser.add_argument(\"--label\", default=\"\")\n","    parser.add_argument(\"--seed\", type=int, default=0)\n","    parser.add_argument(\"--p\", type=int, default=97)\n","    parser.add_argument(\"--budget\", type=int, default=3e5)\n","    parser.add_argument(\"--batch_size\", type=int, default=256)\n","    parser.add_argument(\"--lr\", type=float, default=1e-3)\n","    parser.add_argument(\"--beta1\", type=float, default=0.9)\n","    parser.add_argument(\"--beta2\", type=float, default=0.98)\n","    parser.add_argument(\"--weight_decay\", type=float, default=0)\n","    parser.add_argument(\"--optimizer\", default=\"Adam\")\n","    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"none\")\n","    parser.add_argument(\"--alpha\", type=float, default=0.99)\n","    parser.add_argument(\"--window_size\", type=int, default=50)\n","    parser.add_argument(\"--lamb\", type=float, default=5.0)\n","    parser.add_argument(\"--two_stage\", action='store_true')\n","    parser.add_argument(\"--save_weights\", action='store_true')\n","    parser.add_argument(\"--dataset_fraction\", type=float, default=1.0, help=\"Fraction of the dataset to use (0.0 to 1.0)\")\n","\n","\n","    # Parse known arguments to handle Jupyter conflicts\n","    args, unknown = parser.parse_known_args()\n","\n","    # Modify label dynamically\n","    filter_str = ('_' if args.label != '' else '') + args.filter\n","    window_size_str = f'_w{args.window_size}'\n","    alpha_str = f'_a{args.alpha:.3f}'.replace('.', '')\n","    lamb_str = f'_l{int(args.lamb)}'\n","\n","    if args.filter == \"none\":\n","        filter_suffix = \"\"\n","    elif args.filter == \"ma\":\n","        filter_suffix = window_size_str + lamb_str\n","    elif args.filter == \"ema\":\n","        filter_suffix = alpha_str + lamb_str\n","    else:\n","        raise ValueError(f\"Unrecognized filter type {args.filter}\")\n","\n","    optim_suffix = \"\"\n","    if args.weight_decay != 0:\n","        optim_suffix = optim_suffix + f'_wd{args.weight_decay:.1e}'.replace('.', '')\n","    if args.lr != 1e-3:\n","        optim_suffix = optim_suffix + f'_lrx{int(args.lr / 1e-3)}'\n","\n","    args.label = args.label + filter_str + filter_suffix + optim_suffix\n","    print(f\"Experiment results saved under name: {args.label}\")\n","\n","    print(f\"Dataset size: {args.dataset_fraction}\")\n","\n","\n","   # Call main\n","    main(args)\n"]},{"cell_type":"code","source":["#Running EMA algoritmh with window=20 below"],"metadata":{"id":"QKy1yxUci2h1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    parser = ArgumentParser()\n","    parser.add_argument(\"--label\", default=\"\")\n","    parser.add_argument(\"--seed\", type=int, default=0)\n","    parser.add_argument(\"--p\", type=int, default=97)\n","    parser.add_argument(\"--budget\", type=int, default=3e5)\n","    parser.add_argument(\"--batch_size\", type=int, default=256)\n","    parser.add_argument(\"--lr\", type=float, default=1e-3)\n","    parser.add_argument(\"--beta1\", type=float, default=0.9)\n","    parser.add_argument(\"--beta2\", type=float, default=0.98)\n","    parser.add_argument(\"--weight_decay\", type=float, default=0)\n","    parser.add_argument(\"--optimizer\", default=\"Adam\")\n","    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"ema\")\n","    parser.add_argument(\"--alpha\", type=float, default=0.99)\n","    parser.add_argument(\"--window_size\", type=int, default=50)\n","    parser.add_argument(\"--lamb\", type=float, default=5.0)\n","    parser.add_argument(\"--two_stage\", action='store_true')\n","    parser.add_argument(\"--save_weights\", action='store_true')\n","    parser.add_argument(\"--dataset_fraction\", type=float, default=1.0, help=\"Fraction of the dataset to use (0.0 to 1.0)\")\n","\n","\n","\n","\n","    # Parse known arguments to handle Jupyter conflicts\n","    args, unknown = parser.parse_known_args()\n","\n","    # Modify label dynamically\n","    filter_str = ('_' if args.label != '' else '') + args.filter\n","    window_size_str = f'_w{args.window_size}'\n","    alpha_str = f'_a{args.alpha:.3f}'.replace('.', '')\n","    lamb_str = f'_l{int(args.lamb)}'\n","\n","    if args.filter == \"none\":\n","        filter_suffix = \"\"\n","    elif args.filter == \"ma\":\n","        filter_suffix = window_size_str + lamb_str\n","    elif args.filter == \"ema\":\n","        filter_suffix = alpha_str + lamb_str\n","    else:\n","        raise ValueError(f\"Unrecognized filter type {args.filter}\")\n","\n","    optim_suffix = \"\"\n","    if args.weight_decay != 0:\n","        optim_suffix = optim_suffix + f'_wd{args.weight_decay:.1e}'.replace('.', '')\n","    if args.lr != 1e-3:\n","        optim_suffix = optim_suffix + f'_lrx{int(args.lr / 1e-3)}'\n","\n","    args.label = args.label + filter_str + filter_suffix + optim_suffix\n","    print(f\"Experiment results saved under name: {args.label}\")\n","\n","\n","\n","   # Call main\n","    main(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxlnAH9Qitqx","executionInfo":{"status":"ok","timestamp":1736278852661,"user_tz":-120,"elapsed":3493818,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"}},"outputId":"4eeeaec6-df70-40f5-851f-2f02949ff3ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment results saved under name: ema_a0990_l5\n","Dataset size: 1.0\n","Decoder(\n","  (token_embeddings): Embedding(99, 128)\n","  (position_embeddings): Embedding(5, 128)\n","  (layers): ModuleList(\n","    (0-1): 2 x Block(\n","      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=128, out_features=512, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=512, out_features=128, bias=True)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  (head): Linear(in_features=128, out_features=99, bias=False)\n",")\n","Total number of parameters: 422784\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15789/15789 [58:08<00:00,  4.53it/s]\n"]}]},{"cell_type":"code","source":["args = parser.parse_args([\n","    \"--window_size\", \"100\",\n","])\n","print(f\"Experiment results saved under name: {args.label}\")\n","\n","print(f\"Window: {args.window_size}\")\n","\n","print(f\"Alg: {args.filter}\")\n","\n","\n","# Call main\n","main(args)"],"metadata":{"id":"fEIpuOiSW2J1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = parser.parse_args([\n","    \"--window_size\", \"100\",\n","    \"--batch_size\", \"512\"\n","])\n","\n","print(f\"Experiment results saved under name: {args.label}\")\n","\n","print(f\"Window: {args.window_size}\")\n","\n","print(f\"Batches: {args.batch_size}\")\n","\n","\n","# Call main\n","main(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nea6LSCWUrKY","executionInfo":{"status":"ok","timestamp":1736283707972,"user_tz":-120,"elapsed":3880555,"user":{"displayName":"Nikolas Xiros","userId":"00495869844002127525"}},"outputId":"a64aff92-1a02-4443-804c-eb67b409ef18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment results saved under name: \n","Window: 100\n","Batches: 512\n","Decoder(\n","  (token_embeddings): Embedding(99, 128)\n","  (position_embeddings): Embedding(5, 128)\n","  (layers): ModuleList(\n","    (0-1): 2 x Block(\n","      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=128, out_features=512, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=512, out_features=128, bias=True)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  (head): Linear(in_features=128, out_features=99, bias=False)\n",")\n","Total number of parameters: 422784\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 30000/30000 [1:04:39<00:00,  7.73it/s]\n"]}]},{"cell_type":"code","source":["    parser = ArgumentParser()\n","    parser.add_argument(\"--label\", default=\"\")\n","    parser.add_argument(\"--seed\", type=int, default=0)\n","    parser.add_argument(\"--p\", type=int, default=97)\n","    parser.add_argument(\"--budget\", type=int, default=3e5)\n","    parser.add_argument(\"--batch_size\", type=int, default=256)\n","    parser.add_argument(\"--lr\", type=float, default=1e-3)\n","    parser.add_argument(\"--beta1\", type=float, default=0.9)\n","    parser.add_argument(\"--beta2\", type=float, default=0.98)\n","    parser.add_argument(\"--weight_decay\", type=float, default=0)\n","    parser.add_argument(\"--optimizer\", default=\"Adam\")\n","    parser.add_argument(\"--filter\", type=str, choices=[\"none\", \"ma\", \"ema\", \"fir\"], default=\"none\")\n","    parser.add_argument(\"--alpha\", type=float, default=0.99)\n","    parser.add_argument(\"--window_size\", type=int, default=50)\n","    parser.add_argument(\"--lamb\", type=float, default=5.0)\n","    parser.add_argument(\"--two_stage\", action='store_true')\n","    parser.add_argument(\"--save_weights\", action='store_true')\n","    parser.add_argument(\"--dataset_fraction\", type=float, default=1.0, help=\"Fraction of the dataset to use (0.0 to 1.0)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypF_G-4GpD-q","executionInfo":{"status":"ok","timestamp":1736284851728,"user_tz":-120,"elapsed":270,"user":{"displayName":"Chat Shared","userId":"05183133997536361572"}},"outputId":"3f69fb2b-6d67-41d3-b33b-16aa8773ba0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--dataset_fraction'], dest='dataset_fraction', nargs=None, const=None, default=1.0, type=<class 'float'>, choices=None, required=False, help='Fraction of the dataset to use (0.0 to 1.0)', metavar=None)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["args = parser.parse_args([\n","    \"--window_size\", \"100\",\n","    \"--filter\",\"ma\",\n","    \"--batch_size\", \"512\",\n","    \"--label\", \"same_as_paper\"\n","])\n","\n","print(f\"Experiment results saved under name: {args.label}\")\n","\n","print(f\"Alg: {args.filter}\")\n","\n","\n","# Call main\n","main(args)"],"metadata":{"id":"yoDNKEN6Zsb1","colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"status":"error","timestamp":1736289952197,"user_tz":-120,"elapsed":1667896,"user":{"displayName":"Chat Shared","userId":"05183133997536361572"}},"outputId":"483b9a9a-5ac0-458d-fc40-f0a6cc8740fd"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Experiment results saved under name: same_as_paper\n","Alg: ma\n","Decoder(\n","  (token_embeddings): Embedding(99, 128)\n","  (position_embeddings): Embedding(5, 128)\n","  (layers): ModuleList(\n","    (0-1): 2 x Block(\n","      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=128, out_features=512, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=512, out_features=128, bias=True)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  (head): Linear(in_features=128, out_features=99, bias=False)\n",")\n","Total number of parameters: 422784\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 15113/30000 [1:24:53<1:23:37,  2.97it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-30eaf293eab3>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Call main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-11a58f3cb4ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0;31m# calculate loss only on the answer part of the equation (last element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-95143f75131c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-95143f75131c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             )\n\u001b[1;32m   1367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1369\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6095\u001b[0m             \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6096\u001b[0m         ), \"use_separate_proj_weight is False but in_proj_weight is None\"\n\u001b[0;32m-> 6097\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6098\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6099\u001b[0m         assert (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}